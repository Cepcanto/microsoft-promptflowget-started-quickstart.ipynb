{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: what is BERT?\n",
      "Bot: BERT refers to \"Bidirectional Encoder Representations from Transformers\".\n",
      "User: what NLP tasks does it perform well?\n",
      "Bot: BERT can perform well on various Natural Language Processing (NLP) tasks such as:\n",
      "\n",
      "1. Sentence classification: BERT can classify the sentiment of single sentences using tasks like Stanford Sentiment Treebank (SST-2).\n",
      "\n",
      "2. Linguistic acceptability: It can determine whether an English sentence is linguistically \"acceptable\" or not, as in the Corpus of Linguistic Acceptability (CoLA) task.\n",
      "\n",
      "3. Textual similarity: BERT can evaluate the semantic similarity between sentence pairs, as in the Semantic Textual Similarity Benchmark (STS-B).\n",
      "\n",
      "4. Paraphrase detection: It can identify whether sentence pairs are semantically equivalent, like in the Microsoft Research Paraphrase Corpus (MRPC) task.\n",
      "\n",
      "5. Textual entailment: BERT can determine if one sentence entails another, as in the Recognizing Textual Entailment (RTE) task.\n",
      "\n",
      "BERT has also been applied to other tasks, although it may not perform as well on all of them.\n",
      "User: is BERT suitable for NER?\n",
      "Bot: Based on the provided context, BERT can perform well on Named Entity Recognition (NER) tasks. The context mentions the introduction of the CoNLL-2003 shared task, which is focused on Language-independent Named Entity Recognition. Therefore, BERT can be applied to NER tasks effectively.\n",
      "User: is it better than GPT\n",
      "Bot: Based on the provided context, it can be inferred that BERT and GPT are two different pre-training models with some similarities and differences. While both models have their own strengths and use cases, it is difficult to directly compare them as they serve different purposes.\n",
      "\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a pre-training approach that utilizes bidirectionality and two pre-training tasks, namely masked language modeling and next sentence prediction. BERT has shown significant improvements in various NLP tasks, such as sentence classification, text similarity, paraphrase detection, and textual entailment.\n",
      "\n",
      "On the other hand, GPT (Generative Pre-training Transformer) is a left-to-right Transformer language model trained on a large text corpus. It uses a different pre-training methodology by using a single sentence separator and classifier token introduced at fine-tuning time.\n",
      "\n",
      "While both BERT and GPT have their own advantages and differences in training data, model architecture, and fine-tuning approaches, the context does not provide a direct comparison in terms of which model is \"better.\" The context mainly focuses on the details and differences between BERT, ELMo, and GPT, without explicitly stating one model's superiority over the other.\n",
      "User: when was GPT come up?\n",
      "Bot: The provided context does not explicitly mention the introduction of GPT (Generative Pre-training Transformer). Therefore, based on the given context, we cannot determine the specific date or year of its introduction.\n",
      "User: when was BERT come up?\n",
      "Bot: Based on the provided context, BERT (Bidirectional Encoder Representations from Transformers) was introduced in a paper titled \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The paper was uploaded on arXiv on October 11, 2018. Therefore, BERT was introduced in 2018.\n",
      "User: so about same time?\n",
      "Bot: Based on the context provided, BERT was introduced in a paper titled \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, which was uploaded on arXiv on October 11, 2018. On the other hand, the context does not mention the exact time or date of the introduction of GPT (Generative Pre-training Transformer). Therefore, we cannot determine if BERT and GPT were introduced around the same time based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "from main import chat_with_pdf, print_stream_and_return_full_answer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "bert_paper_url = 'https://arxiv.org/pdf/1810.04805.pdf'\n",
    "questions = [\n",
    "    'what is BERT?',\n",
    "    'what NLP tasks does it perform well?',\n",
    "    'is BERT suitable for NER?',\n",
    "    'is it better than GPT',\n",
    "    'when was GPT come up?',\n",
    "    \"when was BERT come up?\",\n",
    "    'so about same time?'\n",
    "]\n",
    "\n",
    "history = []\n",
    "for q in questions:\n",
    "    stream, context = chat_with_pdf(q, bert_paper_url, history)\n",
    "    print(\"User: \" + q, flush=True)\n",
    "    print(\"Bot: \", end=\"\", flush=True)\n",
    "    answer = print_stream_and_return_full_answer(stream)\n",
    "    history = history + [{\"role\":\"user\", \"content\": q}, {\"role\":\"assistant\", \"content\": answer}]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfsdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
