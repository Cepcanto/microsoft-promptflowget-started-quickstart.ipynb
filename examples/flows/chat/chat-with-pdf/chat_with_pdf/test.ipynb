{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: what is BERT?\n",
      "Bot: BERT stands for Bidirectional Encoder Representations from Transformers.\n",
      "User: what NLP tasks does it perform well?\n",
      "Bot: BERT can perform well on various Natural Language Processing (NLP) tasks. Some of the tasks where BERT has shown good performance include:\n",
      "\n",
      "1. Sentence classification: Classifying sentences into different categories, such as sentiment analysis or linguistic acceptability.\n",
      "2. Textual entailment: Determining if a given sentence logically entails another sentence.\n",
      "3. Semantic textual similarity: Measuring the similarity in meaning between two sentences.\n",
      "4. Paraphrase detection: Identifying whether two sentence pairs convey the same meaning.\n",
      "5. Named entity recognition (NER): Identifying and classifying named entities within a text.\n",
      "\n",
      "These are just a few examples, and BERT's performance can be further improved through fine-tuning on specific tasks.\n",
      "User: is BERT suitable for NER?\n",
      "Bot: Yes, BERT is suitable for Named Entity Recognition (NER) tasks. The introduction to the CoNLL-2003 shared task on Language-independent Named Entity Recognition mentions the use of the BERT model by Erik F Tjong Kim Sang and Fien De Meulder for NER. BERT's bidirectional architecture and contextual understanding make it effective in capturing relationships and dependencies within a sentence, which is beneficial for NER tasks.\n",
      "User: is it better than GPT\n",
      "Bot: The comparison between BERT and GPT depends on the specific task and context. In terms of pre-training methods, BERT and GPT have some differences in architecture and training approaches. BERT is bidirectional and utilizes two pre-training tasks (masked LM and next sentence prediction), while GPT is a left-to-right Transformer language model trained on a large text corpus.\n",
      "\n",
      "Empirically, BERT has shown significant improvements in various NLP tasks and achieved state-of-the-art performance on benchmarks like the GLUE benchmark. BERT's bi-directionality and pre-training tasks contribute to its effectiveness. However, it's important to note that GPT and BERT have different strengths and can excel in specific tasks based on their respective architectures and training methods.\n",
      "\n",
      "To determine which model is better, it is crucial to evaluate their performance on a specific task or benchmark dataset. It is recommended to conduct comparative experiments or refer to existing research studies to make a more informed assessment of their relative performance in a given context.\n",
      "User: when was GPT come up?\n",
      "Bot: The provided context does not explicitly mention the development or creation date of GPT (Generative Pre-trained Transformer). Therefore, based on the given context, it is not possible to derive the specific answer to the question.\n",
      "User: when was BERT come up?\n",
      "Bot: Based on the provided context, BERT, which stands for Bidirectional Encoder Representations from Transformers, was introduced in the paper titled \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" The paper was authored by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova from Google AI Language. The development of BERT was documented in the paper, but the specific date of its creation or release is not mentioned in the available context.\n",
      "User: so about same time?\n",
      "Bot: Based on the provided context, BERT and GPT were both developed as pre-training approaches for Natural Language Processing, but their exact development times are not directly mentioned. However, the paper on BERT, titled \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\" was published in October 2018. On the other hand, GPT, which stands for Generative Pre-trained Transformer, was introduced by OpenAI in June 2018. Based on this information, it can be inferred that BERT was developed around a similar timeframe as GPT.\n"
     ]
    }
   ],
   "source": [
    "from main import chat_with_pdf, print_stream_and_return_full_answer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "bert_paper_url = 'https://arxiv.org/pdf/1810.04805.pdf'\n",
    "questions = [\n",
    "    'what is BERT?',\n",
    "    'what NLP tasks does it perform well?',\n",
    "    'is BERT suitable for NER?',\n",
    "    'is it better than GPT',\n",
    "    'when was GPT come up?',\n",
    "    \"when was BERT come up?\",\n",
    "    'so about same time?'\n",
    "]\n",
    "\n",
    "history = []\n",
    "for q in questions:\n",
    "    stream, context = chat_with_pdf(q, bert_paper_url, history)\n",
    "    print(\"User: \" + q, flush=True)\n",
    "    print(\"Bot: \", end=\"\", flush=True)\n",
    "    answer = print_stream_and_return_full_answer(stream)\n",
    "    history = history + [{\"role\":\"user\", \"content\": q}, {\"role\":\"assistant\", \"content\": answer}]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfsdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
