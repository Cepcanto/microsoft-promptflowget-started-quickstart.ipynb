{"node_name": "summarize_text_content", "line_number": 2, "run_info": {"node": "summarize_text_content", "flow_run_id": "ff9d2245-7be7-4d9a-a501-1e1a09dabf22", "run_id": "ff9d2245-7be7-4d9a-a501-1e1a09dabf22_summarize_text_content_2", "status": "Completed", "inputs": {"deployment_name": "gpt-35-turbo", "temperature": 0.2, "top_p": 1.0, "max_tokens": 128, "presence_penalty": 0.0, "frequency_penalty": 0.0, "text": "The Capacity for Moral Self-Correction in Large Language Models \\ AnthropicProductResearchCompanyNewsCareersResearchSocietal ImpactThe Capacity for Moral Self-Correction in Large Language ModelsFeb 15, 2023Read PaperAbstractWe test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.Policy MemoMoral Self-Correction Policy MemoRelatedSee AllResearchSleeper Agents: Training Deceptive LLMs that Persist Through Safety TrainingJan 14, 2024ResearchEvaluating and Mitigating Discrimination in Language Model DecisionsDec 7, 2023ResearchSpecific versus General Principles for Constitutional AIOct 24, 2023ProductResearchCompanyNewsCareersPress InquiriesSupportTwitterLinkedInTerms of ServicePrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance\u00a9 2024 Anthropic PBC"}, "output": "A new paper by Anthropic PBC explores the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" and avoid producing harmful outputs. The paper presents evidence from three different experiments that support this hypothesis, showing that the capability for moral self-correction emerges at 22B model parameters and typically improves with increasing model size and RLHF training. The authors believe that language models can follow instructions to avoid certain kinds of morally harmful outputs and that their results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.", "metrics": null, "error": null, "parent_run_id": "ff9d2245-7be7-4d9a-a501-1e1a09dabf22_2", "start_time": "2024-02-07T07:07:54.384496Z", "end_time": "2024-02-07T07:07:56.094766Z", "index": 2, "api_calls": [{"name": "AzureOpenAI.chat", "type": "Tool", "inputs": {"prompt": "system:\nPlease summarize the following text in one paragraph. 100 words.\nDo not add any information that is not in the text.\n\nuser:\nText: {{text}}\nSummary: ", "deployment_name": "gpt-35-turbo", "temperature": 0.2, "top_p": 1.0, "max_tokens": 128, "presence_penalty": 0.0, "frequency_penalty": 0.0, "text": "The Capacity for Moral Self-Correction in Large Language Models \\ AnthropicProductResearchCompanyNewsCareersResearchSocietal ImpactThe Capacity for Moral Self-Correction in Large Language ModelsFeb 15, 2023Read PaperAbstractWe test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.Policy MemoMoral Self-Correction Policy MemoRelatedSee AllResearchSleeper Agents: Training Deceptive LLMs that Persist Through Safety TrainingJan 14, 2024ResearchEvaluating and Mitigating Discrimination in Language Model DecisionsDec 7, 2023ResearchSpecific versus General Principles for Constitutional AIOct 24, 2023ProductResearchCompanyNewsCareersPress InquiriesSupportTwitterLinkedInTerms of ServicePrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance\u00a9 2024 Anthropic PBC"}, "output": "A new paper by Anthropic PBC explores the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" and avoid producing harmful outputs. The paper presents evidence from three different experiments that support this hypothesis, showing that the capability for moral self-correction emerges at 22B model parameters and typically improves with increasing model size and RLHF training. The authors believe that language models can follow instructions to avoid certain kinds of morally harmful outputs and that their results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.", "start_time": 1707289674.385092, "end_time": 1707289676.092965, "error": null, "children": [{"name": "openai.resources.chat.completions.Completions.create", "type": "LLM", "inputs": {"self": "<openai.resources.chat.completions.Completions object at 0x7f1818552070>", "model": "gpt-35-turbo", "messages": [{"role": "system", "content": "Please summarize the following text in one paragraph. 100 words.\nDo not add any information that is not in the text."}, {"role": "user", "content": "Text: The Capacity for Moral Self-Correction in Large Language Models \\ AnthropicProductResearchCompanyNewsCareersResearchSocietal ImpactThe Capacity for Moral Self-Correction in Large Language ModelsFeb 15, 2023Read PaperAbstractWe test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.Policy MemoMoral Self-Correction Policy MemoRelatedSee AllResearchSleeper Agents: Training Deceptive LLMs that Persist Through Safety TrainingJan 14, 2024ResearchEvaluating and Mitigating Discrimination in Language Model DecisionsDec 7, 2023ResearchSpecific versus General Principles for Constitutional AIOct 24, 2023ProductResearchCompanyNewsCareersPress InquiriesSupportTwitterLinkedInTerms of ServicePrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance\u00a9 2024 Anthropic PBC\nSummary:"}], "temperature": 0.2, "top_p": 1.0, "n": 1, "stream": false, "stop": null, "max_tokens": 128, "presence_penalty": 0.0, "frequency_penalty": 0.0, "logit_bias": {}, "user": "", "response_format": null}, "output": {"id": "chatcmpl-8pWKALrNfE47Wg7xlaG7ZrCLfP5Xm", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "A new paper by Anthropic PBC explores the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" and avoid producing harmful outputs. The paper presents evidence from three different experiments that support this hypothesis, showing that the capability for moral self-correction emerges at 22B model parameters and typically improves with increasing model size and RLHF training. The authors believe that language models can follow instructions to avoid certain kinds of morally harmful outputs and that their results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.", "role": "assistant", "function_call": null, "tool_calls": null}}], "created": 1707289674, "model": "gpt-35-turbo", "object": "chat.completion", "system_fingerprint": null, "usage": {"completion_tokens": 122, "prompt_tokens": 390, "total_tokens": 512}}, "start_time": 1707289674.389251, "end_time": 1707289676.092841, "error": null, "children": [], "node_name": null, "parent_id": "a1f97583-f86c-403d-aae0-331bc3fc132a", "id": "71e3f77b-7df9-4578-884a-85491c2af50a", "system_metrics": {"completion_tokens": 122, "prompt_tokens": 390, "total_tokens": 512}}], "node_name": "summarize_text_content", "parent_id": "", "id": "a1f97583-f86c-403d-aae0-331bc3fc132a", "system_metrics": {"completion_tokens": 122, "prompt_tokens": 390, "total_tokens": 512}}], "variant_id": "", "cached_run_id": null, "cached_flow_run_id": null, "logs": {"stdout": "", "stderr": ""}, "system_metrics": {"completion_tokens": 122, "prompt_tokens": 390, "total_tokens": 512, "duration": 1.71027}, "result": "A new paper by Anthropic PBC explores the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" and avoid producing harmful outputs. The paper presents evidence from three different experiments that support this hypothesis, showing that the capability for moral self-correction emerges at 22B model parameters and typically improves with increasing model size and RLHF training. The authors believe that language models can follow instructions to avoid certain kinds of morally harmful outputs and that their results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles."}, "start_time": "2024-02-07T07:07:54.384496", "end_time": "2024-02-07T07:07:56.094766", "status": "Completed"}