{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with prompty\n",
    "\n",
    "**Learning Objectives** - Upon completing this tutorial, you should be able to:\n",
    "\n",
    "- Write LLM application using prompty and visualize the trace of your application.\n",
    "- Understand how to handle chat conversation using prompty\n",
    "- batch run prompty against multi lines of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install promptflow-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompty\n",
    "\n",
    "Prompty is a file with .prompty extension for developing prompt template. \n",
    "The prompty asset is a markdown file with a modified front matter. \n",
    "The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chat.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before running below cell, please configure required environment variable `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` by create an `.env` file. Please refer to [.env.example](../.env.example) as an template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import Flow\n",
    "\n",
    "# load prompty as a flow\n",
    "f = Flow.load(\"chat.prompty\")\n",
    "# execute the flow as function\n",
    "question = \"What is the capital of France?\"\n",
    "result = f(firstName=\"John\", lastName=\"Doe\", question=question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trace by using start_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run below cell will collect a trace in trace UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun the function, which will be recorded in the trace\n",
    "result = f(firstName=\"John\", lastName=\"Doe\", question=question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval the result \n",
    "\n",
    "In this example, we will use a prompt that determines whether a chat conversation contains an apology from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompty = \"../eval-apology/apology.prompty\"\n",
    "\n",
    "with open(eval_prompty) as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompty as a flow\n",
    "eval_flow = Flow.load(eval_prompty)\n",
    "# execute the flow as function\n",
    "result = eval_flow(question=question, answer=result[\"answer\"], messages=[])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch run with multi-line data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# batch run requires promptflow-devkit package\n",
    "%pip install promptflow-devkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import PFClient\n",
    "\n",
    "pf = PFClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = \"chat.prompty\"  # path to the prompty file\n",
    "data = \"./data.jsonl\"  # path to the data file\n",
    "\n",
    "# create run with the flow and data\n",
    "base_run = pf.run(\n",
    "    flow=flow,\n",
    "    data=data,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf.get_details(base_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate your prompty\n",
    "Then you can use an evaluation prompty to evaluate your prompty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on the previous batch run\n",
    "The **base_run** is the batch run we completed in step 2 above, for web-classification flow with \"data.jsonl\" as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_run = pf.run(\n",
    "    flow=eval_prompty,\n",
    "    data=\"./data.jsonl\",  # path to the data file\n",
    "    run=base_run,  # specify base_run as the run you want to evaluate\n",
    "    column_mapping={\n",
    "        \"messages\": \"${data.chat_history}\",\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"answer\": \"${run.outputs.answer}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf.get_details(eval_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "By now you've successfully run your first prompt flow and even did evaluation on it. That's great!\n",
    "\n",
    "You can check out more examples:\n",
    "- [Basic Chat](../chat-basic/README.md): demonstrates how to create a chatbot that can remember previous interactions and use the conversation history to generate next message."
   ]
  }
 ],
 "metadata": {
  "description": "A quickstart tutorial to run a chat prompty and evaluate it.",
  "kernelspec": {
   "display_name": "prompt_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "resources": "examples/requirements.txt, examples/prompty/chat-basic, examples/prompty/eval-apology"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
