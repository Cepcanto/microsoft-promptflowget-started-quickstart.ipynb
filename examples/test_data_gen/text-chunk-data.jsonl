{"text_chunk": "Connections\n\nConnections are for storing information about how to access external services like LLMs: endpoint, api keys etc.\n\n- In your local development environment, the connections are persisted in your local machine with keys encrypted.\n- In Azure AI, connections can be configured to be shared across the entire workspace. Secrets associated with connections are securely persisted in the corresponding Azure Key Vault, adhering to robust security and compliance standards.\n\nPrompt flow provides a variety of pre-built connections, including Azure Open AI, Open AI, etc. These pre-built connections enable seamless integration with these resources within the built-in tools. Additionally, you have the flexibility to create custom connection types using key-value pairs, empowering them to tailor the connections to their specific requirements, particularly in Python tools.\n\n| Connection type                                              | Built-in tools                  |\n| ------------------------------------------------------------ | ------------------------------- |\n| Azure Open AI | LLM or Python                   |\n| Open AI                               | LLM or Python                   |\n| Cognitive Search | Vector DB Lookup or Python      |\n| Serp                                 | Serp API or Python              |\n| Custom                                                       | Python                          |\n\nBy leveraging connections in prompt flow, you can easily establish and manage connections to external APIs and data sources, facilitating efficient data exchange and interaction within their AI applications."}
{"text_chunk": "Next steps\n\n- Create connections"}
{"text_chunk": "Flows\n\nA flow in prompt flow is a DAG of functions (we call them tools). These functions/tools connected via input/output dependencies and executed based on the topology by prompt flow executor.\n\nA flow is represented as a YAML file and can be visualized with our Prompt flow for VS Code extension. Here is an example:\n\n!flow_dag"}
{"text_chunk": "Flow types\n\nPrompt flow has three flow types:\n\n- **Standard flow** and **Chat flow**: these two are for you to develop your LLM application. The primary difference between the two lies in the additional support provided by the \"Chat Flow\" for chat applications. For instance, you can define chat_history, chat_input, and chat_output for your flow. The prompt flow, in turn, will offer a chat-like experience (including conversation history) during the development of the flow. Moreover, it also provides a sample chat application for deployment purposes.\n- **Evaluation flow** is for you to test/evaluate the quality of your LLM application (standard/chat flow). It usually run on the outputs of standard/chat flow, and compute some metrics that can be used to determine whether the standard/chat flow performs well. E.g. is the answer accurate? is the answer fact-based?"}
{"text_chunk": "When to use standard flow vs. chat flow?\n\nAs a general guideline, if you are building a chatbot that needs to maintain conversation history, try chat flow. In most other cases, standard flow should serve your needs.\n\nOur examples should also give you an idea when to use what:\n- examples/flows/standard\n- examples/flows/chat"}
{"text_chunk": "Next steps\n\n- Quick start\n- Initialize and test a flow\n- Run and evaluate a flow\n- Tune prompts using variants"}
{"text_chunk": "Tools\n\nPrompt flow provides 3 basic tools:\n- LLM: The LLM tool allows you to write custom prompts and leverage large language models to achieve specific goals, such as summarizing articles, generating customer support responses, and more.\n- Python: The Python tool enables you to write custom Python functions to perform various tasks, such as fetching web pages, processing intermediate data, calling third-party APIs, and more.\n- Prompt: The Prompt tool allows you to prepare a prompt as a string for more complex use cases or for use in conjunction with other prompt tools or python tools."}
{"text_chunk": "More tools\n\nOur partners also contributes other useful tools for advanced scenarios, here are some links:\n- Vector DB Lookup: vector search tool that allows users to search top k similar vectors from vector database.\n- Faiss Index Lookup: querying within a user-provided Faiss-based vector store."}
{"text_chunk": "Custom tools\n\nYou can create your own tools that can be shared with your team or anyone in the world. \nLearn more on Create and Use Tool Package"}
{"text_chunk": "Next steps\n\nFor more information on the available tools and their usage, visit the our reference doc."}
{"text_chunk": "Variants\n\nA variant refers to a specific version of a tool node that has distinct settings. Currently, variants are supported only in the LLM tool. For example, in the LLM tool, a new variant can represent either a different prompt content or different connection settings.\n\nSuppose you want to generate a summary of a news article. You can set different variants of prompts and settings like this:\n\n| Variants  | Prompt                                                       | Connection settings |\n| --------- | ------------------------------------------------------------ | ------------------- |\n| Variant 0 | `Summary: {{input sentences}}`                               | Temperature = 1     |\n| Variant 1 | `Summary: {{input sentences}}`                               | Temperature = 0.7   |\n| Variant 2 | `What is the main point of this article? {{input sentences}}` | Temperature = 1     |\n| Variant 3 | `What is the main point of this article? {{input sentences}}` | Temperature = 0.7   |\n\nBy utilizing different variants of prompts and settings, you can explore how the model responds to various inputs and outputs, enabling you to discover the most suitable combination for your requirements."}
{"text_chunk": "Benefits of using variants\n\n- **Enhance the quality of your LLM generation**: By creating multiple variants of the same LLM node with diverse prompts and configurations, you can identify the optimal combination that produces high-quality content aligned with your needs.\n- **Save time and effort**: Even slight modifications to a prompt can yield significantly different results. It's crucial to track and compare the performance of each prompt version. With variants, you can easily manage the historical versions of your LLM nodes, facilitating updates based on any variant without the risk of forgetting previous iterations. This saves you time and effort in managing prompt tuning history.\n- **Boost productivity**: Variants streamline the optimization process for LLM nodes, making it simpler to create and manage multiple variations. You can achieve improved results in less time, thereby increasing your overall productivity.\n- **Facilitate easy comparison**: You can effortlessly compare the results obtained from different variants side by side, enabling you to make data-driven decisions regarding the variant that generates the best outcomes."}
{"text_chunk": "Next steps\n\n- Tune prompts with variants"}
{"text_chunk": "Design principles\n\nWhen we started this project, LangChain already became popular esp. after the ChatGPT launch. One of the questions we\u2019ve been asked is what\u2019s the difference between prompt flow and LangChain. This article is to elucidate the reasons for building prompt flow and the deliberate design choices we have made. To put it succinctly, prompt flow is a suite of development tools for you to build LLM apps with a strong emphasis of quality through experimentations, not a framework - which LangChain is.\n\nWhile LLM apps are mostly in exploration stage, Microsoft started in this area a bit earlier and we\u2019ve had the opportunity to observe how developers are integrating LLMs into existing systems or build new applications. These invaluable insights have shaped the fundamental design principles of prompt flow."}
{"text_chunk": "1. Expose the prompts vs. hiding them\n\nThe core essence of LLM applications lies in the prompts themselves, at least for today. When developing a reasonably complex LLM application, the majority of development work should be \u201ctuning\u201d the prompts (note the intentional use of the term \"tuning,\" which we will delve into further later on). Any framework or tool trying to help in this space should focus on making prompt tuning easier and more straightforward. On the other hand, prompts are very volatile, it's unlikely to write a single prompt that can work across different models or even different version of same models. Building a successful LLM-based application, you have to understand every prompt introduced, so that you can tune it when necessary. LLM is simply not powerful or deterministic enough that you can use a prompt written by others like you use libraries in traditional programming languages.\n\nIn this context, any design that tries to provide a smart function or agent by encapsulating a few prompts in a library is unlikely to yield favorable results in real-world scenarios. And hiding prompts inside a library\u2019s code base only makes it\u2019s hard for people to improve or tailor the prompts to suit their specific needs.\n\nPrompt flow, being positioned as a tool, refrains from wrapping any prompts within its core codebase. The only place you will see prompts are our sample flows, which are, of course, available for adoption and utilization. Every prompt should be authored and controlled by the developers themselves, rather than relying on us."}
{"text_chunk": "2. A new way of work\n\nLLMs possess remarkable capabilities that enable developers to enhance their applications without delving deep into the intricacies of machine learning. In the meantime, LLMs make these apps more stochastic, which pose new challenges to application development. Merely asserting \"no exception\" or \"result == x\" in gated tests is no longer sufficient. Adopting a new methodology and employing new tools becomes imperative to ensure the quality of LLM applications \u2014 an entirely novel way of working is required.\n\nAt the center of this paradigm shift is evaluation, a term frequently used in machine learning space, refers to the process of assessing the performance and quality of a trained model. It involves measuring how well the model performs on a given task or dataset, which plays a pivotal role in understanding the model's strengths, weaknesses, and overall effectiveness. Evaluation metrics and techniques vary depending on the specific task and problem domain. Some common metrics include accuracy, precision and recall, you probably already familiar with. Now the LLM apps share similarities with machine learning models, they requires an evaluation-centric approach integrated into the development workflow, with a robust set of metrics and evaluation forming the foundation for ensuring the quality of LLM applications.\n\nPrompt flow offers a range of tools to streamline the new way of work:\n\n* Develop your evaluation program as Evaluation flow to calculate metrics for your app/flow, learn from our sample evaluation flows.\n* Iterate on your application flow and run evaluation flows via the SDK/CLI, allowing you to compare metrics and choose the optimal candidate for release. These iterations include trying different prompts, different LLM parameters like temperature etc. - this is referred as \u201ctuning\u201d process earlier, or sometime referred as experimentation.\n* Integrate the evaluation into your CI/CD pipeline, aligning the assertions in your gated tests with the selected metrics.\n\n\nPrompt flow introduces two conceptual components to facilitate this workflow:\n\n* Evaluation flow: a flow type that indicates this flow is not for deploy or integrate into your app, it\u2019s for evaluating an app/flow performance.\n* Run: every time you run your flow with data, or run an evaluation on the output of a flow, a Run object is created to manage the history and allow for comparison and additional analysis.\n\nWhile new concepts introduce additional cognitive load, we firmly believe they hold greater importance compared to abstracting different LLM APIs or vector database APIs."}
{"text_chunk": "3. Optimize for \u201cvisibility\u201d\n\nThere are quite some interesting application patterns emerging because of LLMs, like Retrieval Augmented Generation (RAG), ReAct and more. Though how LLMs work may remain enigmatic to many developers, how LLM apps work is not - they essentially involve a series of calls to external services such as LLMs, databases, and search engines, all glued together. Architecturally there isn\u2019t much new, patterns like RAG and ReAct are both straightforward to implement once a developer understands what they are - plain Python programs with API calls to external services can totally serve the purpose effectively.\n\nBy observing many internal use cases, we learned that deeper insight into the detail of the execution is critical. Establishing a systematic method for tracking interactions with external systems is one of design priority. Consequently, We adopted an unconventional approach - prompt flow has a YAML file describing how function calls (we call them Tools) are executed and connected into a Directed Acyclic Graph (DAG). \n\nThis approach offers several key benefits, primarily centered around **enhanced visibility**:\n1) During development, your flow can be visualized in an intelligible manner, enabling clear identification of any faulty components. As a byproduct, you obtain an architecturally descriptive diagram that can be shared with others.\n2) Each node in the flow has it\u2019s internal detail visualized in a consistent way.\n3) Single nodes can be individually run or debugged without the need to rerun previous nodes.\n\n\n!promptflow-dag\n\nThe emphasis on visibility in prompt flow's design helps developers to gain a comprehensive understanding of the intricate details of their applications. This, in turn, empowers developers to engage in effective troubleshooting and optimization.\n\nDespite there're some control flow features like \"activate-when\" to serve the needs of branches/switch-case, we do not intend to make Flow itself Turing-complete. If you want to develop an agent which is fully dynamic and guided by LLM, leveraging Semantic Kernel together with prompt flow would be a favorable option."}
{"text_chunk": "Dev Setup"}
{"text_chunk": "Set up process\n\n- First create a new conda environment. Please specify python version as 3.9.\n  `conda create -n  python=3.9`.\n- Activate the env you created.\n- Set environment variable `PYTHONPATH` in your new conda environment.\n  `conda env config vars set PYTHONPATH=\\promptflow`.\n  Once you have set the environment variable, you have to reactivate your environment.\n  `conda activate `.\n- In root folder, run `python scripts/building/dev_setup.py --promptflow-extra-deps azure` to install the package and dependencies."}
{"text_chunk": "How to run tests"}
{"text_chunk": "Set up your secrets\n\n`dev-connections.json.example` is a template about connections provided in `src/promptflow`. You can follow these steps to refer to this template to configure your connection for the test cases:\n1. `cd ./src/promptflow`\n2. Run the command `cp dev-connections.json.example connections.json`;\n3. Replace the values in the json file with your connection info;\n4. Set the environment `PROMPTFLOW_CONNECTIONS='connections.json'`;\n\nAfter above setup process is finished. You can use `pytest` command to run test, for example in root folder you can:"}
{"text_chunk": "Run tests via command\n\n- Run all tests under a folder: `pytest src/promptflow/tests -v`\n- Run a single test: ` pytest src/promptflow/tests/promptflow_test/e2etests/test_executor.py::TestExecutor::test_executor_basic_flow -v`"}
{"text_chunk": "Run tests in VSCode\n\n1. Set up your python interperter\n\n- Open the Command Palette (Ctrl+Shift+P) and select `Python: Select Interpreter`.\n\n!img0\n\n- Select existing conda env which you created previously.\n\n!img1\n\n2. Set up your test framework and directory\n\n- Open the Command Palette (Ctrl+Shift+P) and select `Python: Configure Tests`.\n\n!img2\n\n- Select `pytest` as test framework.\n\n!img3\n\n- Select `Root directory` as test directory.\n\n!img4\n\n3. Exclude specific test folders.\n\nYou can exclude specific test folders if you don't have some extra dependency to avoid VS Code's test discovery fail.\nFor example, if you don't have azure dependency, you can exclude `sdk_cli_azure_test`.\nOpen `.vscode/settings.json`, write `\"--ignore=src/promptflow/tests/sdk_cli_azure_test\"` to `\"python.testing.pytestArgs\"`.\n\n!img6\n\n4. Click the `Run Test` button on the left\n\n!img5"}
{"text_chunk": "Run tests in pycharm\n\n1. Set up your pycharm python interpreter\n\n!img0\n\n2. Select existing conda env which you created previously\n\n!img1\n\n3. Run test, right-click the test name to run, or click the green arrow button on the left.\n\n!img2"}
{"text_chunk": "Record and replay tests\n\nPlease refer to Replay End-to-End Tests to learn how to record and replay tests."}
{"text_chunk": "How to write docstring.\n\nA clear and consistent API documentation is crucial for the usability and maintainability of our codebase. Please refer to API Documentation Guidelines to learn how to write docstring when developing the project."}
{"text_chunk": "How to write tests\n\n- Put all test data/configs under `src/promptflow/tests/test_configs`.\n- Write unit tests:\n  - Flow run: `src/promptflow/tests/sdk_cli_test/unittest/`\n  - Flow run in azure: `src/promptflow/tests/sdk_cli_azure_test/unittest/`\n- Write e2e tests:\n  - Flow run: `src/promptflow/tests/sdk_cli_test/e2etests/`\n  - Flow run in azure: `src/promptflow/tests/sdk_cli_azure_test/e2etests/`\n- Test file name and the test case name all start with `test_`.\n- A basic test example, see test_connection.py."}
{"text_chunk": "Test structure\n\nCurrently all tests are under `src/promptflow/tests/` folder:\n\n- tests/\n  - promptflow/\n    - sdk_cli_test/\n      - e2etests/\n      - unittests/\n    - sdk_cli_azure_test/\n      - e2etests/\n      - unittests/\n  - test_configs/\n    - connections/\n    - datas/\n    - flows/\n    - runs/\n    - wrong_flows/\n    - wrong_tools/\n\nWhen you want to add tests for a new feature, you can add new test file let's say a e2e test file `test_construction.py`\nunder `tests/promptflow/**/e2etests/`.\n\nOnce the project gets more complicated or anytime you find it necessary to add new test folder and test configs for\na specific feature, feel free to split the `promptflow` to more folders, for example:\n\n- tests/\n  - (Test folder name)/\n    - e2etests/\n      - test_xxx.py\n    - unittests/\n      - test_xxx.py\n  - test_configs/\n    - (Data or config folder name)/"}
{"text_chunk": "Promptflow Reference Documentation Guide"}
{"text_chunk": "Overview\n\nThis guide describes how to author Python docstrings for promptflow public interfaces. See our doc site at Promptflow API reference documentation."}
{"text_chunk": "Principles\n\n- **Coverage**: Every public object must have a docstring. For private objects, docstrings are encouraged but not required.\n- **Style**: All docstrings should be written in Sphinx style noting all types and if any exceptions are raised.\n- **Relevance**: The documentation is up-to-date and relevant to the current version of the product.\n- **Clarity**: The documentation is written in clear, concise language that is easy to understand.\n- **Consistency**: The documentation has a consistent format and structure, making it easy to navigate and follow."}
{"text_chunk": "How to write the docstring\n\nFirst please read through Sphinx style to have a basic understanding of sphinx style docstring."}
{"text_chunk": "Write class docstring\n\nLet's start with a class example:\n```python\nfrom typing import Dict, Optional, Union\nfrom promptflow import PFClient\n\nclass MyClass:\n    \"\"\"One-line summary of the class.\n\n    More detailed explanation of the class. May include below notes, admonitions, code blocks.\n\n    .. note::\n\n        Here are some notes to show, with a nested python code block:\n\n        .. code-block:: python\n\n            from promptflow import MyClass, PFClient\n            obj = MyClass(PFClient())\n\n    .. admonition:: [Title of the admonition]\n\n        Here are some admonitions to show.\n\n    :param client: Descrition of the client.\n    :type client: ~promptflow.PFClient\n    :param param_int: Description of the parameter.\n    :type param_int: Optional[int]\n    :param param_str: Description of the parameter.\n    :type param_str: Optional[str]\n    :param param_dict: Description of the parameter.\n    :type param_dict: Optional[Dict[str, str]]\n    \"\"\"\n    def __init__(\n        client: PFClient,\n        param_int: Optional[int] = None,\n        param_str: Optional[str] = None,\n        param_dict: Optional[Dict[str, str]] = None,\n    ) -> None:\n        \"\"\"No docstring for __init__, it should be written in class definition above.\"\"\"\n        ...\n\n\n```\n\n**Notes**:\n\n1. One-line summary is required. It should be clear and concise.\n2. Detailed explanation is encouraged but not required. This part may or may not include notes, admonitions and code blocks.\n    - The format like `.. note::` is called `directive`. Directives are a mechanism to extend the content of reStructuredText. Every directive declares a block of content with specific role. Start a new line with `.. directive_name::` to use the directive. \n    - The directives used in the sample(`note/admonition/code-block`) should be enough for basic usage of docstring in our project. But you are welcomed to explore more Directives.\n3. Parameter description and type is required.\n    - A pair of `:param [ParamName]:` and `:type [ParamName]:` is required.\n    - If the type is a promptflow public class, use the `full path to the class` and prepend it with a \"~\". This will create a link when the documentation is rendered on the doc site that will take the user to the class reference documentation for more information.\n        ```text\n        :param client: Descrition of the client.\n        :type client: ~promptflow.PFClient\n        ```\n    - Use `Union/Optional` when appropriate in function declaration. And use the same annotaion after `:type [ParamName]:`\n        ```text\n        :type param_int: Optional[int]\n        ```\n4. For classes, include docstring in definition only. If you include a docstring in both the class definition and the constructor (init method) docstrings, it will show up twice in the reference docs.\n5. Constructors (def `__init__`) should return `None`, per PEP 484 standards.\n6. To create a link for promptflow class on our doc site. `~promptflow.xxx.MyClass` alone only works after `:type [ParamName]` and `:rtype:`. If you want to achieve the same effect in docstring summary, you should use it with `:class:`:\n     ```python\n     \"\"\"\n     An example to achieve link effect in summary for :class:`~promptflow.xxx.MyClass`\n     For function, use :meth:`~promptflow.xxx.my_func`\n     \"\"\"\n     ```\n\n7. There are some tricks to highlight the content in your docstring:\n    - Single backticks (`): Single backticks are used to represent inline code elements within the text. It is typically used to highlight function names, variable names, or any other code elements within the documentation.\n    - Double backticks(``): Double backticks are typically used to highlight a literal value.\n\n8. If there are any class level constants you don't want to expose to doc site, make sure to add `_` in front of the constant to hide it."}
{"text_chunk": "Write function docstring\n\n```python\nfrom typing import Optional\n\ndef my_method(param_int: Optional[int] = None) -> int:\n    \"\"\"One-line summary\n\n    Detailed explanations.\n\n    :param param_int: Description of the parameter.\n    :type param_int: int\n    :raises [ErrorType1]: [ErrorDescription1]\n    :raises [ErrorType2]: [ErrorDescription2]\n    :return: Description of the return value.\n    :rtype: int\n    \"\"\"\n    ...\n```\n\nIn addition to `class docstring` notes:\n\n1. Function docstring should include return values.\n    - If return type is promptflow class, we should also use `~promptflow.xxx.[ClassName]`.\n2. Function docstring should include exceptions that may be raised in this function.\n    - If exception type is `PromptflowException`, use `~promptflow.xxx.[ExceptionName]`\n    - If multiple exceptions are raised, just add new lines of `:raises`, see the example above."}
{"text_chunk": "How to build doc site locally\n\nYou can build the documentation site locally to preview the final effect of your docstring on the rendered site. This will provide you with a clear understanding of how your docstring will appear on our site once your changes are merged into the main branch.\n\n1. Setup your dev environment, see dev_setup for details. Sphinx will load all source code to process docstring.\n    - Skip this step if you just want to build the doc site without reference doc, but do remove `-WithReferenceDoc` from the command in step 3.\n2. Install `langchain` package since it is used in our code but not covered in `dev_setup`.\n3. Open a `powershell`, activate the conda env and navigate to `/scripts/docs` , run `doc_generation.ps1`:\n    ```pwsh\n    cd scripts\\docs\n    .\\doc_generation.ps1 -WithReferenceDoc -WarningAsError\n    ```\n    - For the first time you execute this command, it will take some time to install `sphinx` dependencies. After the initial installation, next time you can add param `-SkipInstall` to above command to save some time for dependency check.\n4. Check warnings/errors in the build log, fix them if any, then build again.\n5. Open `scripts/docs/_build/index.html` to preview the local doc site."}
{"text_chunk": "Additional comments\n\n- **Utilities**: The autoDocstring VSCode extension or GitHub Copilot can help autocomplete in this style for you.\n\n- **Advanced principles**\n  - Accuracy: The documentation accurately reflects the features and functionality of the product.\n  - Completeness: The documentation covers all relevant features and functionality of the product.\n  - Demonstration: Every docstring should include an up-to-date code snippet that demonstrates how to use the product effectively."}
{"text_chunk": "References\n\n- AzureML v2 Reference Documentation Guide\n- Azure SDK for Python documentation guidelines\n- How to document a Python API"}
{"text_chunk": "Replay end-to-end tests\n\n* This document introduces replay tests for those located in sdk_cli_azure_test and sdk_cli_test.\n* The primary purpose of replay tests is to avoid the need for credentials, Azure workspaces, OpenAI tokens, and to directly test prompt flow behavior.\n* Although there are different techniques behind recording/replaying, there are some common steps to run the tests in replay mode.\n* The key handle of replay tests is the environment variable `PROMPT_FLOW_TEST_MODE`."}
{"text_chunk": "How to run tests in replay mode\n\nAfter cloning the full repo and setting up the proper test environment following dev_setup.md, run the following command in the root directory of the repo:\n\n1. If you have changed/affected tests in __sdk_cli_test__ : Copy or rename the file dev-connections.json.example to `connections.json` in the same folder.\n2. In your Python environment, set the environment variable `PROMPT_FLOW_TEST_MODE` to `'replay'` and run the test(s).\n\nThese tests should work properly without any real connection settings."}
{"text_chunk": "Test modes\n\nThere are 3 representative values of the environment variable `PROMPT_FLOW_TEST_MODE`\n- `live`: Tests run against the real backend, which is the way traditional end-to-end tests do.\n- `record`: Tests run against the real backend, and network traffic will be sanitized (filter sensitive and unnecessary requests/responses) and recorded to local files (recordings).\n- `replay`: There is no real network traffic between SDK/CLI and the backend, tests run against local recordings."}
{"text_chunk": "Update test recordings\n\nTo record a test, don\u2019t forget to clone the full repo and set up the proper test environment following dev_setup.md:\n1. Prepare some data.\n   * If you have changed/affected tests in __sdk_cli_test__: Copy or rename the file dev-connections.json.example to `connections.json` in the same folder.\n   * If you have changed/affected tests in __sdk_cli_azure_test__: prepare your Azure ML workspace, make sure your Azure CLI logged in, and set the environment variable `PROMPT_FLOW_SUBSCRIPTION_ID`, `PROMPT_FLOW_RESOURCE_GROUP_NAME`, `PROMPT_FLOW_WORKSPACE_NAME` and `PROMPT_FLOW_RUNTIME_NAME` (if needed) pointing to your workspace.\n2. Record the test.\n   * Specify the environment variable `PROMPT_FLOW_TEST_MODE` to `'record'`. If you have a `.env` file, we recommend specifying it there. Here is an example .env file. Then, just run the test that you want to record.\n3. Once the test completed.\n   * If you have changed/affected tests in __sdk_cli_azure_test__: There should be one new YAML file located in `src/promptflow/tests/test_configs/recordings/`, containing the network traffic of the test.\n   * If you have changed/affected tests in __sdk_cli_test__: There may be changes in the folder `src/promptflow/tests/test_configs/node_recordings/`.  Don\u2019t worry if there are no changes, because similar LLM calls may have been recorded before."}
{"text_chunk": "Techniques behind replay test"}
{"text_chunk": "Sdk_cli_azure_test\n\nEnd-to-end tests for pfazure aim to test the behavior of the PromptFlow SDK/CLI as it interacts with the service. This process can be time-consuming, error-prone, and require credentials (which are unavailable to pull requests from forked repositories); all of these go against our intention for a smooth development experience.\n\nTherefore, we introduce replay tests, which leverage VCR.py to record all required network traffic to local files and replay during tests. In this way, we avoid the need for credentials, speed up, and stabilize the test process."}
{"text_chunk": "Sdk_cli_test\n\nsdk_cli_test often doesn\u2019t use a real backend. It will directly invokes LLM calls from localhost. Thus the key target of replay tests is to avoid the need for OpenAI tokens. If you have OpenAI / Azure OpenAI tokens yourself, you can try recording the tests. Record Storage will not record your own LLM connection, but only the inputs and outputs of the LLM calls.\n\nThere are also limitations. Currently, recorded calls are:\n* AzureOpenAI calls\n* OpenAI calls\n* tool name \"fetch_text_content_from_url\" and tool name \"my_python_tool\""}
{"text_chunk": "Add conditional control to a flow\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nIn prompt flow, we support control logic by activate config, like if-else, switch. Activate config enables conditional execution of nodes within your flow, ensuring that specific actions are taken only when the specified conditions are met.\n\nThis guide will help you learn how to use activate config to add conditional control to your flow."}
{"text_chunk": "Prerequisites\n\nPlease ensure that your promptflow version is greater than `0.1.0b5`."}
{"text_chunk": "Usage\n\nEach node in your flow can have an associated activate config, specifying when it should execute and when it should bypass. If a node has activate config, it will only be executed when the activate condition is met. The configuration consists of two essential components:\n- `activate.when`: The condition that triggers the execution of the node. It can be based on the outputs of a previous node, or the inputs of the flow.\n- `activate.is`: The condition's value, which can be a constant value of string, boolean, integer, double.\n\nYou can manually change the flow.dag.yaml in the flow folder or use the visual editor in VS Code Extension to add activate config to nodes in the flow.\n\n::::{tab-set}\n:::{tab-item} YAML\n:sync: YAML\n\nYou can add activate config in the node section of flow yaml.\n```yaml\nactivate:\n  when: ${node.output}\n  is: true\n```\n\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\n- Click `Visual editor` in the flow.dag.yaml to enter the flow interface.\n!visual_editor\n\n- Click on the `Activation config` section in the node you want to add and fill in the values for \"when\" and \"is\".\n!activate_config\n\n:::\n\n::::"}
{"text_chunk": "Further details and important notes\n1. If the node using the python tool has an input that references a node that may be bypassed, please provide a default value for this input whenever possible. If there is no default value for input, the output of the bypassed node will be set to None.\n\n    !provide_default_value\n\n2. It is not recommended to directly connect nodes that might be bypassed to the flow's outputs. If it is connected, the output will be None and a warning will be raised.\n\n    !output_bypassed\n\n3. In a conditional flow, if a node has activate config, we will always use this config to determine whether the node should be bypassed. If a node is bypassed, its status will be marked as \"Bypassed\", as shown in the figure below Show. There are three situations in which a node is bypassed.\n\n    !bypassed_nodes\n\n\n    (1) If a node has activate config and the value of `activate.when` is not equals to `activate.is`, it will be bypassed. If you want to fore a node to always be executed, you can set the activate config to `when dummy is dummy` which always meets the activate condition.\n\n    !activate_condition_always_met\n\n    (2) If a node has activate config and the node pointed to by `activate.when` is bypassed, it will be bypassed.\n\n    !activate_when_bypassed\n\n    (3) If a node does not have activate config but depends on other nodes that have been bypassed, it will be bypassed.\n\n    !dependencies_bypassed"}
{"text_chunk": "Example flow\n\nLet's illustrate how to use activate config with practical examples.\n\n- If-Else scenario: Learn how to develop a conditional flow for if-else scenarios. View Example\n- Switch scenario: Explore conditional flow for switch scenarios. View Example"}
{"text_chunk": "Next steps\n\n- Run and evaluate a flow"}
{"text_chunk": "Deploy a flow using development server\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nOnce you have created and thoroughly tested a flow, you can use it as an HTTP endpoint.\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nWe are going to use the web-classification as\nan example to show how to deploy a flow.\n\nPlease ensure you have create the connection required by flow, if not, you could\nrefer to Setup connection for web-classification.\n\nNote: We will use relevant environment variable ({connection_name}_{key_name}) to override connection configurations in \nserving mode, white space in connection name will be removed directly from environment variable name. For instance, \nif there is a custom connection named 'custom_connection' with a configuration key called 'chat_deployment_name,' the \nfunction will attempt to retrieve 'chat_deployment_name' from the environment variable \n'CUSTOM_CONNECTION_CHAT_DEPLOYMENT_NAME' by default. If the environment variable is not set, it will use the original \nvalue as a fallback.\n\n\nThe following CLI commands allows you serve a flow folder as an endpoint. By running this command, a flask app will start in the environment where command is executed, please ensure all prerequisites required by flow have been installed.\n```bash"}
{"text_chunk": "Serve the flow at localhost:8080\npf flow serve --source  --port 8080 --host localhost\n```\n\nThe expected result is as follows if the flow served successfully, and the process will keep alive until it be killed manually.\n\n!img\n:::\n:::{tab-item} VS Code Extension\n:sync: VSC\nIn visual editor, choose:\n!img\nthen choose format:\n!img\nthen in yaml editor:\n!img\n:::\n::::"}
{"text_chunk": "Test endpoint\n::::{tab-set}\n:::{tab-item} Bash\nYou could open another terminal to test the endpoint with the following command:\n```bash\ncurl http://localhost:8080/score --data '{\"url\":\"https://play.google.com/store/apps/details?id=com.twitter.android\"}' -X POST  -H \"Content-Type: application/json\"\n```\n:::\n:::{tab-item} PowerShell\nYou could open another terminal to test the endpoint with the following command:\n```powershell\nInvoke-WebRequest -URI http://localhost:8080/score -Body '{\"url\":\"https://play.google.com/store/apps/details?id=com.twitter.android\"}' -Method POST  -ContentType \"application/json\"\n```\n:::\n:::{tab-item} Test Page\nThe development server has a built-in web page you can use to test the flow. Open 'http://localhost:8080' in your browser.\n!img\n:::\n::::"}
{"text_chunk": "Next steps\n- Try the example here.\n- See how to deploy a flow using docker.\n- See how to deploy a flow using kubernetes."}
{"text_chunk": "Deploy a flow using Docker\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nThere are two steps to deploy a flow using docker:\n1. Build the flow as docker format.\n2. Build and run the docker image."}
{"text_chunk": "Build a flow as docker format\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nUse the command below to build a flow as docker format:\n```bash\npf flow build --source  --output  --format docker\n```\n:::\n:::{tab-item} VS Code Extension\n:sync: VSC\n\nClick the button below to build a flow as docker format:\n!img\n:::\n::::\n\nNote that all dependent connections must be created before exporting as docker."}
{"text_chunk": "Docker format folder structure\n\nExported Dockerfile & its dependencies are located in the same folder. The structure is as below:\n- flow: the folder contains all the flow files\n  - ...\n- connections: the folder contains yaml files to create all related connections\n  - ...\n- Dockerfile: the dockerfile to build the image\n- start.sh: the script used in `CMD` of `Dockerfile` to start the service\n- runit: the folder contains all the runit scripts\n  - ...\n- settings.json: a json file to store the settings of the docker image\n- README.md: Simple introduction of the files"}
{"text_chunk": "Deploy with Docker\nWe are going to use the web-classification as\nan example to show how to deploy with docker.\n\nPlease ensure you have create the connection required by flow, if not, you could\nrefer to Setup connection for web-classification."}
{"text_chunk": "Build a flow as docker format app\n\nUse the command below to build a flow as docker format app:\n\n```bash\npf flow build --source ../../flows/standard/web-classification --output dist --format docker\n```\n\nNote that all dependent connections must be created before exporting as docker."}
{"text_chunk": "Build Docker image\n\nLike other Dockerfile, you need to build the image first. You can tag the image with any name you want. In this example, we use `promptflow-serve`.\n\nRun the command below to build image:\n\n```bash\ndocker build dist -t web-classification-serve\n```"}
{"text_chunk": "Run Docker image\n\nRun the docker image will start a service to serve the flow inside the container."}
{"text_chunk": "Connections\nIf the service involves connections, all related connections will be exported as yaml files and recreated in containers.\nSecrets in connections won't be exported directly. Instead, we will export them as a reference to environment variables:\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/OpenAIConnection.schema.json\ntype: open_ai\nname: open_ai_connection\nmodule: promptflow.connections\napi_key: ${env:OPEN_AI_CONNECTION_API_KEY} # env reference\n```\nYou'll need to set up the environment variables in the container to make the connections work."}
{"text_chunk": "Run with `docker run`\n\nYou can run the docker image directly set via below commands:\n```bash"}
{"text_chunk": "The started service will listen on port 8080.You can map the port to any port on the host machine as you want.\ndocker run -p 8080:8080 -e OPEN_AI_CONNECTION_API_KEY= web-classification-serve\n```"}
{"text_chunk": "Test the endpoint\nAfter start the service, you can use curl to test it:\n\n```bash\ncurl http://localhost:8080/score --data '{\"url\":\"https://play.google.com/store/apps/details?id=com.twitter.android\"}' -X POST  -H \"Content-Type: application/json\"\n```"}
{"text_chunk": "Next steps\n- Try the example here.\n- See how to deploy a flow using kubernetes."}
{"text_chunk": "Deploy a flow using Kubernetes\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nThere are four steps to deploy a flow using Kubernetes:\n1. Build the flow as docker format.\n2. Build the docker image.\n3. Create Kubernetes deployment yaml.\n4. Apply the deployment."}
{"text_chunk": "Build a flow as docker format\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nNote that all dependent connections must be created before building as docker.\n```bash"}
{"text_chunk": "create connection if not created before\npf connection create --file ../../../examples/connections/azure_openai.yml --set api_key= api_base= --name open_ai_connection\n```\n\nUse the command below to build a flow as docker format:\n```bash\npf flow build --source  --output  --format docker\n```\n:::\n:::{tab-item} VS Code Extension\n:sync: VSC\n\nClick the button below to build a flow as docker format:\n!img\n:::\n::::\n\nNote that all dependent connections must be created before exporting as docker."}
{"text_chunk": "Docker format folder structure\n\nExported Dockerfile & its dependencies are located in the same folder. The structure is as below:\n- flow: the folder contains all the flow files\n  - ...\n- connections: the folder contains yaml files to create all related connections\n  - ...\n- Dockerfile: the dockerfile to build the image\n- start.sh: the script used in `CMD` of `Dockerfile` to start the service\n- runit: the folder contains all the runit scripts\n  - ...\n- settings.json: a json file to store the settings of the docker image\n- README.md: Simple introduction of the files"}
{"text_chunk": "Deploy with Kubernetes\nWe are going to use the web-classification as\nan example to show how to deploy with Kubernetes.\n\nPlease ensure you have create the connection required by flow, if not, you could\nrefer to Setup connection for web-classification.\n\nAdditionally, please ensure that you have installed all the required dependencies. You can refer to the \"Prerequisites\" section in the README of the web-classification for a comprehensive list of prerequisites and installation instructions."}
{"text_chunk": "Build Docker image\n\nLike other Dockerfile, you need to build the image first. You can tag the image with any name you want. In this example, we use `web-classification-serve`.\n\nThen run the command below:\n\n```bash\ncd \ndocker build . -t web-classification-serve\n```"}
{"text_chunk": "Create Kubernetes deployment yaml.\nThe Kubernetes deployment yaml file acts as a guide for managing your docker container in a Kubernetes pod. It clearly specifies important information like the container image, port configurations, environment variables, and various settings. Below, you'll find a simple deployment template that you can easily customize to meet your needs.\n\n**Note**: You need encode the secret using base64 firstly and input the  as 'open-ai-connection-api-key' in the deployment configuration. For example, you can run below commands in linux:\n```bash\nencoded_secret=$(echo -n  | base64)\n```\n\n```yaml\n---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: \n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: open-ai-connection-api-key\n  namespace: \ntype: Opaque\ndata:\n  open-ai-connection-api-key: \n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-classification-service\n  namespace: \nspec:\n  type: NodePort\n  ports:\n  - name: http\n    port: 8080\n    targetPort: 8080\n    nodePort: 30123\n  selector:\n    app: web-classification-serve-app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-classification-serve-app\n  namespace: \nspec:\n  selector:\n    matchLabels:\n      app: web-classification-serve-app\n  template:\n    metadata:\n      labels:\n        app: web-classification-serve-app\n    spec:\n      containers:\n      - name: web-classification-serve-container\n        image: \n        imagePullPolicy: Never\n        ports:\n        - containerPort: 8080\n        env:\n        - name: OPEN_AI_CONNECTION_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: open-ai-connection-api-key\n              key: open-ai-connection-api-key\n```"}
{"text_chunk": "Apply the deployment.\nBefore you can deploy your application, ensure that you have set up a Kubernetes cluster and installed kubectl if it's not already installed. In this documentation, we will use Minikube as an example. To start the cluster, execute the following command:\n```bash\nminikube start\n```\nOnce your Kubernetes cluster is up and running, you can proceed to deploy your application by using the following command:\n```bash\nkubectl apply -f deployment.yaml\n```\nThis command will create the necessary pods to run your application within the cluster.\n\n**Note**: You need replace  below with your specific pod_name. You can retrieve it by running `kubectl get pods -n web-classification`."}
{"text_chunk": "Retrieve flow service logs of the container\nThe kubectl logs command is used to retrieve the logs of a container running within a pod, which can be useful for debugging, monitoring, and troubleshooting applications deployed in a Kubernetes cluster.\n\n```bash\nkubectl -n  logs \n```"}
{"text_chunk": "Connections\nIf the service involves connections, all related connections will be exported as yaml files and recreated in containers.\nSecrets in connections won't be exported directly. Instead, we will export them as a reference to environment variables:\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/OpenAIConnection.schema.json\ntype: open_ai\nname: open_ai_connection\nmodule: promptflow.connections\napi_key: ${env:OPEN_AI_CONNECTION_API_KEY} # env reference\n```\nYou'll need to set up the environment variables in the container to make the connections work."}
{"text_chunk": "Test the endpoint\n- Option1:\n\n  Once you've started the service, you can establish a connection between a local port and a port on the pod. This allows you to conveniently test the endpoint from your local terminal.\n  To achieve this, execute the following command:\n\n  ```bash\n  kubectl port-forward  : -n \n  ```\n  With the port forwarding in place, you can use the curl command to initiate the endpoint test:\n\n  ```bash\n  curl http://localhost:/score --data '{\"url\":\"https://play.google.com/store/apps/details?id=com.twitter.android\"}' -X POST  -H \"Content-Type: application/json\"\n  ```\n\n- Option2:\n\n  `minikube service web-classification-service --url -n ` runs as a process, creating a tunnel to the cluster. The command exposes the service directly to any program running on the host operating system.\n\n  The command above will retrieve the URL of a service running within a Minikube Kubernetes cluster (e.g. http://:), which you can click to interact with the flow service in your web browser. Alternatively, you can use the following command to test the endpoint: \n\n    **Note**: Minikube will use its own external port instead of nodePort to listen to the service. So please substitute  with the port obtained above.\n\n    ```bash\n  curl http://localhost:/score --data '{\"url\":\"https://play.google.com/store/apps/details?id=com.twitter.android\"}' -X POST  -H \"Content-Type: application/json\"\n  ```"}
{"text_chunk": "Next steps\n- Try the example here."}
{"text_chunk": "Distribute flow as executable app\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nWe are going to use the web-classification as\nan example to show how to distribute flow as executable app with Pyinstaller.\n\n\nPlease ensure that you have installed all the required dependencies. You can refer to the \"Prerequisites\" section in the README of the web-classification for a comprehensive list of prerequisites and installation instructions. And we recommend you to add a `requirements.txt` to indicate all the required dependencies for each flow. \n\nPyinstaller is a popular tool used for converting Python applications into standalone executables. It allows you to package your Python scripts into a single executable file, which can be run on a target machine without requiring the Python interpreter to be installed.\nStreamlit is an open-source Python library used for creating web applications quickly and easily. It's designed for data scientists and engineers who want to turn data scripts into shareable web apps with minimal effort.\nWe use Pyinstaller to package the flow and Streamlit to create custom web apps. Prior to distributing the workflow, kindly ensure that you have installed them."}
{"text_chunk": "Build a flow as executable format\nNote that all dependent connections must be created before building as executable.\n```bash"}
{"text_chunk": "create connection if not created before\npf connection create --file ../../../examples/connections/azure_openai.yml --set api_key= api_base= --name open_ai_connection\n```\n\nUse the command below to build a flow as executable format:\n```bash\npf flow build --source  --output  --format executable\n```"}
{"text_chunk": "Executable format folder structure\n\nExported files & its dependencies are located in the same folder. The structure is as below:\n- flow: the folder contains all the flow files.\n- connections: the folder contains yaml files to create all related connections.\n- app.py: the entry file is included as the entry point for the bundled application.\n- app.spec: the spec file tells PyInstaller how to process your script.\n- main.py: it will start streamlit service and be called by the entry file.\n- settings.json: a json file to store the settings of the executable application.\n- build: a folder contains various log and working files.\n- dist: a folder contains the executable application.\n- README.md: Simple introduction of the files."}
{"text_chunk": "A template script of the entry file\nPyInstaller reads a spec file or Python script written by you. It analyzes your code to discover every other module and library your script needs in order to execute. Then it collects copies of all those files, including the active Python interpreter, and puts them with your script in a single folder, or optionally in a single executable file. \n\nWe provide a Python entry script named `app.py` as the entry point for the bundled app, which enables you to serve a flow folder as an endpoint.\n\n```python\nimport os\nimport sys\n\nfrom promptflow._cli._pf._connection import create_connection\nfrom streamlit.web import cli as st_cli\nfrom streamlit.runtime import exists\n\nfrom main import start\n\ndef is_yaml_file(file_path):\n    _, file_extension = os.path.splitext(file_path)\n    return file_extension.lower() in ('.yaml', '.yml')\n\ndef create_connections(directory_path) -> None:\n    for root, dirs, files in os.walk(directory_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if is_yaml_file(file_path):\n                create_connection(file_path)\n\n\nif __name__ == \"__main__\":\n    create_connections(os.path.join(os.path.dirname(__file__), \"connections\"))\n    if exists():\n        start()\n    else:\n        main_script = os.path.join(os.path.dirname(__file__), \"main.py\")\n        sys.argv = [\"streamlit\", \"run\", main_script, \"--global.developmentMode=false\"]\n        st_cli.main(prog_name=\"streamlit\")\n\n```"}
{"text_chunk": "A template script of the spec file\nThe spec file tells PyInstaller how to process your script. It encodes the script names and most of the options you give to the pyinstaller command. The spec file is actually executable Python code. PyInstaller builds the app by executing the contents of the spec file.\n\nTo streamline this process, we offer a `app.spec` spec file that bundles the application into a single file. For additional information on spec files, you can refer to the Using Spec Files. Please replace `streamlit_runtime_interpreter_path` with the path of streamlit runtime interpreter in your environment.\n\n```spec"}
{"text_chunk": "-*- mode: python ; coding: utf-8 -*-\nfrom PyInstaller.utils.hooks import collect_data_files\nfrom PyInstaller.utils.hooks import copy_metadata\n\ndatas = [('connections', 'connections'), ('flow', 'flow'), ('settings.json', '.'), ('main.py', '.'), ('{{streamlit_runtime_interpreter_path}}', './streamlit/runtime')]\ndatas += collect_data_files('streamlit')\ndatas += copy_metadata('streamlit')\ndatas += collect_data_files('keyrings.alt', include_py_files=True)\ndatas += copy_metadata('keyrings.alt')\ndatas += collect_data_files('streamlit_quill')\n\nblock_cipher = None\n\n\na = Analysis(\n    ['app.py', 'main.py'],\n    pathex=[],\n    binaries=[],\n    datas=datas,\n    hiddenimports=['bs4'],\n    hookspath=[],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    [],\n    name='app',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    upx_exclude=[],\n    runtime_tmpdir=None,\n    console=True,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\n```"}
{"text_chunk": "The bundled application using Pyinstaller\nOnce you've build a flow as executable format following Build a flow as executable format.\nIt will create two folders named `build` and `dist` within your specified output directory, denoted as . The `build` folder houses various log and working files, while the `dist` folder contains the `app` executable application."}
{"text_chunk": "Connections\nIf the service involves connections, all related connections will be exported as yaml files and recreated in the executable package.\nSecrets in connections won't be exported directly. Instead, we will export them as a reference to environment variables:\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/OpenAIConnection.schema.json\ntype: open_ai\nname: open_ai_connection\nmodule: promptflow.connections\napi_key: ${env:OPEN_AI_CONNECTION_API_KEY} # env reference\n```"}
{"text_chunk": "Test the endpoint\nFinally, You can distribute the bundled application `app` to other people. They can execute your program by double clicking the executable file, e.g. `app.exe` in Windows system or running the binary file, e.g. `app` in Linux system. \n\nThe development server has a built-in web page they can use to test the flow by opening 'http://localhost:8501' in the browser. The expected result is as follows: if the flow served successfully, the process will keep alive until it is killed manually.\n\nTo your users, the app is self-contained. They do not need to install any particular version of Python or any modules. They do not need to have Python installed at all.\n\n**Note**: The executable generated is not cross-platform. One platform (e.g. Windows) packaged executable can't run on others (Mac, Linux)."}
{"text_chunk": "Known issues\n1. Note that Python 3.10.0 contains a bug making it unsupportable by PyInstaller. PyInstaller will also not work with beta releases of Python 3.13."}
{"text_chunk": "Next steps\n- Try the example here"}
{"text_chunk": "Develop chat flow\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nFrom this document, you can learn how to develop a chat flow by writing a flow yaml from scratch. You can \nfind additional information about flow yaml schema in Flow YAML Schema."}
{"text_chunk": "Flow input data\n\nThe most important elements that differentiate a chat flow from a standard flow are **chat input** and **chat history**. A chat flow can have multiple inputs, but **chat history** and **chat input** are required inputs in chat flow.\n\n- **Chat Input**: Chat input refers to the messages or queries submitted by users to the chatbot. Effectively handling chat input is crucial for a successful conversation, as it involves understanding user intentions, extracting relevant information, and triggering appropriate responses.\n\n- **Chat History**: Chat history is the record of all interactions between the user and the chatbot, including both user inputs and AI-generated outputs. Maintaining chat history is essential for keeping track of the conversation context and ensuring the AI can generate contextually relevant responses. Chat history is a special type of chat flow input, that stores chat messages in a structured format.\n    \n    An example of chat history:\n    ```python\n    [\n      {\"inputs\": {\"question\": \"What types of container software there are?\"}, \"outputs\": {\"answer\": \"There are several types of container software available, including: Docker, Kubernetes\"}},\n      {\"inputs\": {\"question\": \"What's the different between them?\"}, \"outputs\": {\"answer\": \"The main difference between the various container software systems is their functionality and purpose. Here are some key differences between them...\"}},\n    ] \n    ```\n\nYou can set **is_chat_input**/**is_chat_history** to **true** to add chat_input/chat_history to the chat flow.\n```yaml\ninputs:\n  chat_history:\n    type: list\n    is_chat_history: true\n    default: []\n  question:\n    type: string\n    is_chat_input: true\n    default: What is ChatGPT?\n```\n\n\nFor more information see develop the flow using different tools."}
{"text_chunk": "Develop the flow using different tools\nIn one flow, you can consume different kinds of tools. We now support built-in tool like \nLLM, Python and \nPrompt and \nthird-party tool like Serp API, \nVector Search, etc.\n\nFor more information see develop the flow using different tools."}
{"text_chunk": "Chain your flow - link nodes together\nBefore linking nodes together, you need to define and expose an interface.\n\nFor more information see chain your flow."}
{"text_chunk": "Set flow output\n\n**Chat output** is required output in the chat flow. It refers to the AI-generated messages that are sent to the user in response to their inputs. Generating contextually appropriate and engaging chat outputs is vital for a positive user experience.\n\nYou can set **is_chat_output** to **true** to add chat_output to the chat flow.\n\n```yaml\noutputs:\n  answer:\n    type: string\n    reference: ${chat.output}\n    is_chat_output: true\n```"}
{"text_chunk": "Develop evaluation flow\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nThe evaluation flow is a flow to test/evaluate the quality of your LLM application (standard/chat flow). It usually runs on the outputs of standard/chat flow, and compute key metrics that can be used to determine whether the standard/chat flow performs well. See Flows for more information.\n\nBefore proceeding with this document, it is important to have a good understanding of the standard flow. Please make sure you have read Develop standard flow, since they share many common features and these features won't be repeated in this doc, such as:\n- `Inputs/Outputs definition`\n- `Nodes`\n- `Chain nodes in a flow`\n\nWhile the evaluation flow shares similarities with the standard flow, there are some important differences that set it apart. The main distinctions are as follows:\n- `Inputs from an existing run`: The evaluation flow contains inputs that are derived from the outputs of the standard/chat flow. These inputs are used for evaluation purposes.\n- `Aggregation node`: The evaluation flow contains one or more aggregation nodes, where the actual evaluation takes place. These nodes are responsible for computing metrics and determining the performance of the standard/chat flow."}
{"text_chunk": "Evaluation flow example\n\nIn this guide, we use eval-classification-accuracy flow as an example of the evaluation flow. This is a flow illustrating how to evaluate the performance of a classification flow. It involves comparing each prediction to the groundtruth and assigns a `Correct` or `Incorrect` grade, and aggregating the results to produce metrics such as `accuracy`, which reflects how good the system is at classifying the data."}
{"text_chunk": "Flow inputs\n\nThe flow `eval-classification-accuracy` contains two inputs:\n\n```yaml\ninputs:\n  groundtruth:\n    type: string\n    description: Groundtruth of the original question, it's the correct label that you hope your standard flow could predict.\n    default: APP\n  prediction:\n    type: string\n    description: The actual predicted outputs that your flow produces.\n    default: APP\n```\n\nAs evident from the inputs description, the evaluation flow requires two specific inputs: \n- `groundtruth`: This input represents the actual or expected values against which the performance of the standard/chat flow will be evaluated.\n- `prediction`: The prediction input is derived from the outputs of another standard/chat flow. It contains the predicted values generated by the standard/chat flow, which will be compared to the groundtruth values during the evaluation process.\n\nFrom the definition perspective, there is no difference compared with adding an input/output in a `standard/chat flow`. However when running an evaluation flow, you may need to specify the data source from both data file and flow run outputs. For more details please refer to Run and evaluate a flow."}
{"text_chunk": "Aggregation node\n\n\nBefore introducing the aggregation node, let's see what a regular node looks like, we use node `grade` in the example flow for instance:\n\n```yaml\n- name: grade\n  type: python\n  source:\n    type: code\n    path: grade.py\n  inputs:\n    groundtruth: ${inputs.groundtruth}\n    prediction: ${inputs.prediction}\n```\n\nIt takes both `groundtruth` and `prediction` from the flow inputs, compare them in the source code to see if they match:\n\n```python\nfrom promptflow import tool\n\n@tool\ndef grade(groundtruth: str, prediction: str):\n    return \"Correct\" if groundtruth.lower() == prediction.lower() else \"Incorrect\"\n```\n\nWhen it comes to an `aggregation node`, there are two key distinctions that set it apart from a regular node:\n1. It has an attribute `aggregation` set to be `true`.\n\n```yaml\n- name: calculate_accuracy\n  type: python\n  source:\n    type: code\n    path: calculate_accuracy.py\n  inputs:\n    grades: ${grade.output}\n  aggregation: true  # Add this attribute to make it an aggregation node\n```\n\n2. Its source code accepts a `List` type parameter which is a collection of the previous regular node's outputs.\n\n```python\nfrom typing import List\nfrom promptflow import log_metric, tool\n\n@tool\ndef calculate_accuracy(grades: List[str]):\n    result = []\n    for index in range(len(grades)):\n        grade = grades[index]\n        result.append(grade)\n\n    # calculate accuracy for each variant\n    accuracy = round((result.count(\"Correct\") / len(result)), 2)\n    log_metric(\"accuracy\", accuracy)\n\n    return result\n```\n\nThe parameter `grades` in above function, contains all results that are produced by the regular node `grade`. Assuming the referred standard flow run has 3 outputs:\n\n```json\n{\"prediction\": \"App\"}\n{\"prediction\": \"Channel\"}\n{\"prediction\": \"Academic\"}\n```\n\n\n And we provides a data file like this:\n ```json\n{\"groundtruth\": \"App\"}\n{\"groundtruth\": \"Channel\"}\n{\"groundtruth\": \"Wiki\"}\n```\n\nThen the `grades` value would be `[\"Correct\", \"Correct\", \"Incorrect\"]`, and the final accuracy is `0.67`. \n\nThis example provides a straightforward demonstration of how to evaluate the classification flow. Once you have a solid understanding of the evaluation mechanism, you can customize and design your own evaluation method to suit your specific needs."}
{"text_chunk": "More about the list parameter\n\nWhat if the number of referred standard flow run outputs does not match the provided data file? We know that a standard flow can be executed against multiple line data and some of them could fail while others succeed. Consider the same standard flow run mentioned in above example but the `2nd` line run has failed, thus we have below run outputs:\n\n\n```json\n{\"prediction\": \"App\"}\n{\"prediction\": \"Academic\"}\n```\n\nThe promptflow flow executor has the capability to recognize the index of the referred run's outputs and extract the corresponding data from the provided data file. This means that during the execution process, even if the same data file is provided(3 lines), only the specific data mentioned below will be processed:\n\n ```json\n{\"groundtruth\": \"App\"}\n{\"groundtruth\": \"Wiki\"}\n```\n\nIn this case, the `grades` value would be `[\"Correct\", \"Incorrect\"]` and the accuracy is `0.5`."}
{"text_chunk": "How to set aggregation node in VS Code Extention\n\n\n!img"}
{"text_chunk": "How to log metrics\n:::{admonition} Limitation\nYou can only log metrics in an `aggregation node`, otherwise the metric will be ignored.\n:::\nPromptflow supports logging and tracking experiments using `log_metric` function. A metric is a key-value pair that records a single float measure. In a python node, you can log a metric with below code: \n\n\n```python\nfrom typing import List\nfrom promptflow import log_metric, tool\n\n@tool\ndef example_log_metrics(grades: List[str]):\n    # this node is an aggregation node so it accepts a list of grades\n    metric_key = \"accuracy\"\n    metric_value = round((grades.count(\"Correct\") / len(result)), 2)\n    log_metric(metric_key, metric_value)\n```\n\nAfter the run is completed, you can run `pf run show-metrics -n ` to see the metrics.\n\n!img"}
{"text_chunk": "Develop standard flow\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nFrom this document, you can learn how to develop a standard flow by writing a flow yaml from scratch. You can \nfind additional information about flow yaml schema in Flow YAML Schema."}
{"text_chunk": "Flow input data\nThe flow input data is the data that you want to process in your flow. \n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nYou can add a flow input in inputs section of flow yaml.\n```yaml\ninputs:\n  url:\n    type: string\n    default: https://www.microsoft.com/en-us/d/xbox-wireless-controller-stellar-shift-special-edition/94fbjc7h0h6h\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\nWhen unfolding Inputs section in the authoring page, you can set and view your flow inputs, including input schema (name and type), \nand the input value.\n\n!flow_input\n:::\n\n::::\nFor Web Classification sample as shown the screenshot above, the flow input is an url of string type.\nFor more input types in a python tool, please refer to Input types."}
{"text_chunk": "Develop the flow using different tools\nIn one flow, you can consume different kinds of tools. We now support built-in tool like \nLLM, Python and \nPrompt and \nthird-party tool like Serp API, \nVector Search, etc."}
{"text_chunk": "Add tool as your need\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nYou can add a tool node in nodes section of flow yaml. For example, yaml below shows how to add a Python tool node in the flow.\n\n```yaml\nnodes:\n- name: fetch_text_content_from_url\n  type: python\n  source:\n    type: code\n    path: fetch_text_content_from_url.py\n  inputs:\n    url: ${inputs.url}\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\nBy selecting the tool card on the very top, you'll add a new tool node to flow.\n\n!add_tool\n:::\n\n::::"}
{"text_chunk": "Edit tool\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nYou can edit the tool by simply opening the source file and making edits. For example, we provide a simple Python tool code below.\n\n```python\nfrom promptflow import tool"}
{"text_chunk": "The inputs section will change based on the arguments of the tool function, after you save the code\n@tool\ndef my_python_tool(input1: str) -> str:\n  return 'hello ' + input1\n```\n\nWe also provide an LLM tool prompt below.\n\n```jinja\nPlease summarize the following text in one paragraph. 100 words.\nDo not add any information that is not in the text.\nText: {{text}}\nSummary:\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\nWhen a new tool node is added to flow, it will be appended at the bottom of flatten view with a random name by default. \nAt the top of each tool node card, there's a toolbar for adjusting the tool node. You can move it up or down, you can delete or rename it too.\nFor a python tool node, you can edit the tool code by clicking the code file. For a LLM tool node, you can edit the \ntool prompt by clicking the prompt file and adjust input parameters like connection, api and etc.\n!edit_tool\n:::\n\n::::"}
{"text_chunk": "Create connection\nPlease refer to the Create necessary connections for details."}
{"text_chunk": "Chain your flow - link nodes together\nBefore linking nodes together, you need to define and expose an interface."}
{"text_chunk": "Define LLM node interface\nLLM node has only one output, the completion given by LLM provider.\n\nAs for inputs, we offer a templating strategy that can help you create parametric prompts that accept different input \nvalues. Instead of fixed text, enclose your input name in `{{}}`, so it can be replaced on the fly. We use Jinja as our \ntemplating language. For example:\n\n```jinja\nYour task is to classify a given url into one of the following types:\nMovie, App, Academic, Channel, Profile, PDF or None based on the text content information.\nThe classification will be based on the url, the webpage text content summary, or both.\n\nHere are a few examples:\n{% for ex in examples %}\nURL: {{ex.url}}\nText content: {{ex.text_content}}\nOUTPUT:\n{\"category\": \"{{ex.category}}\", \"evidence\": \"{{ex.evidence}}\"}\n\n{% endfor %}\n\nFor a given URL : {{url}}, and text content: {{text_content}}.\nClassify above url to complete the category and indicate evidence.\nOUTPUT:\n```"}
{"text_chunk": "Define Python node interface\nPython node might have multiple inputs and outputs. Define inputs and outputs as shown below. \nIf you have multiple outputs, remember to make it a dictionary so that the downstream node can call each key separately.\nFor example:\n\n```python\nimport json\nfrom promptflow import tool\n\n@tool\ndef convert_to_dict(input_str: str, input_str2: str) -> dict:\n    try:\n        print(input_str2)\n        return json.loads(input_str)\n    except Exception as e:\n        print(\"input is not valid, error: {}\".format(e))\n        return {\"category\": \"None\", \"evidence\": \"None\"}\n```"}
{"text_chunk": "Link nodes together\nAfter the interface is defined, you can use:\n\n- ${inputs.key} to link with flow input.\n- ${upstream_node_name.output} to link with single-output upstream node.\n- ${upstream_node_name.output.key} to link with multi-output upstream node.\n\nBelow are common scenarios for linking nodes together."}
{"text_chunk": "Scenario 1 - Link LLM node with flow input and single-output upstream node\nAfter you add a new LLM node and edit the prompt file like Define LLM node interface, \nthree inputs called `url`, `examples` and `text_content` are created in inputs section.\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nYou can link the LLM node input with flow input by `${inputs.url}`. \nAnd you can link `examples` to the upstream `prepare_examples` node and `text_content` to the `summarize_text_content` node \nby `${prepare_examples.output}` and `${summarize_text_content.output}`. \n```yaml\n- name: classify_with_llm\n  type: llm\n  source:\n    type: code\n    path: classify_with_llm.jinja2\n  inputs:\n    deployment_name: text-davinci-003\n    suffix: \"\"\n    max_tokens: 128\n    temperature: 0.2\n    top_p: 1\n    echo: false\n    presence_penalty: 0\n    frequency_penalty: 0\n    best_of: 1\n    url: ${inputs.url}    # Link with flow input\n    examples: ${prepare_examples.output} # Link LLM node with single-output upstream node\n    text_content: ${summarize_text_content.output} # Link LLM node with single-output upstream node\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\nIn the value drop-down, select `${inputs.url}`, `${prepare_examples.output}` and `${summarize_text_content.output}`, then \nyou'll see in the graph view that the newly created LLM node is linked to the flow input, upstream `prepare_examples` and `summarize_text_content` node. \n\n!link_llm_with_flow_input_single_output_node\n:::\n\n::::\nWhen running the flow, the `url` input of the node will be replaced by flow input on the fly, and the `examples` and \n`text_content` input of the node will be replaced by `prepare_examples` and `summarize_text_content` node output on the fly."}
{"text_chunk": "Scenario 2 - Link LLM node with multi-output upstream node\nSuppose we want to link the newly created LLM node with `covert_to_dict` Python node whose output is a dictionary with two keys: `category` and `evidence`.\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nYou can link `examples` to the `evidence` output of upstream `covert_to_dict` node by `${convert_to_dict.output.evidence}` like below: \n```yaml\n- name: classify_with_llm\n  type: llm\n  source:\n    type: code\n    path: classify_with_llm.jinja2\n  inputs:\n    deployment_name: text-davinci-003\n    suffix: \"\"\n    max_tokens: 128\n    temperature: 0.2\n    top_p: 1\n    echo: false\n    presence_penalty: 0\n    frequency_penalty: 0\n    best_of: 1\n    text_content: ${convert_to_dict.output.evidence} # Link LLM node with multi-output upstream node\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\nIn the value drop-down, select `${convert_to_dict.output}`, then manually append `evidence`, then you'll see in the graph \nview that the newly created LLM node is linked to the upstream `convert_to_dict node`.\n\n!link_llm_with_multi_output_node\n:::\n::::\nWhen running the flow, the `text_content` input of the node will be replaced by `evidence` value from `convert_to_dict node` output dictionary on the fly."}
{"text_chunk": "Scenario 3 - Link Python node with upstream node/flow input\nAfter you add a new Python node and edit the code file like Define Python node interface], \ntwo inputs called `input_str` and `input_str2` are created in inputs section. The linkage is the same as LLM node, \nusing `${flow.input_name}` to link with flow input or `${upstream_node_name.output}` to link with upstream node.\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n```yaml\n- name: prepare_examples\n  type: python\n  source:\n    type: code\n    path: prepare_examples.py\n  inputs:\n    input_str: ${inputs.url}  # Link Python node with flow input\n    input_str2: ${fetch_text_content_from_url.output} # Link Python node with single-output upstream node\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\n!link_python_with_flow_node_input\n:::\n\n::::\nWhen running the flow, the `input_str` input of the node will be replaced by flow input on the fly and the `input_str2` \ninput of the node will be replaced by `fetch_text_content_from_url` node output dictionary on the fly."}
{"text_chunk": "Set flow output\nWhen the flow is complicated, instead of checking outputs on each node, you can set flow output and check outputs of \nmultiple nodes in one place. Moreover, flow output helps:\n\n- Check bulk test results in one single table.\n- Define evaluation interface mapping.\n- Set deployment response schema.\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nYou can add flow outputs in outputs section of flow yaml . The linkage is the same as LLM node, \nusing `${convert_to_dict.output.category}` to link `category` flow output with with `category` value of upstream node \n`convert_to_dict`.\n\n```yaml\noutputs:\n  category:\n    type: string\n    reference: ${convert_to_dict.output.category}\n  evidence:\n    type: string\n    reference: ${convert_to_dict.output.evidence}\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\nFirst define flow output schema, then select in drop-down the node whose output you want to set as flow output. \nSince `convert_to_dict` has a dictionary output with two keys: `category` and `evidence`, you need to manually append \n`category` and `evidence` to each. Then run flow, after a while, you can check flow output in a table.\n\n!flow_output\n:::\n\n::::"}
{"text_chunk": "Referencing external files/folders in a flow\n\nSometimes, pre-existing code assets are essential for the flow reference. In most cases, you can accomplish this by importing a Python package into your flow. However, if a Python package is not available or it is heavy to create a package, you can still reference external files or folders located outside of the current flow folder by using our **additional includes** feature in your flow configuration.\n\nThis feature provides an efficient mechanism to list relative file or folder paths that are outside of the flow folder, integrating them seamlessly into your flow.dag.yaml. For example:\n\n```yaml\nadditional_includes:\n- ../web-classification/classify_with_llm.jinja2\n- ../web-classification/convert_to_dict.py\n- ../web-classification/fetch_text_content_from_url.py\n- ../web-classification/prepare_examples.py\n- ../web-classification/summarize_text_content.jinja2\n- ../web-classification/summarize_text_content__variant_1.jinja2\n```\n\nYou can add this field `additional_includes` into the flow.dag.yaml. The value of this field is a list of the **relative file/folder path** to the flow folder.\n\nJust as with the common definition of the tool node entry, you can define the tool node entry in the flow.dag.yaml using only the file name, eliminating the need to specify the relative path again. For example:\n\n```yaml\nnodes:\n- name: fetch_text_content_from_url\n  type: python\n  source:\n    type: code\n    path: fetch_text_content_from_url.py\n  inputs:\n    url: ${inputs.url}\n- name: summarize_text_content\n  use_variants: true\n- name: prepare_examples\n  type: python\n  source:\n    type: code\n    path: prepare_examples.py\n  inputs: {}\n```\n\nThe entry file \"fetch_text_content_from_url.py\" of the tool node \"fetch_text_content_from_url\" is located in \"../web-classification/fetch_text_content_from_url.py\", as specified in the additional_includes field. The same applies to the \"summarize_text_content\" tool nodes.\n\n> **Note**:\n>\n> 1. If you have two files with the same name located in different folders specified in the `additional_includes` field, and the file name is also specified as the entry of a tool node, the system will reference the **last one** it encounters in the `additional_includes` field.\n> > 1. If you have a file in the flow folder with the same name as a file specified in the `additional_includes` field, the system will prioritize the file listed in the `additional_includes` field.\nTake the following YAML structure as an example:\n\n```yaml\nadditional_includes:\n- ../web-classification/prepare_examples.py\n- ../tmp/prepare_examples.py\n...\nnodes:\n- name: summarize_text_content\n  use_variants: true\n- name: prepare_examples\n  type: python\n  source:\n    type: code\n    path: prepare_examples.py\n  inputs: {}\n``` \n\nIn this case, the system will use \"../tmp/prepare_examples.py\" as the entry file for the tool node \"prepare_examples\". Even if there is a file named \"prepare_examples.py\" in the flow folder, the system will still use the file \"../tmp/prepare_examples.py\" specified in the `additional_includes` field.\n\n> Tips:\n> The additional includes feature can significantly streamline your workflow by eliminating the need to manually handle these references.\n> 1. To get a hands-on experience with this feature, practice with our sample flow-with-additional-includes.\n> 1. You can learn more about How the 'additional includes' flow operates during the transition to the cloud."}
{"text_chunk": "Adding a tool icon\nA tool icon serves as a graphical representation of your tool in the user interface (UI). Follow this guidance to add a custom tool icon when developing your own tool package.\n\nAdding a custom tool icon is optional. If you do not provide one, the system uses a default icon."}
{"text_chunk": "Prerequisites\n\n- Please ensure that your Prompt flow for VS Code is updated to version 1.4.2 or later.\n- Please install promptflow package and ensure that its version is 1.1.0 or later.\n- Create a tool package as described in Create and Use Tool Package.\n- Prepare custom icon image that meets these requirements:\n\n  - Use PNG, JPG or BMP format.\n  - 16x16 pixels to prevent distortion when resizing.\n  - Avoid complex images with lots of detail or contrast, as they may not resize well. \n\n  See this example as a reference."}
{"text_chunk": "Add tool icon with _icon_ parameter"}
{"text_chunk": "Initialize a package tool with icon\nYou can use pf tool init to initialize a package tool with icon:\n```bash\npf tool init --package  --tool  --set icon=\n```\n\nPF CLI will copy the icon file to the folder `/icons/` and generate a tool script in the package. The tool icon will be configured in the tool script. Here we use an existing tool as an example, the code is as follows:\n```python\nfrom pathlib import Path\n\nfrom promptflow import tool\nfrom promptflow.connections import CustomConnection\n\n\n@tool(\n    name=\"My First Tool\",\n    description=\"This is my first tool\",\n    icon=Path(__file__).parent.parent / \"icons\" / \"custom-tool-icon.png\"\n)\ndef my_tool(connection: CustomConnection, input_text: str) -> str:\n    # Replace with your tool code.\n    # Usually connection contains configs to connect to an API.\n    # Use CustomConnection is a dict. You can use it like: connection.api_key, connection.api_base\n    # Not all tools need a connection. You can remove it if you don't need it.\n    return \"Hello \" + input_text\n```\n\nThe folder structure of the generated tool package is as follows:\n```\n\n\u2502   MANIFEST.in\n\u2502   README.md\n\u2502   setup.py\n\u2502\n\u251c\u2500\u2500\u2500icons\n\u2502       \n\u2502\n\u2514\u2500\u2500\u2500\n        .py\n        utils.py\n        __init__.py\n```"}
{"text_chunk": "Verify the tool icon in VS Code extension\nFollow steps to use your tool from VS Code extension. Your tool displays with the custom icon:  \n!custom-tool-with-icon-in-extension"}
{"text_chunk": "FAQ\nYes, you could run below command under the root folder to generate a data URI for your custom tool icon. Make sure the output file has an `.html` extension.\n```\npython \\tool\\convert_image_to_data_url.py --image-path  -o \n```\nFor example:\n```\npython D:\\proj\\github\\promptflow\\scripts\\tool\\convert_image_to_data_url.py --image-path D:\\proj\\github\\promptflow\\examples\\tools\\tool-package-quickstart\\my_tool_package\\icons\\custom-tool-icon.png -o output.html\n```\nThe content of `output.html` looks like the following, open it in a web browser to preview the icon.\n```html\n\n\n\n\n\n```"}
{"text_chunk": "Can I add a tool icon to an existing tool package\n\nYou can follow these steps to add an icon to an existing package tool:\n1. Copy the icon image to the package folder.\n2. Configure the icon for the tool.\n\n    In the tool script, add the `icon` parameter to the decorator method `@tool`, and the parameter value is the `icon path`. The code is as follows:\n    ```python\n    from promptflow import tool\n\n    @tool(name=\"tool_name\", icon=)\n    def tool_func(input_text: str) -> str:\n        # Tool logic\n        pass\n    ```\n3. Update `MANIFEST.in` in the package folder.\n\n    This file is used to determine which files to include in the distribution of the project. You need to add the icon path relative to the package folder to this file.\n    ```\n    include \n    ```"}
{"text_chunk": "Can I add tool icons for dark and light mode separately?\nYes, you can add the tool icon data to the tool code as follows:\n```python\nfrom promptflow import tool\n\n@tool(name=\"tool_name\", icon_dark=, icon_light=)\ndef tool_func(input_text: str) -> str:\n    # Tool logic\n    pass\n```\n\nOr run the command below in your tool project directory to automatically generate tool code, use _--icon_light_ to add a custom tool icon for the light mode and use _--icon_dark_ to add a custom tool icon for the dark mode:\n```python\npf tool init --tool  --set icon_dark= icon_light=\n```\n\nNote: Both light and dark icons are optional. If you set either a light or dark icon, it will be used in its respective mode, and the system default icon will be used in the other mode."}
{"text_chunk": "Adding category and tags for tool\n\nThis document is dedicated to guiding you through the process of categorizing and tagging your tools for optimal organization and efficiency. Categories help you organize your tools into specific folders, making it much easier to find what you need. Tags, on the other hand, work like labels that offer more detailed descriptions. They enable you to quickly search and filter tools based on specific characteristics or functions. By using categories and tags, you'll not only tailor your tool library to your preferences but also save time by effortlessly finding the right tool for any task.\n\n| Attribute | Type | Required | Description |\n| --------- | ---- | -------- | ----------- |\n| category  | str  | No       | Organizes tools into folders by common features. |\n| tags      | dict | No       | Offers detailed, searchable descriptions of tools through key-value pairs. |\n\n**Important Notes:**\n- Tools without an assigned category will be listed in the root folder.\n- Tools lacking tags will display an empty tags field."}
{"text_chunk": "Prerequisites\n- Please ensure that your Prompt flow for VS Code is updated to version 1.1.0 or later.\n- Please install promptflow package and ensure that its version is 1.1.0 or later."}
{"text_chunk": "How to add category and tags for a tool\n\nYou can use pf tool init to initialize a package tool with category and tags:\n```python\npf tool init --package  --tool  --set category= tags=\n\n```\n\nHere, we use an example to show the categories and tags of the tool after initialization. Assume that the user executes this command:\n```python\npf tool init --tool my_tool --set name=\"My First Tool\" description=\"This is my first tool\"  category=\"test_tool\" tags=\"{'tag1':'value1','tag2':'value2'}\"\n```\nThe generated tool script is as follows, where category and tags have been configured on the tool:\n```python\nfrom promptflow import tool\nfrom promptflow.connections import CustomConnection\n\n\n@tool(\n    name=\"My First Tool\",\n    description=\"This is my first tool\",\n    category=\"test_tool\",\n    tags={\"tag1\": \"value1\", \"tag2\": \"value2\"},\n)\ndef my_tool(self, input_text: str) -> str:\n    # Replace with your tool code.\n    # Usually connection contains configs to connect to an API.\n    # Use CustomConnection is a dict. You can use it like: connection.api_key, connection.api_base\n    # Not all tools need a connection. You can remove it if you don't need it.\n    return \"Hello \" + input_text\n```"}
{"text_chunk": "Tool with category and tags experience in VS Code extension\nFollow the steps to use your tool via the VS Code extension.\n- Experience in the tool tree\n!category_and_tags_in_tool_tree\n\n- Experience in the tool list\nBy clicking `More` in the visual editor, you can view your tools along with their category and tags:\n!category_and_tags_in_tool_list\nFurthermore, you have the option to search or filter tools based on tags:\n!filter_tools_by_tag"}
{"text_chunk": "FAQ\nCustomer can configure category and tags directly on the tool script, as shown in the following code:\n```python\n@tool(\n    name=\"tool_name\",\n    description=\"This is tool_name tool\",\n    category=,\n    tags=,\n)\ndef tool_name(input_text: str) -> str:\n    # tool logic\n    pass\n```"}
{"text_chunk": "Create and use tool package\nIn this document, we will guide you through the process of developing your own tool package, offering detailed steps and advice on how to utilize your creation.\n\nThe custom tool is the prompt flow tool developed by yourself. If you find it useful, you can follow this guidance to make it a tool package. This will enable you to conveniently reuse it, share it with your team, or distribute it to anyone in the world.\n\nAfter successful installation of the package, your custom \"tool\" will show up in VSCode extension as below: \n!custom-tool-list"}
{"text_chunk": "Create your own tool package\nYour tool package should be a python package. To try it quickly, just use my-tools-package 0.0.1 and skip this section."}
{"text_chunk": "Prerequisites\nCreate a new conda environment using python 3.9 or 3.10. Run below command to install PromptFlow dependencies:\n```\npip install promptflow\n```"}
{"text_chunk": "Create custom tool package\nYou can use pf tool init to initialize a package tool in current folder:\n\n```bash\npf tool init --package  --tool \n\n```\nFor example:\n```bash\npf tool init --package hello_world --tool hello_world_tool\n```\nThis auto-generated script will create one tool for you. The parameters _destination_ and _package-name_ are mandatory. The parameters _tool-name_ and _function-name_ are optional. If left unfilled, the _tool-name_ will default to _hello_world_tool_, and the _function-name_ will default to _tool-name_.\n\nThe command will generate the tool project as follows with one tool `hello_world_tool.py` in it:\n\n```\nhello_world/\n\u2502   MANIFEST.in\n\u2502   README.md\n\u2502   setup.py\n\u2502\n\u2514\u2500\u2500\u2500hello_world/\n        hello_world_tool.py\n        utils.py\n        __init__.py\n```\n\n```The points outlined below explain the purpose of each folder/file in the package. If your aim is to develop multiple tools within your package, please make sure to closely examine point 2 and 5.```\n\n1. **hello_world**: This is the source directory. All of your project's source code should be placed in this directory.\n2. **hello_world/hello_world**: This directory contains the individual tools for your project. Your tool package can contain either one tool or many tools. When adding a new tool, you should create another *.py under this folder.\n3. **hello-world/hello_world/hello_world_tool.py**: Develop your tool within the def function. Use the `@tool` decorator to identify the function as a tool.\n    > !Note] There are two ways to write a tool. The default and recommended way is the function implemented way. You can also use the class implementation way, referring to [my_tool_2.py as an example.\n4. **hello-world/hello_world/utils.py**: This file implements the tool list method, which collects all the tools defined. It is required to have this tool list method, as it allows the User Interface (UI) to retrieve your tools and display them within the UI.\n    > [!Note] There's no need to create your own list method if you maintain the existing folder structure. You can simply use the auto-generated list method provided in the `utils.py` file.\n7. **MANIFEST.in**: This file is used to determine which files to include in the distribution of the project.\n    > [!Note] There's no need to update this file if you maintain the existing folder structure.\n8. **setup.py**: This file contains metadata about your project like the name, version, author, and more. Additionally, the entry point is automatically configured for you by `pf tool init`. In Python, configuring the entry point in `setup.py` helps establish the primary execution point for a package, streamlining its integration with other software. \n\n    The `package_tools` entry point together with the tool list method are used to retrieve all the tools and display them in the UI.\n    ```python\n    entry_points={\n          \"package_tools\": [\" = :\"],\n    },\n    ```\n    > [!Note] There's no need to update this file if you maintain the existing folder structure."}
{"text_chunk": "Build and share the tool package\n  Execute the following command in the tool package root directory to build your tool package:\n  ```\n  python setup.py sdist bdist_wheel\n  ```\n  This will generate a tool package `-0.0.1.tar.gz` and corresponding `whl file` inside the `dist` folder.\n\n  Create an account on PyPI if you don't already have one, and install `twine` package by running `pip install twine`.\n\n  Upload your package to PyPI by running `twine upload dist/*`, this will prompt you for your Pypi username and password, and then upload your package on PyPI. Once your package is uploaded to PyPI, others can install it using pip by running `pip install your-package-name`. Make sure to replace `your-package-name` with the name of your package as it appears on PyPI.\n\n  If you only want to put it on Test PyPI, upload your package by running `twine upload --repository-url https://test.pypi.org/legacy/ dist/*`. Once your package is uploaded to Test PyPI, others can install it using pip by running `pip install --index-url https://test.pypi.org/simple/ your-package-name`."}
{"text_chunk": "Use your tool from VSCode Extension\n* Step1: Install Prompt flow for VS Code extension. \n\n* Step2: Go to terminal and install your tool package in conda environment of the extension. Assume your conda env name is `prompt-flow`.\n   ```\n   (local_test) PS D:\\projects\\promptflow\\tool-package-quickstart> conda activate prompt-flow\n   (prompt-flow) PS D:\\projects\\promptflow\\tool-package-quickstart> pip install .\\dist\\my_tools_package-0.0.1-py3-none-any.whl\n   ``` \n\n* Step3: Go to the extension and open one flow folder. Click 'flow.dag.yaml' and preview the flow. Next, click `+` button and you will see your tools. You may need to reload the windows to clean previous cache if you don't see your tool in the list.\n!auto-list-tool-in-extension"}
{"text_chunk": "FAQs\nConfirm that configured the package tool correctly. Alternatively, you can test your tool package using the script below to ensure that you've configured the package tool entry point correctly.\n\n  1. Make sure to install the tool package in your conda environment before executing this script.\n  2. Create a python file anywhere and copy the content below into it.\n      ```python\n      import importlib\n      import importlib.metadata\n\n      def test():\n          \"\"\"List all package tools information using the `package-tools` entry point.\n\n          This function iterates through all entry points registered under the group \"package_tools.\"\n          For each tool, it imports the associated module to ensure its validity and then prints\n          information about the tool.\n\n          Note:\n          - Make sure your package is correctly packed to appear in the list.\n          - The module is imported to validate its presence and correctness.\n\n          Example of tool information printed:\n          ----identifier\n          {'module': 'module_name', 'package': 'package_name', 'package_version': 'package_version', ...}\n          \"\"\"\n          entry_points = importlib.metadata.entry_points()\n          PACKAGE_TOOLS_ENTRY = \"package_tools\"\n          if isinstance(entry_points, list):\n              entry_points = entry_points.select(group=PACKAGE_TOOLS_ENTRY)\n          else:\n              entry_points = entry_points.get(PACKAGE_TOOLS_ENTRY, [])\n          for entry_point in entry_points:\n              list_tool_func = entry_point.load()\n              package_tools = list_tool_func()\n\n              for identifier, tool in package_tools.items():\n                  importlib.import_module(tool[\"module\"])  # Import the module to ensure its validity\n                  print(f\"----{identifier}\\n{tool}\")\n\n      if __name__ == \"__main__\":\n          test()\n      ```\n  3. Run this script in your conda environment. This will return the metadata of all tools installed in your local environment, and you should verify that your tools are listed."}
{"text_chunk": "Why am I unable to upload package to PyPI?\n* Make sure that the entered username and password of your PyPI account are accurate.\n* If you encounter a `403 Forbidden Error`, it's likely due to a naming conflict with an existing package. You will need to choose a different name. Package names must be unique on PyPI to avoid confusion and conflicts among users. Before creating a new package, it's recommended to search PyPI (https://pypi.org/) to verify that your chosen name is not already taken. If the name you want is unavailable, consider selecting an alternative name or a variation that clearly differentiates your package from the existing one."}
{"text_chunk": "Advanced features\n- Add a Tool Icon  \n- Add Category and Tags for Tool  \n- Create and Use Your Own Custom Strong Type Connection  \n- Customize an LLM Tool  \n- Use File Path as Tool Input  \n- Create a Dynamic List Tool Input  \n- Create Cascading Tool Inputs"}
{"text_chunk": "Creating cascading tool inputs\n\nCascading input settings are useful when the value of one input field determines which subsequent inputs are shown. This makes the input process more streamlined, user-friendly, and error-free. This guide will walk through how to create cascading inputs for your tools."}
{"text_chunk": "Prerequisites\n- Please make sure you have the latest version of Prompt flow for VS Code installed (v1.2.0+).\n- Please install promptflow package and ensure that its version is 1.0.0 or later.\n  ```\n  pip install promptflow>=1.0.0\n  ```"}
{"text_chunk": "Create a tool with cascading inputs\nWe'll build out an example tool to show how cascading inputs work. The `student_id` and `teacher_id` inputs will be controlled by the value selected for the `user_type` input. Here's how to configure this in the tool code.\n\nDevelop the tool function, following the cascading inputs example. Key points:\n * Use the `@tool` decorator to mark the function as a tool.\n * Define `UserType` as an Enum class, as it accepts only a specific set of fixed values in this example.\n * Conditionally use inputs in the tool logic based on `user_type`.\n * Add `enabled_by` and `enabled_by_value` to control visibility of dependent inputs.\n    * The `enabled_by` attribute specifies the input field, which must be an enum type, that controls the visibility of the dependent input field.\n    * The `enabled_by_value` attribute defines the accepted enum values from the `enabled_by` field that will make this dependent input field visible.\n    > Note: `enabled_by_value` takes a list, allowing multiple values to enable an input.\n\n```python\nfrom enum import Enum\n\nfrom promptflow.entities import InputSetting\nfrom promptflow import tool\n\n\nclass UserType(str, Enum):\n    STUDENT = \"student\"\n    TEACHER = \"teacher\"\n\n\n@tool(\n    name=\"My Tool with Enabled By Value\",\n    description=\"This is my tool with enabled by value\",\n    input_settings={\n        \"teacher_id\": InputSetting(enabled_by=\"user_type\", enabled_by_value=[UserType.TEACHER]),\n        \"student_id\": InputSetting(enabled_by=\"user_type\", enabled_by_value=[UserType.STUDENT]),\n    }\n)\ndef my_tool(user_type: UserType, student_id: str = \"\", teacher_id: str = \"\") -> str:\n    \"\"\"This is a dummy function to support enabled by feature.\n    :param user_type: user type, student or teacher.\n    :param student_id: student id.\n    :param teacher_id: teacher id.\n    :return: id of the user.\n    If user_type is student, return student_id.\n    If user_type is teacher, return teacher_id.\n    \"\"\"\n    if user_type == UserType.STUDENT:\n        return student_id\n    elif user_type == UserType.TEACHER:\n        return teacher_id\n    else:\n        raise Exception(\"Invalid user.\")\n```"}
{"text_chunk": "Use the tool in VS Code\nOnce you package and share your tool, you can use it in VS Code per the tool package guide. We have a demo flow you can try.\n\nBefore selecting a `user_type`, the `student_id` and `teacher_id` inputs are hidden. Once you pick the `user_type`, the corresponding input appears.\n!before_user_type_selected.png\n!after_user_type_selected_with_student.png\n!after_user_type_selected_with_teacher.png"}
{"text_chunk": "FAQs\nIf you are dealing with multiple levels of cascading inputs, you can effectively manage the dependencies between them by using the `enabled_by` and `enabled_by_value` attributes. For example:\n```python\nfrom enum import Enum\n\nfrom promptflow.entities import InputSetting\nfrom promptflow import tool\n\n\nclass EventType(str, Enum):\n    CORPORATE = \"corporate\"\n    PRIVATE = \"private\"\n\n\nclass CorporateTheme(str, Enum):\n    SEMINAR = \"seminar\"\n    TEAM_BUILDING = \"team_building\"\n\n\n@tool(\n    name=\"My Tool with Multi-Layer Cascading Inputs\",\n    description=\"This is my tool with multi-layer cascading inputs\",\n    input_settings={\n        \"corporate_theme\": InputSetting(enabled_by=\"event_type\", enabled_by_value=[EventType.CORPORATE]),\n        \"seminar_location\": InputSetting(enabled_by=\"corporate_theme\", enabled_by_value=[CorporateTheme.SEMINAR]),\n        \"private_theme\": InputSetting(enabled_by=\"event_type\", enabled_by_value=[CorporateTheme.PRIVATE]),\n    }\n)\ndef my_tool(event_type: EventType, corporate_theme: CorporateTheme, seminar_location: str, private_theme: str) -> str:\n    \"\"\"This is a dummy function to support enabled by feature.\"\"\"\n    pass\n```\nInputs will be enabled in a cascading way based on selections."}
{"text_chunk": "Creating a dynamic list tool input\n\nTool input options can be generated on the fly using a dynamic list. Instead of having predefined static options, the tool author defines a request function that queries backends like APIs to retrieve real-time options. This enables flexible integration with various data sources to populate dynamic options. For instance, the function could call a storage API to list current files. Rather than a hardcoded list, the user sees up-to-date options when running the tool."}
{"text_chunk": "Prerequisites\n\n- Please make sure you have the latest version of Prompt flow for VS Code installed (v1.3.1+).\n- Please install promptflow package and ensure that its version is 1.0.0 or later.\n  ```\n  pip install promptflow>=1.0.0\n  ```"}
{"text_chunk": "Create a tool input with dynamic listing"}
{"text_chunk": "Create a list function\n\nTo enable dynamic listing, the tool author defines a request function with the following structure:\n\n- Type: Regular Python function, can be in tool file or separate file\n- Input: Accepts parameters needed to fetch options\n- Output: Returns a list of option objects as `List[Dict[str, Union[str, int, float, list, Dict]]]`:\n  - Required key:\n    - `value`: Internal option value passed to tool function\n  - Optional keys:\n    - `display_value`: Display text shown in dropdown (defaults to `value`)\n    - `hyperlink`: URL to open when option clicked\n    - `description`: Tooltip text on hover\n\nThis function can make backend calls to retrieve the latest options, returning them in a standardized dictionary structure for the dynamic list. The required and optional keys enable configuring how each option appears and behaves in the tool input dropdown. See my_list_func as an example.\n\n```python\ndef my_list_func(prefix: str = \"\", size: int = 10, **kwargs) -> List[Dict[str, Union[str, int, float, list, Dict]]]:\n    \"\"\"This is a dummy function to generate a list of items.\n\n    :param prefix: prefix to add to each item.\n    :param size: number of items to generate.\n    :param kwargs: other parameters.\n    :return: a list of items. Each item is a dict with the following keys:\n        - value: for backend use. Required.\n        - display_value: for UI display. Optional.\n        - hyperlink: external link. Optional.\n        - description: information icon tip. Optional.\n    \"\"\"\n    import random\n\n    words = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\", \"honeydew\", \"kiwi\", \"lemon\"]\n    result = []\n    for i in range(size):\n        random_word = f\"{random.choice(words)}{i}\"\n        cur_item = {\n            \"value\": random_word,\n            \"display_value\": f\"{prefix}_{random_word}\",\n            \"hyperlink\": f'https://www.bing.com/search?q={random_word}',\n            \"description\": f\"this is {i} item\",\n        }\n        result.append(cur_item)\n\n    return result\n```"}
{"text_chunk": "Configure a tool input with the list function\n\nIn `input_settings` section of tool, add following properties to the input that you want to make dynamic:\n\n- `DynamicList`:\n  - `function`: Path to the list function (module_name.function_name).\n  - `input_mapping`: Parameters to pass to the function, can reference other input values.\n- `allow_manual_entry`: Allow user to enter input value manually. Default to false.\n- `is_multi_select`: Allow user to select multiple values. Default to false.\n\nSee tool_with_dynamic_list_input.py as an example.\n\n```python\nfrom promptflow._core.tool import tool\nfrom promptflow.entities import InputSetting, DynamicList\n\n\ndynamic_list_setting = DynamicList(function=my_list_func, input_mapping={\"prefix\": \"input_prefix\"})\ninput_settings = {\n    \"input_text\": InputSetting(\n        dynamic_list=dynamic_list_setting,\n        allow_manual_entry=True,\n        is_multi_select=True\n    )\n}\n\n\n@tool(\n    name=\"My Tool with Dynamic List Input\",\n    description=\"This is my tool with dynamic list input\",\n    input_settings=input_settings\n)\ndef my_tool(input_text: list, input_prefix: str) -> str:\n    return f\"Hello {input_prefix} {','.join(input_text)}\"\n```"}
{"text_chunk": "Use the tool in VS Code\n\nOnce you package and share your tool, you can use it in VS Code per the tool package guide. You could try `my-tools-package` for a quick test.\n\n```sh\npip install my-tools-package>=0.0.8\n```\n\n!dynamic list tool input options\n!dynamic list tool input selected\n\n> Note: If your dynamic list function call Azure APIs, you need to login to Azure and set default workspace. Otherwise, the tool input will be empty and you can't select anything. See FAQs for more details."}
{"text_chunk": "FAQs"}
{"text_chunk": "I'm a tool author, and want to dynamically list Azure resources in my tool input. What should I pay attention to?\n1. Clarify azure workspace triple \"subscription_id\", \"resource_group_name\", \"workspace_name\" in the list function signature. System helps append workspace triple to function input parameters if they are in function signature. See list_endpoint_names as an example.\n```python\ndef list_endpoint_names(subscription_id, resource_group_name, workspace_name, prefix: str = \"\") -> List[Dict[str, str]]:\n    \"\"\"This is an example to show how to get Azure ML resource in tool input list function.\n\n    :param subscription_id: Azure subscription id.\n    :param resource_group_name: Azure resource group name.\n    :param workspace_name: Azure ML workspace name.\n    :param prefix: prefix to add to each item.\n    \"\"\"\n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n\n    credential = DefaultAzureCredential()\n    credential.get_token(\"https://management.azure.com/.default\")\n\n    ml_client = MLClient(\n        credential=credential,\n        subscription_id=subscription_id,\n        resource_group_name=resource_group_name,\n        workspace_name=workspace_name)\n    result = []\n    for ep in ml_client.online_endpoints.list():\n        hyperlink = (\n            f\"https://ml.azure.com/endpoints/realtime/{ep.name}/detail?wsid=/subscriptions/\"\n            f\"{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.\"\n            f\"MachineLearningServices/workspaces/{workspace_name}\"\n        )\n        cur_item = {\n            \"value\": ep.name,\n            \"display_value\": f\"{prefix}_{ep.name}\",\n            # external link to jump to the endpoint page.\n            \"hyperlink\": hyperlink,\n            \"description\": f\"this is endpoint: {ep.name}\",\n        }\n        result.append(cur_item)\n    return result\n```\n2. Note in your tool doc that if your tool user want to use the tool at local, they should login to azure and set ws triple as default. Or the tool input will be empty and user can't select anything.\n```sh\naz login\naz account set --subscription \naz configure --defaults group= workspace=\n```\nInstall azure dependencies.\n```sh\npip install azure-ai-ml\n```\n```sh\npip install my-tools-package[azure]>=0.0.8\n```\n!dynamic list function azure"}
{"text_chunk": "I'm a tool user, and cannot see any options in dynamic list tool input. What should I do?\n\nIf you are unable to see any options in a dynamic list tool input, you may see an error message below the input field stating:\n\n\"Unable to display list of items due to XXX. Please contact the tool author/support team for troubleshooting assistance.\"\n\nIf this occurs, follow these troubleshooting steps:\n\n- Note the exact error message shown. This provides details on why the dynamic list failed to populate.\n- Contact the tool author/support team and report the issue. Provide the error message so they can investigate the root cause."}
{"text_chunk": "Create and use your own custom strong type connection\nConnections provide a secure method for managing credentials for external APIs and data sources in prompt flow. This guide explains how to create and use a custom strong type connection."}
{"text_chunk": "What is a Custom Strong Type Connection?\nA custom strong type connection in prompt flow allows you to define a custom connection class with strongly typed keys. This provides the following benefits:\n\n* Enhanced user experience - no need to manually enter connection keys.\n* Rich intellisense experience - defining key types enables real-time suggestions and auto-completion of available keys as you work in VS Code.\n* Central location to view available keys and data types.\n\nFor other connections types, please refer to Connections."}
{"text_chunk": "Prerequisites\n- Please ensure that your Prompt flow for VS Code is updated to at least version 1.2.1.\n- Please install promptflow package and ensure that its version is 1.0.0 or later.\n  ```\n  pip install promptflow>=1.0.0\n  ```"}
{"text_chunk": "Create a custom strong type connection\nFollow these steps to create a custom strong type connection:\n\n1. Define a Python class inheriting from `CustomStrongTypeConnection`.\n  > [!Note] Please avoid using the `CustomStrongTypeConnection` class directly.\n\n2. Use the Secret type to indicate secure keys. This enhances security by scrubbing secret keys.\n\n3. Document with docstrings explaining each key.\n\nFor example:\n\n```python\nfrom promptflow.connections import CustomStrongTypeConnection\nfrom promptflow.contracts.types import Secret\n\n\nclass MyCustomConnection(CustomStrongTypeConnection):\n    \"\"\"My custom strong type connection.\n\n    :param api_key: The api key.\n    :type api_key: Secret\n    :param api_base: The api base.\n    :type api_base: String\n    \"\"\"\n    api_key: Secret\n    api_base: str = \"This is a fake api base.\"\n\n```\n\nSee this example for a complete implementation."}
{"text_chunk": "Use the connection in a flow\nOnce you create a custom strong type connection, here are two ways to use it in your flows:"}
{"text_chunk": "With Package Tools:\n\n1. Refer to the Create and Use Tool Package to build and install your tool package containing the connection.\n\n2. Develop a flow with custom tools. Please take this folder as an example.\n\n3. Create a custom strong type connection using one of the following methods:\n    - If the connection type hasn't been created previously, click the 'Add connection' button to create the connection.\n    !create_custom_strong_type_connection_in_node_interface\n    - Click the 'Create connection' plus sign in the CONNECTIONS section.\n    !create_custom_strong_type_connection_add_sign\n    - Click 'Create connection' plus sign in the Custom category.\n    !create_custom_strong_type_connection_in_custom_category \n\n4. Fill in the `values` starting with `to-replace-with` in the connection template.\n!custom_strong_type_connection_template\n\n5. Run the flow with the created custom strong type connection.\n!use_custom_strong_type_connection_in_flow"}
{"text_chunk": "With Script Tools:\n\n1. Develop a flow with python script tools. Please take this folder as an example.\n\n2. Create a `CustomConnection`. Fill in the `keys` and `values` in the connection template.\n  !custom\n\n3. Run the flow with the created custom connection.\n  !use_custom_connection_in_flow"}
{"text_chunk": "Local to cloud\nWhen creating the necessary connections in Azure AI, you will need to create a `CustomConnection`. In the node interface of your flow, this connection will be displayed as the `CustomConnection` type.\n\nPlease refer to Run prompt flow in Azure AI for more details.\n\nHere is an example command:\n```\npfazure run create --subscription 96aede12-2f73-41cb-b983-6d11a904839b -g promptflow -w my-pf-eus --flow D:\\proj\\github\\ms\\promptflow\\examples\\flows\\standard\\flow-with-package-tool-using-custom-strong-type-connection --data D:\\proj\\github\\ms\\promptflow\\examples\\flows\\standard\\flow-with-package-tool-using-custom-strong-type-connection\\data.jsonl --runtime test-compute\n```"}
{"text_chunk": "FAQs"}
{"text_chunk": "I followed the steps to create a custom strong type connection, but it's not showing up. What could be the issue?\n\nOnce the new tool package is installed in your local environment, a window reload is necessary. This action ensures that the new tools and custom strong type connections become visible and accessible."}
{"text_chunk": "Customizing an LLM tool\nIn this document, we will guide you through the process of customizing an LLM tool, allowing users to seamlessly connect to a large language model with prompt tuning experience using a `PromptTemplate`."}
{"text_chunk": "Prerequisites\n- Please ensure that your Prompt flow for VS Code is updated to version 1.2.0 or later."}
{"text_chunk": "How to customize an LLM tool\nHere we use an existing tool package as an example. If you want to create your own tool, please refer to create and use tool package.  \n\nDevelop the tool code as in this example.\n- Add a `CustomConnection` input to the tool, which is used to authenticate and establish a connection to the large language model.\n- Add a `PromptTemplate` input to the tool, which serves as an argument to be passed into the large language model.\n\n    ```python\n    from jinja2 import Template\n    from promptflow import tool\n    from promptflow.connections import CustomConnection\n    from promptflow.contracts.types import PromptTemplate\n\n\n    @tool\n    def my_tool(connection: CustomConnection, prompt: PromptTemplate, **kwargs) -> str:\n        # Customize your own code to use the connection and prompt here.\n        rendered_prompt = Template(prompt, trim_blocks=True, keep_trailing_newline=True).render(**kwargs)\n        return rendered_prompt\n    ```"}
{"text_chunk": "Use the tool in VS Code\nFollow the steps to build and install your tool package and use your tool from VS Code extension.  \n\nHere we use an existing flow to demonstrate the experience, open this flow in VS Code extension.  \n- There is a node named \"my_custom_llm_tool\" with a prompt template file. You can either use an existing file or create a new one as the prompt template file.  \n!use_my_custom_llm_tool"}
{"text_chunk": "Using file path as tool input\n\nUsers sometimes need to reference local files within a tool to implement specific logic. To simplify this, we've introduced the `FilePath` input type. This input type enables users to either select an existing file or create a new one, then pass it to a tool, allowing the tool to access the file's content.\n\nIn this guide, we will provide a detailed walkthrough on how to use `FilePath` as a tool input. We will also demonstrate the user experience when utilizing this type of tool within a flow."}
{"text_chunk": "Prerequisites\n\n- Please install promptflow package and ensure that its version is 1.0.0 or later.\n  ```\n  pip install promptflow>=1.0.0\n  ```\n- Please ensure that your Prompt flow for VS Code is updated to version 1.1.0 or later."}
{"text_chunk": "Using File Path as Package Tool Input"}
{"text_chunk": "How to create a package tool with file path input\n\nHere we use an existing tool package as an example. If you want to create your own tool, please refer to create and use tool package.\n\nAdd a `FilePath` input for your tool, like in this example.\n\n```python\nimport importlib\nfrom pathlib import Path\nfrom promptflow import tool"}
{"text_chunk": "1. import the FilePath type\nfrom promptflow.contracts.types import FilePath"}
{"text_chunk": "2. add a FilePath input for your tool method\n@tool()\ndef my_tool(input_file: FilePath, input_text: str) -> str:\n    # 3. customise your own code to handle and use the input_file here\n    new_module = importlib.import_module(Path(input_file).stem)\n\n    return new_module.hello(input_text)   \n```\n\n> !Note] tool yaml file can be generated using a python script. For further details, please refer to [create custom tool package."}
{"text_chunk": "Use tool with a file path input in VS Code extension\n\nFollow steps to build and install your tool package and use your tool from VS Code extension.\n\nHere we use an existing flow to demonstrate the experience, open this flow in VS Code extension:\n\n- There is a node named \"Tool_with_FilePath_Input\" with a `file_path` type input called `input_file`.\n- Click the picker icon to open the UI for selecting an existing file or creating a new file to use as input.\n\n   !use file path in flow"}
{"text_chunk": "Using File Path as Script Tool Input\n\nWe can also utilize the `FilePath` input type directly in a script tool, eliminating the need to create a package tool.\n\n1. Initiate an empty flow in the VS Code extension and add a python node titled 'python_node_with_filepath' into it in the Visual Editor page.\n2. Select the link `python_node_with_filepath.py` in the node to modify the python method to include a `FilePath` input as shown below, and save the code change.\n    ```python\n    import importlib\n    from pathlib import Path\n    from promptflow import tool\n    # 1. import the FilePath type\n    from promptflow.contracts.types import FilePath\n\n    # 2. add a FilePath input for your tool method\n    @tool\n    def my_tool(input_file: FilePath, input_text: str) -> str:\n        # 3. customise your own code to handle and use the input_file here\n        new_module = importlib.import_module(Path(input_file).stem)\n    \n        return new_module.hello(input_text)   \n    ```\n\n3. Return to the flow Visual Editor page, click the picker icon to launch the UI for selecting an existing file or creating a new file to use as input, here we select this file as an example.\n   \n    !use file path in script tool"}
{"text_chunk": "FAQ"}
{"text_chunk": "What are some practical use cases for this feature?\nThe `FilePath` input enables several useful workflows:\n\n1. **Dynamically load modules** - As shown in the demo, you can load a Python module from a specific script file selected by the user. This allows flexible custom logic.\n2. **Load arbitrary data files** - The tool can load data from files like .csv, .txt, .json, etc. This provides an easy way to inject external data into a tool.\n\nSo in summary, `FilePath` input gives tools flexible access to external files provided by users at runtime. This unlocks many useful scenarios like the ones above."}
{"text_chunk": "Use streaming endpoints deployed from prompt flow\n\nIn prompt flow, you can deploy flow as REST endpoint for real-time inference.\n\nWhen consuming the endpoint by sending a request, the default behavior is that the online endpoint will keep waiting until the whole response is ready, and then send it back to the client. This can cause a long delay for the client and a poor user experience.\n\nTo avoid this, you can use streaming when you consume the endpoints. Once streaming enabled, you don't have to wait for the whole response ready. Instead, the server will send back the response in chunks as they are generated. The client can then display the response progressively, with less waiting time and more interactivity.\n\nThis article will describe the scope of streaming, how streaming works, and how to consume streaming endpoints."}
{"text_chunk": "Create a streaming enabled flow\n\nIf you want to use the streaming mode, you need to create a flow that has a node that produces a string generator as the flow\u2019s output. A string generator is an object that can return one string at a time when requested. You can use the following types of nodes to create a string generator:\n\n- LLM node: This node uses a large language model to generate natural language responses based on the input.\n  ```jinja\n  {# Sample prompt template for LLM node #}\n\n  system:\n  You are a helpful assistant.\n\n  user:\n  {{question}}\n  ```\n- Python tools node: This node allows you to write custom Python code that can yield string outputs. You can use this node to call external APIs or libraries that support streaming. For example, you can use this code to echo the input word by word:\n  ```python\n  from promptflow import tool\n\n  # Sample code echo input by yield in Python tool node\n\n  @tool\n  def my_python_tool(paragraph: str) -> str:\n      yield \"Echo: \"\n      for word in paragraph.split():\n          yield word + \" \"\n  ```\n\nIn this guide, we will use the \"Chat with Wikipedia\" sample flow as an example. This flow processes the user\u2019s question, searches Wikipedia for relevant articles, and answers the question with information from the articles. It uses streaming mode to show the progress of the answer generation.\n\n!chat_wikipedia.png"}
{"text_chunk": "Deploy the flow as an online endpoint\n\nTo use the streaming mode, you need to deploy your flow as an online endpoint. This will allow you to send requests and receive responses from your flow in real time.\n\nFollow this guide to deploy your flow as an online endpoint.\n\n> [!NOTE]\n> \n> You can follow this document to deploy an online endpoint.\n> Please deploy with runtime environment version later than version `20230816.v10`.\n> You can check your runtime version and update runtime in the run time detail page."}
{"text_chunk": "Understand the streaming process\n\nWhen you have an online endpoint, the client and the server need to follow specific principles for content negotiation to utilize the streaming mode:\n\nContent negotiation is like a conversation between the client and the server about the preferred format of the data they want to send and receive. It ensures effective communication and agreement on the format of the exchanged data.\n\nTo understand the streaming process, consider the following steps:\n\n- First, the client constructs an HTTP request with the desired media type included in the `Accept` header. The media type tells the server what kind of data format the client expects. It's like the client saying, \"Hey, I'm looking for a specific format for the data you'll send me. It could be JSON, text, or something else.\" For example, `application/json` indicates a preference for JSON data, `text/event-stream` indicates a desire for streaming data, and `*/*` means the client accepts any data format.\n    > [!NOTE]\n    > \n    > If a request lacks an `Accept` header or has empty `Accept` header, it implies that the client will accept any media type in response. The server treats it as `*/*`.\n    \n- Next, the server responds based on the media type specified in the `Accept` header. It's important to note that the client may request multiple media types in the `Accept` header, and the server must consider its capabilities and format priorities to determine the appropriate response.\n  - First, the server checks if `text/event-stream` is explicitly specified in the `Accept` header:\n    - For a stream-enabled flow, the server returns a response with a `Content-Type` of `text/event-stream`, indicating that the data is being streamed.\n    - For a non-stream-enabled flow, the server proceeds to check for other media types specified in the header.\n  - If `text/event-stream` is not specified, the server then checks if `application/json` or `*/*` is specified in the `Accept` header:\n    - In such cases, the server returns a response with a `Content-Type` of `application/json`, providing the data in JSON format.\n  - If the `Accept` header specifies other media types, such as `text/html`:\n    - The server returns a `424` response with a PromptFlow runtime error code `UserError` and a runtime HTTP status `406`, indicating that the server cannot fulfill the request with the requested data format.\n    > Note: Please refer handle errors for details.\n- Finally, the client checks the `Content-Type` response header. If it is set to `text/event-stream`, it indicates that the data is being streamed.\n\nLet\u2019s take a closer look at how the streaming process works. The response data in streaming mode follows the format of server-sent events (SSE).\n\nThe overall process works as follows:"}
{"text_chunk": "0. The client sends a message to the server.\n\n```\nPOST https://.inference.ml.azure.com/score\nContent-Type: application/json\nAuthorization: Bearer \nAccept: text/event-stream\n\n{\n    \"question\": \"Hello\",\n    \"chat_history\": []\n}\n```\n> [!NOTE]\n> \n> The `Accept` header is set to `text/event-stream` to request a stream response."}
{"text_chunk": "1. The server sends back the response in streaming mode.\n\n```\nHTTP/1.1 200 OK\nContent-Type: text/event-stream; charset=utf-8\nConnection: close\nTransfer-Encoding: chunked\n\ndata: {\"answer\": \"\"}\n\ndata: {\"answer\": \"Hello\"}\n\ndata: {\"answer\": \"!\"}\n\ndata: {\"answer\": \" How\"}\n\ndata: {\"answer\": \" can\"}\n\ndata: {\"answer\": \" I\"}\n\ndata: {\"answer\": \" assist\"}\n\ndata: {\"answer\": \" you\"}\n\ndata: {\"answer\": \" today\"}\n\ndata: {\"answer\": \" ?\"}\n\ndata: {\"answer\": \"\"}\n\n```\n\nNote that the `Content-Type` is set to `text/event-stream; charset=utf-8`, indicating the response is an event stream.\n\nThe client should decode the response data as server-sent events and display them incrementally. The server will close the HTTP connection after all the data is sent.\n\nEach response event is the delta to the previous event. It is recommended for the client to keep track of the merged data in memory and send them back to the server as chat history in the next request."}
{"text_chunk": "2. The client sends another chat message, along with the full chat history, to the server.\n\n```\nPOST https://.inference.ml.azure.com/score\nContent-Type: application/json\nAuthorization: Bearer \nAccept: text/event-stream\n\n{\n    \"question\": \"Glad to know you!\",\n    \"chat_history\": [\n        {\n            \"inputs\": {\n                \"question\": \"Hello\"\n            },\n            \"outputs\": {\n                \"answer\": \"Hello! How can I assist you today?\"\n            }\n        }\n    ]\n}\n```"}
{"text_chunk": "3. The server sends back the answer in streaming mode.\n```\nHTTP/1.1 200 OK\nContent-Type: text/event-stream; charset=utf-8\nConnection: close\nTransfer-Encoding: chunked\n\ndata: {\"answer\": \"\"}\n\ndata: {\"answer\": \"Nice\"}\n\ndata: {\"answer\": \" to\"}\n\ndata: {\"answer\": \" know\"}\n\ndata: {\"answer\": \" you\"}\n\ndata: {\"answer\": \" too\"}\n\ndata: {\"answer\": \"!\"}\n\ndata: {\"answer\": \" Is\"}\n\ndata: {\"answer\": \" there\"}\n\ndata: {\"answer\": \" anything\"}\n\ndata: {\"answer\": \" I\"}\n\ndata: {\"answer\": \" can\"}\n\ndata: {\"answer\": \" help\"}\n\ndata: {\"answer\": \" you\"}\n\ndata: {\"answer\": \" with\"}\n\ndata: {\"answer\": \"?\"}\n\ndata: {\"answer\": \"\"}\n\n```"}
{"text_chunk": "4. The chat continues in a similar way."}
{"text_chunk": "Handle errors\n\nThe client should check the HTTP response code first. See this table for common error codes returned by online endpoints.\n\nIf the response code is \"424 Model Error\", it means that the error is caused by the model\u2019s code. The error response from a PromptFlow model always follows this format:\n\n```json\n{\n  \"error\": {\n    \"code\": \"UserError\",\n    \"message\": \"Media type text/event-stream in Accept header is not acceptable. Supported media type(s) - application/json\",\n  }\n}\n```\n* It is always a JSON dictionary with only one key \"error\" defined.\n* The value for \"error\" is a dictionary, containing \"code\", \"message\".\n* \"code\" defines the error category. Currently, it may be \"UserError\" for bad user inputs and \"SystemError\" for errors inside the service.\n* \"message\" is a description of the error. It can be displayed to the end user."}
{"text_chunk": "How to consume the server-sent events"}
{"text_chunk": "Consume using Python\n\nIn this sample usage, we are using the `SSEClient` class. This class is not a built-in Python class and needs to be installed separately. You can install it via pip:\n\n```bash\npip install sseclient-py  \n```\n\nA sample usage would like:\n\n```python\nimport requests  \nfrom sseclient import SSEClient  \nfrom requests.exceptions import HTTPError  \n\ntry:\n    response = requests.post(url, json=body, headers=headers, stream=stream)\n    response.raise_for_status()\n\n    content_type = response.headers.get('Content-Type')\n    if \"text/event-stream\" in content_type:\n        client = SSEClient(response)\n        for event in client.events():\n            # Handle event, i.e. print to stdout\n    else:\n        # Handle json response\n\nexcept HTTPError:\n    # Handle exceptions\n```"}
{"text_chunk": "Consume using JavaScript\n\nThere are several libraries to consume server-sent events in JavaScript. Here is one of them as an example."}
{"text_chunk": "A sample chat app using Python\n\nHere is a sample chat app written in Python.\n(Click here to view the source code.)\n\n!chat_app"}
{"text_chunk": "Advance usage - hybrid stream and non-stream flow output\nSometimes, you may want to get both stream and non-stream results from a flow output. For example, in the \u201cChat with Wikipedia\u201d flow, you may want to get not only LLM\u2019s answer, but also the list of URLs that the flow searched. To do this, you need to modify the flow to output a combination of stream LLM\u2019s answer and non-stream URL list.\n\nIn the sample \"Chat With Wikipedia\" flow, the output is connected to the LLM node `augmented_chat`. To add the URL list to the output, you need to add an output field with the name `url` and the value `${get_wiki_url.output}`.\n\n!chat_wikipedia_dual_output_center.png\n\nThe output of the flow will be a non-stream field as the base and a stream field as the delta. Here is an example of request and response."}
{"text_chunk": "0. The client sends a message to the server.\n```\nPOST https://.inference.ml.azure.com/score\nContent-Type: application/json\nAuthorization: Bearer \nAccept: text/event-stream\n{\n    \"question\": \"When was ChatGPT launched?\",\n    \"chat_history\": []\n}\n```"}
{"text_chunk": "1. The server sends back the answer in streaming mode.\n```\nHTTP/1.1 200 OK\nContent-Type: text/event-stream; charset=utf-8\nConnection: close\nTransfer-Encoding: chunked\n\ndata: {\"url\": [\"https://en.wikipedia.org/w/index.php?search=ChatGPT\", \"https://en.wikipedia.org/w/index.php?search=GPT-4\"]}\n\ndata: {\"answer\": \"\"}\n\ndata: {\"answer\": \"Chat\"}\n\ndata: {\"answer\": \"G\"}\n\ndata: {\"answer\": \"PT\"}\n\ndata: {\"answer\": \" was\"}\n\ndata: {\"answer\": \" launched\"}\n\ndata: {\"answer\": \" on\"}\n\ndata: {\"answer\": \" November\"}\n\ndata: {\"answer\": \" \"}\n\ndata: {\"answer\": \"30\"}\n\ndata: {\"answer\": \",\"}\n\ndata: {\"answer\": \" \"}\n\ndata: {\"answer\": \"202\"}\n\ndata: {\"answer\": \"2\"}\n\ndata: {\"answer\": \".\"}\n\ndata: {\"answer\": \" \\n\\n\"}\n\n...\n\ndata: {\"answer\": \"PT\"}\n\ndata: {\"answer\": \"\"}\n```"}
{"text_chunk": "2. The client sends another chat message, along with the full chat history, to the server.\n```\nPOST https://.inference.ml.azure.com/score\nContent-Type: application/json\nAuthorization: Bearer \nAccept: text/event-stream\n{\n    \"question\": \"When did OpenAI announce GPT-4? How long is it between these two milestones?\",\n    \"chat_history\": [\n        {\n            \"inputs\": {\n                \"question\": \"When was ChatGPT launched?\"\n            },\n            \"outputs\": {\n                \"url\": [\n                    \"https://en.wikipedia.org/w/index.php?search=ChatGPT\",\n                    \"https://en.wikipedia.org/w/index.php?search=GPT-4\"\n                ],\n                \"answer\": \"ChatGPT was launched on November 30, 2022. \\n\\nSOURCES: https://en.wikipedia.org/w/index.php?search=ChatGPT\"\n            }\n        }\n    ]\n}\n```"}
{"text_chunk": "3. The server sends back the answer in streaming mode.\n```\nHTTP/1.1 200 OK\nContent-Type: text/event-stream; charset=utf-8\nConnection: close\nTransfer-Encoding: chunked\n\ndata: {\"url\": [\"https://en.wikipedia.org/w/index.php?search=Generative pre-trained transformer \", \"https://en.wikipedia.org/w/index.php?search=Microsoft \"]}\n\ndata: {\"answer\": \"\"}\n\ndata: {\"answer\": \"Open\"}\n\ndata: {\"answer\": \"AI\"}\n\ndata: {\"answer\": \" released\"}\n\ndata: {\"answer\": \" G\"}\n\ndata: {\"answer\": \"PT\"}\n\ndata: {\"answer\": \"-\"}\n\ndata: {\"answer\": \"4\"}\n\ndata: {\"answer\": \" in\"}\n\ndata: {\"answer\": \" March\"}\n\ndata: {\"answer\": \" \"}\n\ndata: {\"answer\": \"202\"}\n\ndata: {\"answer\": \"3\"}\n\ndata: {\"answer\": \".\"}\n\ndata: {\"answer\": \" Chat\"}\n\ndata: {\"answer\": \"G\"}\n\ndata: {\"answer\": \"PT\"}\n\ndata: {\"answer\": \" was\"}\n\ndata: {\"answer\": \" launched\"}\n\ndata: {\"answer\": \" on\"}\n\ndata: {\"answer\": \" November\"}\n\ndata: {\"answer\": \" \"}\n\ndata: {\"answer\": \"30\"}\n\ndata: {\"answer\": \",\"}\n\ndata: {\"answer\": \" \"}\n\ndata: {\"answer\": \"202\"}\n\ndata: {\"answer\": \"2\"}\n\ndata: {\"answer\": \".\"}\n\ndata: {\"answer\": \" The\"}\n\ndata: {\"answer\": \" time\"}\n\ndata: {\"answer\": \" between\"}\n\ndata: {\"answer\": \" these\"}\n\ndata: {\"answer\": \" two\"}\n\ndata: {\"answer\": \" milestones\"}\n\ndata: {\"answer\": \" is\"}\n\ndata: {\"answer\": \" approximately\"}\n\ndata: {\"answer\": \" \"}\n\ndata: {\"answer\": \"3\"}\n\ndata: {\"answer\": \" months\"}\n\ndata: {\"answer\": \".\\n\\n\"}\n\n...\n\ndata: {\"answer\": \"Chat\"}\n\ndata: {\"answer\": \"G\"}\n\ndata: {\"answer\": \"PT\"}\n\ndata: {\"answer\": \"\"}\n```"}
{"text_chunk": "Execute flow as a function\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::"}
{"text_chunk": "Overview\n\nPromptflow allows you to load a flow and use it as a function in your code.\nThis feature is useful when building a service on top of a flow, reference here for a simple example service with flow function consumption."}
{"text_chunk": "Load an invoke the flow function\n\nTo use the flow-as-function feature, you first need to load a flow using the `load_flow` function.\nThen you can consume the flow object like a function by providing key-value arguments for it.\n\n```python\nf = load_flow(\"../../examples/flows/standard/web-classification/\")\nf(url=\"sample_url\")\n```"}
{"text_chunk": "Config the flow with context\n\nYou can overwrite some flow configs before flow function execution by setting `flow.context`."}
{"text_chunk": "Load flow as a function with in-memory connection override\n\nBy providing a connection object to flow context, flow won't need to get connection in execution time, which can save time when for cases where flow function need to be called multiple times.\n\n```python\nfrom promptflow.entities import AzureOpenAIConnection\n\nconnection_obj = AzureOpenAIConnection(\n    name=conn_name,\n    api_key=api_key,\n    api_base=api_base,\n    api_type=\"azure\",\n    api_version=api_version,\n)"}
{"text_chunk": "no need to create the connection object.\nf.context = FlowContext(\n    connections={\"classify_with_llm\": {\"connection\": connection_obj}}\n)\n```"}
{"text_chunk": "Local flow as a function with flow inputs override\n\nBy providing overrides, the original flow dag will be updated in execution time.\n\n```python\nf.context = FlowContext(\n    # node \"fetch_text_content_from_url\" will take inputs from the following command instead of from flow input\n    overrides={\"nodes.fetch_text_content_from_url.inputs.url\": sample_url},\n)\n```\n\n**Note**, the `overrides` are only doing YAML content replacement on original `flow.dag.yaml`.\nIf the `flow.dag.yaml` become invalid after `overrides`, validation error will be raised when executing."}
{"text_chunk": "Load flow as a function with streaming output\n\nAfter set `streaming` in flow context, the flow function will return an iterator to stream the output.\n\n```python\nf = load_flow(source=\"../../examples/flows/chat/basic-chat/\")\nf.context.streaming = True\nresult = f(\n    chat_history=[\n        {\n            \"inputs\": {\"chat_input\": \"Hi\"},\n            \"outputs\": {\"chat_output\": \"Hello! How can I assist you today?\"},\n        }\n    ],\n    question=\"How are you?\",\n)\n\n\nanswer = \"\""}
{"text_chunk": "the result will be a generator, iterate it to get the result\nfor r in result[\"answer\"]:\n    answer += r\n\n```\n\nReference our sample for usage."}
{"text_chunk": "Next steps\n\nLearn more about:\n\n- Flow as a function sample\n- Deploy a flow"}
{"text_chunk": "Frequency asked questions (FAQ)"}
{"text_chunk": "General"}
{"text_chunk": "Stable vs experimental\n\nPrompt flow provides both stable and experimental features in the same SDK.\n\n|Feature status | Description | \n|----------------|----------------|\nStable features\t| **Production ready**  These features are recommended for most use cases and production environments. They are updated less frequently then experimental features.|\nExperimental features | **Developmental**   These features are newly developed capabilities & updates that may not be ready or fully tested for production usage. While the features are typically functional, they can include some breaking changes. Experimental features are used to iron out SDK breaking bugs, and will only receive updates for the duration of the testing period. Experimental features are also referred to as features that are in **preview**.  As the name indicates, the experimental (preview) features are for experimenting and is **not considered bug free or stable**. For this reason, we only recommend experimental features to advanced users who wish to try out early versions of capabilities and updates, and intend to participate in the reporting of bugs and glitches."}
{"text_chunk": "OpenAI 1.x support\nPlease use the following command to upgrade promptflow for openai 1.x support:\n```\npip install promptflow>=1.1.0\npip install promptflow-tools>=1.0.0\n```\nNote that the command above will upgrade your openai package a version later than 1.0.0, \nwhich may introduce breaking changes to custom tool code.\n\nReach OpenAI migration guide for more details."}
{"text_chunk": "Troubleshooting"}
{"text_chunk": "Connection creation failed with StoreConnectionEncryptionKeyError\n\n```\nConnection creation failed with StoreConnectionEncryptionKeyError: System keyring backend service not found in your operating system. See https://pypi.org/project/keyring/ to install requirement for different operating system, or 'pip install keyrings.alt' to use the third-party backend.\n```\n\nThis error raised due to keyring can't find an available backend to store keys.\nFor example macOS Keychain and Windows Credential Locker\nare valid keyring backends.\n\nTo resolve this issue, install the third-party keyring backend or write your own keyring backend, for example:\n`pip install keyrings.alt`\n\nFor more detail about keyring third-party backend, please refer to 'Third-Party Backends' in keyring."}
{"text_chunk": "Pf visualize show error: \"tcgetpgrp failed: Not a tty\"\n\nIf you are using WSL, this is a known issue for `webbrowser` under WSL; see this issue for more information. Please try to upgrade your WSL to 22.04 or later, this issue should be resolved.\n\nIf you are still facing this issue with WSL 22.04 or later, or you are not even using WSL, please open an issue to us."}
{"text_chunk": "Installed tool not appearing in VSCode Extension tool list\n\nAfter installing a tool package via `pip install [tool-package-name]`, the new tool may not immediately appear in the tool list within the VSCode Extension, as shown below:\n\n!VSCode Extension tool list\n\nThis is often due to outdated cache. To refresh the tool list and make newly installed tools visible:\n\n1. Open the VSCode Extension window.\n\n2. Bring up the command palette by pressing \"Ctrl+Shift+P\".\n\n3. Type and select the \"Developer: Reload Webviews\" command. \n\n4. Wait a moment for the tool list refreshing.\n\nReloading clears the previous cache and populates the tool list with any newly installed tools. So that the missing tools are now visible."}
{"text_chunk": "Set logging level\n\nPromptflow uses `logging` module to log messages. You can set logging level via environment variable `PF_LOGGING_LEVEL`, valid values includes `CRITICAL`, `ERROR`, `WARNING`, `INFO`, `DEBUG`, default to `INFO`.\nBelow is the serving logs after setting `PF_LOGGING_LEVEL` to `DEBUG`:\n\n!img\n\nCompare to the serving logs with `WARNING` level:\n\n!img"}
{"text_chunk": "Set environment variables\n\nCurrently, promptflow supports the following environment variables:\n\n**PF_WORKER_COUNT** \n\nValid for batch run only. The number of workers to use for parallel execution of the Flow.\n\nDefault value is 16. If you have large number of batch run date row count, and want more efficiency, you can increase the PF_WORKER_COUNT to improve the batch run concurrency, make it run faster.\n\nWhen you modify the concurrency, please consider 2 points:\n\nFirst, the concurrency should be not bigger than your batch run data row count.  If not, meaning if the concurrency is bigger, it will run slower due to the time taken for process startup and shutdown.\n\nSecond, your batch run risks to fail due to rate limit of your LLM endpoint, in this case you need to set up PF_WORKER_COUNT to a smaller number. Take Azure OpenAI endpoint as example, you can go to Azure OpenAI Studio, navigate to Deployment tab, check out the capacity of your endpoints. Then you can refer to this expression to set up the concurrency.\n\n```\nPF_WORKER_COUNT <= TPM * duration_seconds / token_count / 60\n```\nTPM: token per minute, capacity rate limit of your LLM endpoint\n\nduration_seconds: single flow run duration in seconds\n\ntoken_count: single flow run token count\n\n\nFor example, if your endpoint TPM (token per minute) is 50K, the single flow run takes 10k tokens and runs for 30s, pls do not set up PF_WORKER_COUNT bigger than 2. This is a rough estimation. Please also consider collboaration (teammates use the same endpoint at the same time) and tokens consumed in deployed inference endpoints, playground and other cases which might send request to your LLM endpoints.\n\n**PF_BATCH_METHOD**\n\nValid for batch run only. Optional values: 'spawn', 'fork'. \n\n**spawn**\n\n1. The child processes will not inherit resources of the parent process, therefore, each process needs to reinitialize the resources required for the flow, which may use more system memory.\n\n2. Starting a process is slow because it will take some time to initialize the necessary resources.\n \n**fork**\n\n1. Use the copy-on-write mechanism, the child processes will inherit all the resources of the parent process, thereby using less system memory.\n\n2. The process starts faster as it doesn't need to reinitialize resources.\n \nNote: Windows only supports spawn, Linux and macOS support both spawn and fork."}
{"text_chunk": "You can configure environment variables in the following ways\n\n1. If you are using CLI, you can use this parameter: ```--environment-variable```. Example: ```--environment-variable PF_WORKER_COUNT=\"2\" PF_BATCH_METHOD=\"spawn\"```.\n\n2. If you are using SDK, you can specify environment variables when creating run. Example: \n\n``` python\n    pf = PFClient(\n        credential=credential,\n        subscription_id=\"\",\n        resource_group_name=\"\",\n        workspace_name=\"\",\n    )\n\n    flow = \"web-classification\"\n    data = \"web-classification/data.jsonl\"\n    runtime = \"example-runtime-ci\"\n\n    environment_variables = {\"PF_WORKER_COUNT\": \"2\", \"PF_BATCH_METHOD\": \"spawn\"}\n\n    # create run\n    base_run = pf.run(\n        flow=flow,\n        data=data,\n        runtime=runtime,\n        environment_variables=environment_variables,\n    )\n```\n\n3. If you are using VSCode Extension to submit batch run, you can configure environment variables in the ```batch_run_create.yaml```. Example:\n\n``` yaml\n    name: flow_name\n    display_name: display_name\n    flow: flow_folder\n    data: data_file\n    column_mapping:\n        customer_info: \n        history: \n    environment_variables:\n        PF_WORKER_COUNT: \"2\"\n        PF_BATCH_METHOD: \"spawn\"\n```"}
{"text_chunk": "Initialize and test a flow\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nFrom this document, customer can initialize a flow and test it."}
{"text_chunk": "Initialize flow\n\nCreating a flow folder with code/prompts and yaml definitions of the flow."}
{"text_chunk": "Initialize flow from scratch\n\nPromptflow can create three types of flow folder:\n- standard: Basic structure of flow folder.\n- chat: Chat flow is designed for conversational application development, building upon the capabilities of standard flow and providing enhanced support for chat inputs/outputs and chat history management.\n- evaluation: Evaluation flows are special types of flows that assess how well the outputs of a flow align with specific criteria and goals.\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\n```bash"}
{"text_chunk": "Create a flow\npf flow init --flow"}
{"text_chunk": "Create a chat flow\npf flow init --flow  --type chat\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\nUse VS Code explorer pane > directory icon > right click > the \"New flow in this directory\" action. Follow the popped out dialog to initialize your flow in the target folder.\n!img\n\nAlternatively, you can use the \"Create new flow\" action on the prompt flow pane > quick access section to create a new flow\n!img\n\n:::\n\n::::\n\n\nStructure of flow folder:\n- **flow.dag.yaml**: The flow definition with inputs/outputs, nodes, tools and variants for authoring purpose.\n- **.promptflow/flow.tools.json**: It contains tools meta referenced in `flow.dag.yaml`.\n- **Source code files (.py, .jinja2)**: User managed, the code scripts referenced by tools.\n- **requirements.txt**: Python package dependencies for this flow.\n\n!init_flow_folder"}
{"text_chunk": "Create from existing code\n\nCustomer needs to pass the path of tool script to `entry`, and also needs to pass in the promptflow template dict to `prompt-template`, which the key is the input name of the tool and the value is the path to the promptflow template.\nPromptflow CLI can generate the yaml definitions needed for prompt flow from the existing folder, using the tools script and prompt templates.\n\n```bash"}
{"text_chunk": "Create a flow in existing folder\npf flow init --flow  --entry  --function  --prompt-template =\n```\n\nTake customer-intent-extraction for example, which demonstrating how to convert a langchain code into a prompt flow.\n\n!init_output\n\nIn this case, promptflow CLI generates `flow.dag.yaml`, `.promptflow/flow.tools.json`  and `extract_intent_tool.py`, it is a python tool in the flow.\n\n!init_files"}
{"text_chunk": "Test a flow\n\n:::{admonition} Note\nTesting flow will NOT create a batch run record, therefore it's unable to use commands like `pf run show-details` to get the run information. If you want to persist the run record, see Run and evaluate a flow\n:::\n\nPromptflow also provides ways to test the initialized flow or flow node. It will help you quickly test your flow."}
{"text_chunk": "Visual editor on the VS Code for prompt flow.\n\n::::{tab-set}\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\nOpen the flow.dag.yaml file of your flow. On the top of the yaml editor you can find the \"Visual editor\" action. Use it to open the Visual editor with GUI support.\n\n!img\n:::\n\n::::"}
{"text_chunk": "Test flow\n\nCustomer can use CLI or VS Code extension to test the flow.\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\n```bash"}
{"text_chunk": "Test flow\npf flow test --flow"}
{"text_chunk": "Test flow with specified variant\npf flow test --flow  --variant '${.}'\n```\n\nThe log and result of flow test will be displayed in the terminal.\n\n!flow test\n\nPromptflow CLI will generate test logs and outputs in `.promptflow`:\n- **flow.detail.json**: Defails info of flow test, include the result of each node.\n- **flow.log**: The log of flow test.\n- **flow.output.json**: The result of flow test.\n\n!flow_output_files\n\n:::\n\n:::{tab-item} SDK\n:sync: SDK\n\nThe return value of `test` function is the flow outputs.\n\n```python\nfrom promptflow import PFClient\n\npf_client = PFClient()"}
{"text_chunk": "Test flow\ninputs = {\"\": \"\"}  # The inputs of the flow.\nflow_result = pf_client.test(flow=\"\", inputs=inputs)\nprint(f\"Flow outputs: {flow_result}\")\n```\n\nThe log and result of flow test will be displayed in the terminal.\n\n!flow test\n\nPromptflow CLI will generate test logs and outputs in `.promptflow`:\n- **flow.detail.json**: Defails info of flow test, include the result of each node.\n- **flow.log**: The log of flow test.\n- **flow.output.json**: The result of flow test.\n\n!flow_output_files\n\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\nYou can use the action either on the default yaml editor or the visual editor to trigger flow test. See the snapshots below:\n!img\n!img\n\n:::\n\n::::"}
{"text_chunk": "Test a single node in the flow\n\nCustomer can test a single python node in the flow. It will use customer provides date or the default value of the node as input. It will only use customer specified node to execute with the input.\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nCustomer can execute this command to test the flow.\n\n```bash"}
{"text_chunk": "Test flow node\npf flow test --flow  --node \n```\n\nThe log and result of flow node test will be displayed in the terminal. And the details of node test will generated to `.promptflow/flow-.node.detail.json`.\n\n:::\n\n:::{tab-item} SDK\n:sync: SDK\n\nCustomer can execute this command to test the flow. The return value of `test` function is the node outputs.\n\n```python\nfrom promptflow import PFClient\n\npf_client = PFClient()"}
{"text_chunk": "Test not iun the flow\ninputs = {: }  # The inputs of the node.\nnode_result = pf_client.test(flow=, inputs=inputs, node=)\nprint(f\"Node outputs: {node_result}\")\n```\n\nThe log and result of flow node test will be displayed in the terminal. And the details of node test will generated to `.promptflow/flow-.node.detail.json`.\n\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\nThe prompt flow extension provides inline actions in both default yaml editor and visual editor to trigger single node runs.\n\n!img\n!img\n\n:::\n\n::::"}
{"text_chunk": "Test with interactive mode\n\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nPromptflow CLI provides a way to start an interactive chat session for chat flow. Customer can use below command to start an interactive chat session:\n\n```bash"}
{"text_chunk": "Chat in the flow\npf flow test --flow  --interactive\n```\n\nAfter executing this command, customer can interact with the chat flow in the terminal. Customer can press **Enter** to send the message to chat flow. And customer can quit with **ctrl+C**.\nPromptflow CLI will distinguish the output of different roles by color, User input, Bot output, Flow script output, Node output.\n\nUsing this chat flow to show how to use interactive mode.\n\n!chat\n\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\nIf a flow contains chat inputs or chat outputs in the flow interface, there will be a selection when triggering flow test. You can select the interactive mode if you want to.\n\n!img\n!img\n\n:::\n\n::::\n\nWhen the LLM node in the chat flow that is connected to the flow output, Promptflow SDK streams the results of the LLM node.\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nThe flow result will be streamed in the terminal as shown below.\n\n!streaming_output\n\n:::\n\n:::{tab-item} SDK\n:sync: SDK\n\nThe LLM node return value of `test` function is a generator, you can consume the result by this way:\n\n```python\nfrom promptflow import PFClient\n\npf_client = PFClient()"}
{"text_chunk": "Test flow\ninputs = {\"\": \"\"}  # The inputs of the flow.\nflow_result = pf_client.test(flow=\"\", inputs=inputs)\nfor item in flow_result[\"\"]:\n    print(item)\n```\n\n:::\n\n::::"}
{"text_chunk": "Debug a single node in the flow\n\nCustomer can debug a single python node in VScode by the extension.\n\n::::{tab-set}\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\nBreak points and debugging functionalities for the Python steps in your flow. Just set the break points and use the debug actions on either default yaml editor or visual editor.\n!img\n!img\n\n:::\n\n::::"}
{"text_chunk": "Next steps\n\n- Add conditional control to a flow"}
{"text_chunk": "Manage connections\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nConnection helps securely store and manage secret keys or other sensitive credentials required for interacting with LLM (Large Language Models) and other external tools, for example, Azure Content Safety.\n\n:::{note}\nTo use azureml workspace connection locally, refer to this guide.\n:::"}
{"text_chunk": "Connection types\nThere are multiple types of connections supported in promptflow, which can be simply categorized into **strong type connection** and **custom connection**. The strong type connection includes AzureOpenAIConnection, OpenAIConnection, etc. The custom connection is a generic connection type that can be used to store custom defined credentials.\n\nWe are going to use AzureOpenAIConnection as an example for strong type connection, and CustomConnection to show how to manage connections."}
{"text_chunk": "Create a connection\n\n:::{note}\nIf you are using `WSL` or other OS without default keyring storage backend, you may encounter `StoreConnectionEncryptionKeyError`, please refer to FAQ for the solutions.\n:::\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nEach of the strong type connection has a corresponding yaml schema, the example below shows the AzureOpenAIConnection yaml:\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/AzureOpenAIConnection.schema.json\nname: azure_open_ai_connection\ntype: azure_open_ai\napi_key: \"\"\napi_base: \"https://.openai.azure.com/\"\napi_type: \"azure\"\napi_version: \"2023-03-15-preview\"\n```\nThe custom connection yaml will have two dict fields for secrets and configs, the example below shows the CustomConnection yaml:\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/CustomConnection.schema.json\nname: custom_connection\ntype: custom\nconfigs:\n  endpoint: \"\"\n  other_config: \"other_value\"\nsecrets:  # required\n  my_key: \"\"\n```\nAfter preparing the yaml file, use the CLI command below to create them:\n```bash"}
{"text_chunk": "Override keys with --set to avoid yaml file changes\npf connection create -f  --set api_key="}
{"text_chunk": "Create the custom connection\npf connection create -f  --set configs.endpoint= secrets.my_key=\n```\nThe expected result is as follows if the connection created successfully.\n\n!img\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nUsing SDK, each connection type has a corresponding class to create a connection. The following code snippet shows how to import the required class and create the connection:\n\n```python\nfrom promptflow import PFClient\nfrom promptflow.entities import AzureOpenAIConnection, CustomConnection"}
{"text_chunk": "Get a pf client to manage connections\npf = PFClient()"}
{"text_chunk": "Initialize an AzureOpenAIConnection object\nconnection = AzureOpenAIConnection(\n    name=\"my_azure_open_ai_connection\", \n    api_key=\"\", \n    api_base=\"\"\n    api_version=\"2023-03-15-preview\"\n)"}
{"text_chunk": "Create the connection, note that api_key will be scrubbed in the returned result\nresult = pf.connections.create_or_update(connection)\nprint(result)"}
{"text_chunk": "Initialize a custom connection object\nconnection = CustomConnection(\n    name=\"my_custom_connection\", \n    # Secrets is a required field for custom connection\n    secrets={\"my_key\": \"\"},\n    configs={\"endpoint\": \"\", \"other_config\": \"other_value\"}\n)"}
{"text_chunk": "Create the connection, note that all secret values will be scrubbed in the returned result\nresult = pf.connections.create_or_update(connection)\nprint(result)\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n\nOn the VS Code primary sidebar > prompt flow pane. You can find the connections pane to manage your local connections. Click the \"+\" icon on the top right of it and follow the popped out instructions to create your new connection.\n\n!img\n!img\n:::\n::::"}
{"text_chunk": "Update a connection\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nThe commands below show how to update existing connections with new values:\n```bash"}
{"text_chunk": "Update an azure open ai connection with a new api base\npf connection update -n my_azure_open_ai_connection --set api_base='new_value'"}
{"text_chunk": "Update a custom connection\npf connection update -n my_custom_connection --set configs.other_config='new_value'\n```\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nThe code snippet below shows how to update existing connections with new values:\n```python"}
{"text_chunk": "Update an azure open ai connection with a new api base\nconnection = pf.connections.get(name=\"my_azure_open_ai_connection\")\nconnection.api_base = \"new_value\"\nconnection.api_key = \"\"  # secrets are required when updating connection using sdk\nresult = pf.connections.create_or_update(connection)\nprint(connection)"}
{"text_chunk": "Update a custom connection\nconnection = pf.connections.get(name=\"my_custom_connection\")\nconnection.configs[\"other_config\"] = \"new_value\"\nconnection.secrets = {\"key1\": \"val1\"}  # secrets are required when updating connection using sdk\nresult = pf.connections.create_or_update(connection)\nprint(connection)\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n\nOn the VS Code primary sidebar > prompt flow pane. You can find the connections pane to manage your local connections. Right click the item of the connection list to update or delete your connections.\n!img\n:::\n::::"}
{"text_chunk": "List connections\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nList connection command will return the connections with json list format, note that all secrets and api keys will be scrubbed:\n```bash\npf connection list\n```\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nList connection command will return the connections object list, note that all secrets and api keys will be scrubbed:\n```python\nfrom promptflow import PFClient"}
{"text_chunk": "Get a pf client to manage connections\npf = PFClient()"}
{"text_chunk": "List and print connections\nconnection_list = pf.connections.list()\nfor connection in connection_list:\n    print(connection)\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n!img\n:::\n::::"}
{"text_chunk": "Delete a connection\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nDelete a connection with the following command:\n```bash\npf connection delete -n \n```\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nDelete a connection with the following code snippet:\n```python\nfrom promptflow import PFClient"}
{"text_chunk": "Get a pf client to manage connections\npf = PFClient()"}
{"text_chunk": "Delete the connection with specific name\nclient.connections.delete(name=\"my_custom_connection\")\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n\nOn the VS Code primary sidebar > prompt flow pane. You can find the connections pane to manage your local connections. Right click the item of the connection list to update or delete your connections.\n!img\n:::\n::::"}
{"text_chunk": "Next steps\n- Reach more detail about connection concepts.\n- Try the connection samples.\n- Consume connections from Azure AI."}
{"text_chunk": "Manage runs\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nThis documentation will walk you through how to manage your runs with CLI, SDK and VS Code Extension. \n\nIn general:\n- For `CLI`, you can run `pf/pfazure run --help` in terminal to see the help messages.\n- For `SDK`, you can refer to Promptflow Python Library Reference and check `PFClient.runs` for more run operations.\n\nLet's take a look at the following topics:\n\n- Manage runs\n  - Create a run\n  - Get a run\n  - Show run details\n  - Show run metrics\n  - Visualize a run\n  - List runs\n  - Update a run\n  - Archive a run\n  - Restore a run"}
{"text_chunk": "Create a run\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\nTo create a run against bulk inputs, you can write the following YAML file.\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Run.schema.json\nflow: ../web_classification\ndata: ../webClassification1.jsonl\ncolumn_mapping:\n   url: \"${data.url}\"\nvariant: ${summarize_text_content.variant_0}\n```\n\nTo create a run against existing run, you can write the following YAML file.\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Run.schema.json\nflow: ../classification_accuracy_evaluation\ndata: ../webClassification1.jsonl\ncolumn_mapping:\n   groundtruth: \"${data.answer}\"\n   prediction: \"${run.outputs.category}\"\nrun: \n```\n\nReference here for detailed information for column mapping.\nYou can find additional information about flow yaml schema in Run YAML Schema.\n\nAfter preparing the yaml file, use the CLI command below to create them:\n\n```bash"}
{"text_chunk": "create the flow run\npf run create -f"}
{"text_chunk": "create the flow run and stream output\npf run create -f  --stream\n```\n\nThe expected result is as follows if the run is created successfully.\n\n!img\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nUsing SDK, create `Run` object and submit it with `PFClient`. The following code snippet shows how to import the required class and create the run:\n\n```python\nfrom promptflow import PFClient\nfrom promptflow.entities import Run"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "Initialize an Run object\nrun = Run( \n    flow=\"\",\n    # run flow against local data or existing run, only one of data & run can be specified. \n    data=\"\",\n    run=\"\",\n    column_mapping={\"url\": \"${data.url}\"},\n    variant=\"${summarize_text_content.variant_0}\"\n)"}
{"text_chunk": "Create the run\nresult = pf.runs.create_or_update(run)\nprint(result)\n\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\nYou can click on the actions on the top of the default yaml editor or the visual editor for the flow.dag.yaml files to trigger flow batch runs.\n\n!img\n!img\n:::\n::::"}
{"text_chunk": "Get a run\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nGet a run in CLI with JSON format.\n\n```bash\npf run show --name \n```\n\n!img\n\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nShow run with `PFClient`\n```python\nfrom promptflow import PFClient"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "Get and print the run\nrun = pf.runs.get(name=\"\")\nprint(run)\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n!img\n:::\n::::"}
{"text_chunk": "Show run details\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nGet run details with TABLE format.\n\n```bash\npf run show --name \n```\n\n!img\n\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nShow run details with `PFClient`\n```python\nfrom promptflow import PFClient\nfrom tabulate import tabulate"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "Get and print the run-details\nrun_details = pf.runs.get_details(name=\"\")\nprint(tabulate(details.head(max_results), headers=\"keys\", tablefmt=\"grid\"))\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n!img\n:::\n::::"}
{"text_chunk": "Show run metrics\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nGet run metrics with JSON format.\n\n```bash\npf run show-metrics --name \n```\n\n!img\n\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nShow run metrics with `PFClient`\n```python\nfrom promptflow import PFClient\nimport json"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "Get and print the run-metrics\nrun_details = pf.runs.get_metrics(name=\"\")\nprint(json.dumps(metrics, indent=4))\n```\n:::\n\n::::"}
{"text_chunk": "Visualize a run\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nVisualize run in browser.\n\n```bash\npf run visualize --name \n```\n\nA browser will open and display run outputs.\n\n!img\n\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nVisualize run with `PFClient`\n```python\nfrom promptflow import PFClient"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "Visualize the run\nclient.runs.visualize(name=\"\")\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n\nOn the VS Code primary sidebar > the prompt flow pane, there is a run list. It will list all the runs on your machine. Select one or more items and click the \"visualize\" button on the top-right to visualize the local runs.\n\n!img\n:::\n::::"}
{"text_chunk": "List runs\n\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nList runs with JSON format.\n\n```bash\npf run list\n```\n\n!img\n\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nList with `PFClient`\n```python\nfrom promptflow import PFClient"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "list runs\nruns = pf.runs.list()\nprint(runs)\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n\nOn the VS Code primary sidebar > the prompt flow pane, there is a run list. It will list all the runs on your machine. Hover on it to view more details.\n!img\n:::\n::::"}
{"text_chunk": "Update a run\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nGet run metrics with JSON format.\n\n```bash\npf run update --name  --set display_name=new_display_name\n```\n\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nUpdate run with `PFClient`\n```python\nfrom promptflow import PFClient"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "Get and print the run-metrics\nrun = pf.runs.update(name=\"\", display_name=\"new_display_name\")\nprint(run)\n```\n:::\n::::"}
{"text_chunk": "Archive a run\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nArchive the run so it won't show in run list results.\n\n```bash\npf run archive --name \n```\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nArchive with `PFClient`\n```python\nfrom promptflow import PFClient"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "archive a run\nclient.runs.archive(name=\"\")\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VSC\n!img\n:::\n::::"}
{"text_chunk": "Restore a run\n\n::::{tab-set}\n:::{tab-item} CLI\n:sync: CLI\n\nRestore an archived run so it can show in run list results.\n\n```bash\npf run restore --name \n```\n:::\n\n\n:::{tab-item} SDK\n:sync: SDK\nRestore with `PFClient`\n```python\nfrom promptflow import PFClient"}
{"text_chunk": "Get a pf client to manage runs\npf = PFClient()"}
{"text_chunk": "restore a run\nclient.runs.restore(name=\"\")\n```\n:::\n::::"}
{"text_chunk": "Process image in flow\nPromptFlow defines a contract to represent image data."}
{"text_chunk": "Data class\n`promptflow.contracts.multimedia.Image`\nImage class is a subclass of `bytes`, thus you can access the binary data by directly using the object. It has an extra attribute `source_url` to store the origin url of the image, which would be useful if you want to pass the url instead of content of image to APIs like GPT-4V model."}
{"text_chunk": "Data type in flow input\nSet the type of flow input to `image` and promptflow will treat it as an image."}
{"text_chunk": "Reference image in prompt template\nIn prompt templates that support image (e.g. in OpenAI GPT-4V tool), using markdown syntax to denote that a template input is an image: `!image`. In this case, `test_image` will be substituted with base64 or source_url (if set) before sending to LLM model."}
{"text_chunk": "Serialization/Deserialization\nPromptflow uses a special dict to represent image.\n`{\"data:image/;\": \"\"}`\n\n- `` can be html standard mime image types. Setting it to specific type can help previewing the image correctly, or it can be `*` for unknown type.\n- `` is the image serialized representation, there are 3 supported types:\n\n    - url\n\n        It can point to a public accessable web url. E.g.\n\n        {\"data:image/png;url\": \"https://developer.microsoft.com/_devcom/images/logo-ms-social.png\"}\n    - base64\n\n        It can be the base64 encoding of the image. E.g.\n\n        {\"data:image/png;base64\": \"iVBORw0KGgoAAAANSUhEUgAAAGQAAABLAQMAAAC81rD0AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAABlBMVEUAAP7////DYP5JAAAAAWJLR0QB/wIt3gAAAAlwSFlzAAALEgAACxIB0t1+/AAAAAd0SU1FB+QIGBcKN7/nP/UAAAASSURBVDjLY2AYBaNgFIwCdAAABBoAAaNglfsAAAAZdEVYdGNvbW1lbnQAQ3JlYXRlZCB3aXRoIEdJTVDnr0DLAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIwLTA4LTI0VDIzOjEwOjU1KzAzOjAwkHdeuQAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMC0wOC0yNFQyMzoxMDo1NSswMzowMOEq5gUAAAAASUVORK5CYII=\"}\n\n    - path\n\n        It can reference an image file on local disk. Both absolute path and relative path are supported, but in the cases where the serialized image representation is stored in a file, relative to the containing folder of that file is recommended, as in the case of flow IO data. E.g.\n\n        {\"data:image/png;path\": \"./my-image.png\"}\n\nPlease note that `path` representation is not supported in Deployment scenario."}
{"text_chunk": "Batch Input data\nBatch input data containing image can be of 2 formats:\n1. The same jsonl format of regular batch input, except that some column may be seriliazed image data or composite data type (dict/list) containing images. The serialized images can only be Url or Base64. E.g.\n    ```json\n    {\"question\": \"How many colors are there in the image?\", \"input_image\": {\"data:image/png;url\": \"https://developer.microsoft.com/_devcom/images/logo-ms-social.png\"}}\n    {\"question\": \"What's this image about?\", \"input_image\": {\"data:image/png;url\": \"https://developer.microsoft.com/_devcom/images/404.png\"}}\n    ```\n2. A folder containing a jsonl file under root path, which contains serialized image in File Reference format. The referenced file are stored in the folder and their relative path to the root path is used as path in the file reference. Here is a sample batch input, note that the name of `input.jsonl` is arbitrary as long as it's a jsonl file:\n    ```\n    BatchInputFolder\n    |----input.jsonl\n    |----image1.png\n    |----image2.png\n    ```\n    Content of `input.jsonl`\n    ```json\n    {\"question\": \"How many colors are there in the image?\", \"input_image\": {\"data:image/png;path\": \"image1.png\"}}\n    {\"question\": \"What's this image about?\", \"input_image\": {\"data:image/png;path\": \"image2.png\"}}\n    ```"}
{"text_chunk": "Quick Start\n\nThis guide will walk you through the fist step using of prompt flow code-first experience.\n\n**Prerequisite** - To make the most of this tutorial, you'll need:\n\n- Know how to program with Python :)\n- A basic understanding of Machine Learning can be beneficial, but it's not mandatory.\n\n\n**Learning Objectives** - Upon completing this tutorial, you should learn how to:\n- Setup your python environment to run prompt flow\n- Clone a sample flow & understand what's a flow\n- Understand how to edit the flow using visual editor or yaml\n- Test the flow using your favorite experience: CLI, SDK or VS Code Extension."}
{"text_chunk": "Set up your dev environment\n\n1. A python environment with version `python=3.9` or higher version like 3.10. It's recommended to use python environment manager miniconda. After you have installed miniconda, run below commands to create a python environment:\n```bash\nconda create --name pf python=3.9\nconda activate pf\n```\n\n2. Install `promptflow` and `promptflow-tools`.\n```sh\npip install promptflow promptflow-tools\n```\n\n3. Check the installation.\n```bash"}
{"text_chunk": "should print promptflow version, e.g. \"0.1.0b3\"\npf -v\n```"}
{"text_chunk": "Understand what's a flow\n\nA flow, represented as a YAML file, is a DAG of functions, which is connected via input/output dependencies, and executed based on the topology by prompt flow executor. See Flows for more details."}
{"text_chunk": "Get the flow sample\n\nClone the sample repo and check flows in folder examples/flows.\n\n```bash\ngit clone https://github.com/microsoft/promptflow.git\n```"}
{"text_chunk": "Understand flow directory\nThe sample used in this tutorial is the web-classification flow, which categorizes URLs into several predefined classes. Classification is a traditional machine learning task, and this sample illustrates how to perform classification using GPT and prompts.\n\n```bash\ncd promptflow/examples/flows/standard/web-classification\n```\n\nA flow directory is a directory that contains all contents of a flow. Structure of flow folder:\n- **flow.dag.yaml**: The flow definition with inputs/outputs, nodes, tools and variants for authoring purpose.\n- **.promptflow/flow.tools.json**: It contains tools meta referenced in `flow.dag.yaml`.\n- **Source code files (.py, .jinja2)**: User managed, the code scripts referenced by tools.\n- **requirements.txt**: Python package dependencies for this flow.\n\n\n!flow_dir\n\nIn order to run this specific flow, you need to install its requirements first.\n\n```sh\npip install -r requirements.txt\n```"}
{"text_chunk": "Understand the flow yaml\nThe entry file of a flow directory is `flow.dag.yaml` which describes the `DAG(Directed Acyclic Graph)` of a flow. Below is a sample of flow DAG:\n\n!flow_dag\n\nThis graph is rendered by VS Code extension which will be introduced in the next section."}
{"text_chunk": "Using VS Code Extension to visualize the flow\n_Note: Prompt flow VS Code Extension is highly recommended for flow development and debugging._\n\n1. Prerequisites for VS Code extension.\n   - Install latest stable version of VS Code\n   - Install VS Code Python extension\n\n2. Install Prompt flow for VS Code extension\n\n3. Select python interpreter\n\n    !vscode\n    !vscode\n\n\n2. Open dag in vscode. You can open the `flow.dag.yaml` as yaml file, or you can also open it in `visual editor`.\n    !vscode"}
{"text_chunk": "Develop and test your flow"}
{"text_chunk": "How to edit the flow\n\nTo test your flow with varying input data, you have the option to modify the default input. If you are well-versed with the structure, you may also add or remove nodes to alter the flow's arrangement.\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json\ninputs:\n  url:\n    type: string\n    # change the default value of input url here\n    default: https://play.google.com/store/apps/details?id=com.twitter.android\n...\n```\nSee more details of this topic in Develop a flow."}
{"text_chunk": "Create necessary connections\n\n:::{note}\nIf you are using `WSL` or other OS without default keyring storage backend, you may encounter `StoreConnectionEncryptionKeyError`, please refer to FAQ for the solutions.\n:::\n\n\nThe `connection` helps securely store and manage secret keys or other sensitive credentials required for interacting with LLM and other external tools for example Azure Content Safety.\n\nThe sample flow web-classification uses connection `open_ai_connection` inside, e.g. `classify_with_llm` node needs to talk to `llm` using the connection.\n\nWe need to set up the connection if we haven't added it before. Once created, the connection will be stored in local db and can be used in any flow.\n\n::::{tab-set}\n\n:::{tab-item} CLI\n:sync: CLI\n\nFirstly we need a connection yaml file `connection.yaml`:\n\nIf you are using Azure Open AI, prepare your resource follow with this instruction and get your `api_key` if you don't have one.\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/AzureOpenAIConnection.schema.json\nname: open_ai_connection\ntype: azure_open_ai\napi_key: \napi_base: \napi_type: azure\napi_version: \n```\n\nIf you are using OpenAI, sign up account via OpenAI website, login and find personal API key, then use this yaml:\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/OpenAIConnection.schema.json\nname: open_ai_connection\ntype: open_ai\napi_key: \"\"\norganization: \"\" # optional\n```\nThen we can use CLI command to create the connection.\n\n```sh\npf connection create -f connection.yaml\n```\n\nMore command details can be found in CLI reference.\n\n:::\n\n:::{tab-item} SDK\n:sync: SDK\n\nIn SDK, connections can be created and managed with `PFClient`.\n\n```python\nfrom promptflow import PFClient\nfrom promptflow.entities import AzureOpenAIConnection"}
{"text_chunk": "PFClient can help manage your runs and connections.\npf = PFClient()\n\ntry:\n    conn_name = \"open_ai_connection\"\n    conn = pf.connections.get(name=conn_name)\n    print(\"using existing connection\")\nexcept:\n    connection = AzureOpenAIConnection(\n        name=conn_name,\n        api_key=\"\",\n        api_base=\"\",\n        api_type=\"azure\",\n        api_version=\"\",\n    )\n\n    # use this if you have an existing OpenAI account\n    # from promptflow.entities import OpenAIConnection\n    # connection = OpenAIConnection(\n    #     name=conn_name,\n    #     api_key=\"\",\n    # )\n\n    conn = pf.connections.create_or_update(connection)\n    print(\"successfully created connection\")\n\nprint(conn)\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\n\n\n1. Click the promptflow icon to enter promptflow control panel\n\n    !vsc_add_connection\n\n2. Create your connection.\n\n    !vsc_add_connection\n\n    !vsc_add_connection\n\n    !vsc_add_connection\n\n\n:::\n\n::::\n\nLearn more on more actions like delete connection in: Manage connections."}
{"text_chunk": "Test the flow\n\n:::{admonition} Note\nTesting flow will NOT create a batch run record, therefore it's unable to use commands like `pf run show-details` to get the run information. If you want to persist the run record, see Run and evaluate a flow\n:::\n\n\nAssuming you are in working directory `promptflow/examples/flows/standard/`\n\n::::{tab-set}\n\n:::{tab-item} CLI\n:sync: CLI\n\nChange the default input to the value you want to test.\n\n!q_0\n\n```sh\npf flow test --flow web-classification  # \"web-classification\" is the directory name\n```\n\n!flow-test-output-cli\n\n:::\n\n:::{tab-item} SDK\n:sync: SDK\n\nThe return value of `test` function is the flow/node outputs.\n\n```python\nfrom promptflow import PFClient\n\npf = PFClient()\n\nflow_path = \"web-classification\"  # \"web-classification\" is the directory name"}
{"text_chunk": "Test flow\nflow_inputs = {\"url\": \"https://www.youtube.com/watch?v=o5ZQyXaAv1g\", \"answer\": \"Channel\", \"evidence\": \"Url\"}  # The inputs of the flow.\nflow_result = pf.test(flow=flow_path, inputs=flow_inputs)\nprint(f\"Flow outputs: {flow_result}\")"}
{"text_chunk": "Test node in the flow\nnode_name = \"fetch_text_content_from_url\"  # The node name in the flow.\nnode_inputs = {\"url\": \"https://www.youtube.com/watch?v=o5ZQyXaAv1g\"}  # The inputs of the node.\nnode_result = pf.test(flow=flow_path, inputs=node_inputs, node=node_name)\nprint(f\"Node outputs: {node_result}\")\n```\n\n!Flow test outputs\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n\nUse the code lens action on the top of the yaml editor to trigger flow test\n!dag_yaml_flow_test\n\n\nClick the run flow button on the top of the visual editor to trigger flow test.\n!visual_editor_flow_test\n:::\n\n::::\n\nSee more details of this topic in Initialize and test a flow."}
{"text_chunk": "Next steps\n\nLearn more on how to:\n- Develop a flow: details on how to develop a flow by writing a flow yaml from scratch.\n- Initialize and test a flow: details on how develop a flow from scratch or existing code.\n- Add conditional control to a flow: how to use activate config to add conditional control to a flow.\n- Run and evaluate a flow: run and evaluate the flow using multi line data file.\n- Deploy a flow: how to deploy the flow as a web app.\n- Manage connections: how to manage the endpoints/secrets information to access external services including LLMs.\n- Prompt flow in Azure AI: run and evaluate flow in Azure AI where you can collaborate with team better.\n\nAnd you can also check our examples, especially:\n- Getting started with prompt flow: the notebook covering the python sdk experience for sample introduced in this doc.\n- Tutorial: Chat with PDF: An end-to-end tutorial on how to build a high quality chat application with prompt flow, including flow development and evaluation with metrics."}
{"text_chunk": "Use column mapping\n\nIn this document, we will introduce how to map inputs with column mapping when running a flow."}
{"text_chunk": "Column mapping introduction\n\nColumn mapping is a mapping from flow input name to specified values.\nIf specified, the flow will be executed with provided value for specified inputs.\nThe following types of values in column mapping are supported:\n\n- `${data.}` to reference from your test dataset.\n- `${run.outputs.}` to reference from referenced run's output. **Note**: this only supported when `--run` is provided for `pf run`.\n- `STATIC_VALUE` to create static value for all lines for specified column."}
{"text_chunk": "Flow inputs override priority\n\nFlow input values are overridden according to the following priority:\n\n\"specified in column mapping\" > \"default value\" > \"same name column in provided data\".\n\nFor example, if we have a flow with following inputs:\n\n```yaml\ninputs:\n  input1:\n    type: string\n    default: \"default_val1\"\n  input2:\n    type: string\n    default: \"default_val2\"\n  input3:\n    type: string\n  input4:\n    type: string\n...\n```\n\nAnd the flow will return each inputs in outputs.\n\nWith the following data\n\n```json\n{\"input3\": \"val3_in_data\", \"input4\": \"val4_in_data\"}\n```\n\nAnd use the following YAML to run\n\n```yaml\n$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Run.schema.json\nflow: path/to/flow"}
{"text_chunk": "my_flow has default value val2 for key2\ndata: path/to/data"}
{"text_chunk": "my_data has column key3 with value val3\ncolumn_mapping:\n    input1: \"val1_in_column_mapping\"\n    input3: ${data.input3}\n```\n\nSince the flow will return each inputs in output, we can get the actual inputs from `outputs.output` field in run details:\n\n!column_mapping_details\n\n- Input \"input1\" has value \"val1_in_column_mapping\" since it's specified as constance in `column_mapping`.\n- Input \"input2\" has value \"default_val2\" since it used default value in flow dag.\n- Input \"input3\" has value \"val3_in_data\" since it's specified as data reference in `column_mapping`.\n- Input \"input4\" has value \"val4_in_data\" since it has same name column in provided data."}
{"text_chunk": "Set global configs\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nPromptflow supports setting global configs to avoid passing the same parameters to each command. The global configs are stored in a yaml file, which is located at `~/.promptflow/pf.yaml` by default.\n\nThe config file is shared between promptflow extension and sdk/cli. Promptflow extension controls each config through UI, so the following sections will show how to set global configs using promptflow cli."}
{"text_chunk": "Set config\n```shell\npf config set =\n```\nFor example:\n```shell\npf config set connection.provider=\"azureml://subscriptions//resourceGroups//providers/Microsoft.MachineLearningServices/workspaces/\"\n```"}
{"text_chunk": "Show config\nThe following command will get all configs and show them as json format:\n```shell\npf config show\n```\nAfter running the above config set command, show command will return the following result:\n```json\n{\n  \"connection\": {\n    \"provider\": \"azureml://subscriptions//resourceGroups//providers/Microsoft.MachineLearningServices/workspaces/\"\n  }\n}\n```"}
{"text_chunk": "Supported configs\nThe connection provider, default to \"local\". There are 3 possible provider values."}
{"text_chunk": "local\nSet connection provider to local with `connection.provider=local`.\n\nConnections will be saved locally. `PFClient`(or `pf connection` commands) will manage local connections. Consequently, the flow will be executed using these local connections."}
{"text_chunk": "full azure machine learning workspace resource id\nSet connection provider to a specific workspace with:\n```\nconnection.provider=azureml://subscriptions//resourceGroups//providers/Microsoft.MachineLearningServices/workspaces/\n```\n\nWhen `get` or `list` connections, `PFClient`(or `pf connection` commands) will return workspace connections, and flow will be executed using these workspace connections.\n_Secrets for workspace connection will not be shown by those commands, which means you may see empty dict `{}` for custom connections._\n\n:::{note}\nCommand `create`, `update` and `delete` are not supported for workspace connections, please manage it in workspace portal, az ml cli or AzureML SDK.\n:::"}
{"text_chunk": "azureml\nIn addition to the full resource id, you can designate the connection provider as \"azureml\" with `connection.provider=azureml`. In this case,\npromptflow will attempt to retrieve the workspace configuration by searching `.azureml/config.json` from the current directory, then progressively from its parent folders. So it's possible to set the workspace configuration for different flow by placing the config file in the project folder.\n\nThe expected format of the config file is as follows:\n```json\n{\n  \"workspace_name\": \"\",\n  \"resource_group\": \"\",\n  \"subscription_id\": \"\"\n}\n\n```\n\n> \ud83d\udca1 Tips\n> In addition to the CLI command line setting approach, we also support setting this connection provider through the VS Code extension UI. Click here to learn more."}
{"text_chunk": "Tune prompts using variants\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nTo better understand this part, please read Quick start and Run and evaluate a flow first."}
{"text_chunk": "What is variant and why should we care\n\nIn order to help users tune the prompts in a more efficient way, we introduce the concept of variants which can help you test the model\u2019s behavior under different conditions, such as different wording, formatting, context, temperature, or top-k, compare and find the best prompt and configuration that maximizes the model\u2019s accuracy, diversity, or coherence."}
{"text_chunk": "Create a run with different variant node\n\n In this example, we use the flow web-classification, its node `summarize_text_content` has two variants: `variant_0` and `variant_1`. The difference between them is the inputs parameters:\n\n\n```yaml\n...\nnodes:\n- name: summarize_text_content\n  use_variants: true\n...\nnode_variants:\n  summarize_text_content:\n    default_variant_id: variant_0\n    variants:\n      variant_0:\n        node:\n          type: llm\n          source:\n            type: code\n            path: summarize_text_content.jinja2\n          inputs:\n            deployment_name: text-davinci-003\n            max_tokens: '128'\n            temperature: '0.2'\n            text: ${fetch_text_content_from_url.output}\n          provider: AzureOpenAI\n          connection: open_ai_connection\n          api: completion\n          module: promptflow.tools.aoai\n      variant_1:\n        node:\n          type: llm\n          source:\n            type: code\n            path: summarize_text_content__variant_1.jinja2\n          inputs:\n            deployment_name: text-davinci-003\n            max_tokens: '256'\n            temperature: '0.3'\n            text: ${fetch_text_content_from_url.output}\n          provider: AzureOpenAI\n          connection: open_ai_connection\n          api: completion\n          module: promptflow.tools.aoai\n```\n\nYou can check the whole flow definition in flow.dag.yaml.\n\nNow we will create a variant run which uses node `summarize_text_content`'s variant `variant_1`. \nAssuming you are in working directory `/examples/flows/standard`\n\n\n::::{tab-set}\n\n:::{tab-item} CLI\n:sync: CLI\n\nNote we pass `--variant` to specify which variant of the node should be running.\n\n```sh\npf run create --flow web-classification --data web-classification/data.jsonl --variant '${summarize_text_content.variant_1}' --column-mapping url='${data.url}' --stream --name my_first_variant_run\n```\n\n:::\n\n:::{tab-item} SDK\n:sync: SDK\n\n```python\nfrom promptflow import PFClient\n\npf = PFClient()  # get a promptflow client\nflow = \"web-classification\"\ndata= \"web-classification/data.jsonl\""}
{"text_chunk": "use the variant1 of the summarize_text_content node.\nvariant_run = pf.run(\n    flow=flow,\n    data=data,\n    variant=\"${summarize_text_content.variant_1}\",  # use variant 1.\n    column_mapping={\"url\": \"${data.url}\"},\n)\n\npf.stream(variant_run)\n```\n:::\n\n:::{tab-item} VS Code Extension\n:sync: VS Code Extension\n!img\n!img\n:::\n\n::::\n\nAfter the variant run is created, you can evaluate the variant run with a evaluation flow, just like you evalute a standard flow run."}
{"text_chunk": "Next steps\n\nLearn more about:\n- Run and evaluate a flow\n- Deploy a flow\n- Prompt flow in Azure AI"}
{"text_chunk": "Azure AI Language\nAzure AI Language enables users with task-oriented and optimized pre-trained language models to effectively understand documents and conversations. This Prompt flow tool is a wrapper for various Azure AI Language APIs. The current list of supported capabilities is as follows:\n\n| Name                                      | Description                                           |\n|-------------------------------------------|-------------------------------------------------------|\n| Abstractive Summarization                 | Generate abstractive summaries from documents.        |\n| Extractive Summarization                  | Extract summaries from documents.                     |\n| Conversation Summarization                | Summarize conversations.                              |\n| Entity Recognition                        | Recognize and categorize entities in documents.       |\n| Key Phrase Extraction                     | Extract key phrases from documents.                   |\n| Language Detection                        | Detect the language of documents.                     |\n| PII Entity Recognition                    | Recognize and redact PII entities in documents.       |\n| Sentiment Analysis                        | Analyze the sentiment of documents.                   |\n| Conversational Language Understanding     | Predict intents and entities from user's utterances.  |\n| Translator                                | Translate documents.                                  |"}
{"text_chunk": "Requirements\n- For AzureML users: \n    follow this wiki, starting from `Prepare runtime`. Note that the PyPi package name is `promptflow-azure-ai-language`.\n- For local users: \n    ```\n    pip install promptflow-azure-ai-language\n    ```"}
{"text_chunk": "Prerequisites\nThe tool calls APIs from Azure AI Language. To use it, you must create a connection to an Azure AI Language resource. Create a Language resource first, if necessary.\n- In Prompt flow, add a new `CustomConnection`.\n    - Under the `secrets` field, specify the resource's API key: `api_key: `\n    - Under the `configs` field, specify the resource's endpoint: `endpoint: `\n\nTo use the `Translator` tool, you must set up an additional connection to an Azure AI Translator resource. Create a Translator resource first, if necessary.\n- In Prompt flow, add a new `CustomConnection`.\n    - Under the `secrets` field, specify the resource's API key: `api_key: `\n    - Under the `configs` field, specify the resource's endpoint: `endpoint: `\n    - If your Translator Resource is regional and non-global, specify its region under `configs` as well: `region: `"}
{"text_chunk": "Inputs\nThe tool accepts the following inputs:\n\n- **Abstractive Summarization**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | language           | string           | The ISO 639-1 code for the language of the input. | Yes |\n    | text               | string           | The input text. | Yes |\n    | query              | string           | The query used to structure summarization. | Yes |\n    | summary_length     | string (enum)    | The desired summary length. Enum values are `short`, `medium`, and `long`. | No |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **Extractive Summarization**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | language           | string           | The ISO 639-1 code for the language of the input. | Yes |\n    | text               | string           | The input text. | Yes |\n    | query              | string           | The query used to structure summarization. | Yes |\n    | sentence_count     | int              | The desired number of output summary sentences. Default value is `3`. | No |\n    | sort_by            | string (enum)    | The sorting criteria for extractive summarization results. Enum values are `Offset` to sort results in order of appearance in the text and `Rank` to sort results in order of importance (i.e. rank score) according to model. Default value is `Offset`. | No |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **Conversation Summarization**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | language           | string           | The ISO 639-1 code for the language of the input. | Yes |\n    | text               | string           | The input text. Text should be of the following form: `:  \\n :  \\n ...` | Yes |\n    | modality           | string (enum)    | The modality of the input text. Enum values are `text` for input from a text source, and `transcript` for input from a transcript source. | Yes |\n    | summary_aspect     | string (enum)    | The desired summary \"aspect\" to obtain. Enum values are `chapterTitle` to obtain the chapter title of any conversation, `issue` to obtain the summary of issues in transcripts of web chats and service calls between customer-service agents and customers, `narrative` to obtain the generic summary of any conversation, `resolution` to obtain the summary of resolutions in transcripts of web chats and service calls between customer-service agents and customers, `recap` to obtain a general summary, and `follow-up tasks` to obtain a summary of follow-up or action items. | Yes |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **Entity Recognition**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | language           | string           | The ISO 639-1 code for the language of the input. | Yes |\n    | text               | string           | The input text. | Yes |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **Key Phrase Extraction**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | language           | string           | The ISO 639-1 code for the language of the input. | Yes |\n    | text               | string           | The input text."}
{"text_chunk": "| Yes |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **Language Detection**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | text               | string           | The input text. | Yes |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **PII Entity Recognition**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | language           | string           | The ISO 639-1 code for the language of the input. | Yes |\n    | text               | string           | The input text. | Yes |\n    | domain             | string (enum)    | The PII domain used for PII Entity Recognition. Enum values are `none` for no domain, or `phi` to indicate that entities in the Personal Health domain should be redacted. Default value is `none`. | No |\n    | categories         | list[string]     | Describes the PII categories to return. Default value is `[]`. | No |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **Sentiment Analysis**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | language           | string           | The ISO 639-1 code for the language of the input. | Yes |\n    | text               | string           | The input text. | Yes |\n    | opinion_mining     | bool             | Should opinion mining be enabled. Default value is `False`. | No |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **Conversational Language Understanding**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Language resource. | Yes |\n    | language           | string           | The ISO 639-1 code for the language of the input. | Yes |\n    | utterances         | string           | A single user utterance or a json array of user utterances. | Yes |\n    | project_name       | string           | The Conversational Language Understanding project to be called. | Yes |\n    | deployment_name    | string           | The Conversational Language Understanding project deployment to be called. | Yes |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |\n\n- **Translator**:\n    | Name               | Type             | Description | Required |\n    |--------------------|------------------|-------------|----------|\n    | connection         | CustomConnection | The created connection to an Azure AI Translator resource. | Yes |\n    | text               | string           | The input text. | Yes |\n    | to                 | list[string]     | The languages to translate the input text to. | Yes |\n    | source_language    | string           | The language of the input text. | No |\n    | parse_response     | bool             | Should the raw API json output be parsed. Default value is `False`. | No |"}
{"text_chunk": "Outputs\nIf the input parameter `parse_response` is set to `False` (default value), the raw API json output will be returned as a string. Refer to the REST API reference for details on API output. For Conversational Language Understanding, the output will be a list of raw API json responses, one response for each user utterance in the input. \n\nWhen `parse_response` is set to `True`, the tool will parse API output as follows:\n\n\n| Name | Type | Description |\n|-------------------------------------------------------------|--------|---------------------|\n| Abstractive Summarization | string | Abstractive summary. |\n| Extractive Summarization | list[string] | Extracted summary sentence strings. |\n| Conversation Summarization | string | Conversation summary based on `summary_aspect`. |\n| Entity Recognition | dict[string, string] | Recognized entities, where keys are entity names and values are entity categories. |\n| Key Phrase Extraction | list[string] | Extracted key phrases as strings. |\n| Language Detection | string | Detected language's ISO 639-1 code. |\n| PII Entity Recognition | string | Input `text` with PII entities redacted. |\n| Sentiment Analysis | string | Analyzed sentiment: `positive`, `neutral`, or `negative`. |\n| Conversational Language Understanding | list[dict[string, string]] | List of user utterances and associated intents. |\n| Translator | dict[string, string] | Translated text, where keys are the translated languages and values are the translated texts. |"}
{"text_chunk": "Flow YAML Schema\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nThe source JSON schema can be found at Flow.schema.json"}
{"text_chunk": "YAML syntax\n\n| Key                        | Type      | Description                                                                                                                                                                                                       |\n|----------------------------|-----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `$schema`                  | string    | The YAML schema. If you use the prompt flow VS Code extension to author the YAML file, including `$schema` at the top of your file enables you to invoke schema and resource completions.              |\n| `inputs`                   | object    | Dictionary of flow inputs. The key is a name for the input within the context of the flow and the value is the flow input definition.                                                                             |\n| `inputs.`      | object    | The flow input definition. See Flow input for the set of configurable properties.                                                                                                                  |\n| `outputs`                  | object    | Dictionary of flow outputs. The key is a name for the output within the context of the flow and the value is the flow output definition.                                                                          |\n| `outputs.`    | object    | The component output definition. See Flow output for the set of configurable properties.                                                                                                          |\n| `nodes`                    | array     | Sets of dictionary of individual nodes to run as steps within the flow. Node can use built-in tool or third-party tool. See Nodes for more information.                                                 |\n| `node_variants`            | object    | Dictionary of nodes with variants. The key is the node name and value contains variants definition and `default_variant_id`. See Node variants for more information.                            |\n| `environment`              | object    | The environment to use for the flow. The key can be `image` or `python_requirements_txt` and the value can be either a image or a python requirements text file.                                                  |\n| `additional_includes`      | array     | Additional includes is a list of files that can be shared among flows. Users can specify additional files and folders used by flow, and prompt flow will help copy them all to the snapshot during flow creation. |"}
{"text_chunk": "Flow input\n\n| Key               | Type                                      | Description                                          | Allowed values                                      |\n|-------------------|-------------------------------------------|------------------------------------------------------|-----------------------------------------------------|\n| `type`            | string                                    | The type of flow input.                              | `int`, `double`, `bool`, `string`, `list`, `object`, `image` |\n| `description`     | string                                    | Description of the input.                            |                                                     |\n| `default`         | int, double, bool, string, list, object, image | The default value for the input.                     |                                                     |\n| `is_chat_input`   | boolean                                   | Whether the input is the chat flow input.            |                                                     |\n| `is_chat_history` | boolean                                   | Whether the input is the chat history for chat flow. |                                                     |"}
{"text_chunk": "Flow output\n\n| Key              | Type    | Description                                                                   | Allowed values                                      |\n|------------------|---------|-------------------------------------------------------------------------------|-----------------------------------------------------|\n| `type`           | string  | The type of flow output.                                                      | `int`, `double`, `bool`, `string`, `list`, `object` |\n| `description`    | string  | Description of the output.                                                    |                                                     |\n| `reference`      | string  | A reference to the node output, e.g. ${.output.} |                                                     |\n| `is_chat_output` | boolean | Whether the output is the chat flow output.                                   |                                                     |"}
{"text_chunk": "Nodes\nNodes is a set of node which is a dictionary with following fields. Below, we only show the common fields of a single node using built-in tool.\n\n| Key            | Type   | Description                                                                                                                                                                                                                                               | Allowed values                                                                                       |\n|----------------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n| `name`         | string | The name of the node.                                                                                                                                                                                                                                     |                                                                                                      |\n| `type`         | string | The type of the node.                                                                                                                                                                                                                                     | Type of built-in tool like `Python`, `Prompt`, `LLM` and third-party tool like `Vector Search`, etc. |\n| `inputs`       | object | Dictionary of node inputs. The key is the input name and the value can be primitive value or a reference to the flow input or the node output, e.g. `${inputs.}`, `${.output}` or `${.output.}`  |                                                                                                      |\n| `source`       | object | Dictionary of tool source used by the node. The key contains `type`, `path` and `tool`. The type can be `code`, `package` and `package_with_prompt`.                                                                                                      |                                                                                                      |\n| `provider`     | string | It indicates the provider of the tool. Used when the `type` is LLM.                                                                                                                                                                                       | `AzureOpenAI` or `OpenAI`                                                                            |\n| `connection`   | string | The connection name which has been created before. Used when the `type` is LLM.                                                                                                                                                                           |                                                                                                      |\n| `api`          | string | The api name of the provider. Used when the `type` is LLM.                                                                                                                                                                                                |                                                                                                      |\n| `module`       | string | The module name of the tool using by the node. Used when the `type` is LLM.                                                                                                                                                                               |                                                                                                      |\n| `use_variants` | bool   | Whether the node has variants.                                                                                                                                                                                                                            |                                                                                                      |"}
{"text_chunk": "Node variants\nNode variants is a dictionary containing variants definition for nodes with variants with their respective node names as dictionary keys.\nBelow, we explore the variants for a single node.\n\n| Key                  | Type     | Description                                                                                                                                                                                                                                                                                                   | Allowed values |\n|----------------------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|\n| ``        | string   | The name of the node.                                                                                                                                                                                                                                                                                         |                |\n| `default_variant_id` | string   | Default variant id.                                                                                                                                                                                                                                                                                           |                |\n| `variants `          | object   | This dictionary contains all node variations, with the variant id serving as the key and a node definition dictionary as the corresponding value.  Within the node definition dictionary, the key labeled 'node' should contain a variant definition similar to Nodes, excluding the 'name' field.  |                |"}
{"text_chunk": "Examples\n\nFlow examples are available in the GitHub repository.\n\n- basic\n- web-classification\n- basic-chat\n- chat-with-pdf\n- eval-basic"}
{"text_chunk": "pf\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nManage prompt flow resources with the prompt flow CLI.\n\n| Command                         | Description                     |\n|---------------------------------|---------------------------------|\n| pf flow             | Manage flows.                   |\n| pf connection | Manage connections.             |\n| pf run               | Manage runs.                    |\n| pf tool             | Init or list tools.             |\n| pf config         | Manage config for current user. |"}
{"text_chunk": "pf flow\n\nManage promptflow flow flows.\n\n| Command | Description |\n| --- | --- |\n| pf flow init | Initialize a prompt flow directory. |\n| pf flow test | Test the prompt flow or flow node. |\n| pf flow validate | Validate a flow and generate `flow.tools.json` for it. |\n| pf flow build | Build a flow for further sharing or deployment. |\n| pf flow serve | Serve a flow as an endpoint. |"}
{"text_chunk": "pf flow init\n\nInitialize a prompt flow directory.\n\n```bash\npf flow init [--flow]\n             [--entry]\n             [--function]\n             [--prompt-template]\n             [--type]\n             [--yes]\n```"}
{"text_chunk": "Examples\n\nCreate a flow folder with code, prompts and YAML specification of the flow.\n\n```bash\npf flow init --flow \n```\n\nCreate an evaluation prompt flow\n\n```bash\npf flow init --flow  --type evaluation\n```\n\nCreate a flow in existing folder\n\n```bash\npf flow init --flow  --entry  --function  --prompt-template \n```"}
{"text_chunk": "Optional Parameters\n\n`--flow`\n\nThe flow name to create.\n\n`--entry`\n\nThe entry file name.\n\n`--function`\n\nThe function name in entry file.\n\n`--prompt-template`\n\nThe prompt template parameter and assignment.\n\n`--type`\n\nThe initialized flow type.  \naccepted value: standard, evaluation, chat\n\n`--yes --assume-yes -y`\n\nAutomatic yes to all prompts; assume 'yes' as answer to all prompts and run non-interactively."}
{"text_chunk": "pf flow test\n\nTest the prompt flow or flow node.\n\n```bash\npf flow test --flow\n             [--inputs]\n             [--node]\n             [--variant]\n             [--debug]\n             [--interactive]\n             [--verbose]\n```"}
{"text_chunk": "Examples\n\nTest the flow.\n\n```bash\npf flow test --flow \n```\n\nTest the flow with single line from input file.\n\n```bash\npf flow test --flow  --inputs data_key1=data_val1 data_key2=data_val2\n```\n\nTest the flow with specified variant node.\n\n```bash\npf flow test --flow  --variant '${node_name.variant_name}'\n```\n\nTest the single node in the flow.\n\n```bash\npf flow test --flow  --node \n```\n\nDebug the single node in the flow.\n\n```bash\npf flow test --flow  --node  --debug\n```\n\nChat in the flow.\n\n```bash\npf flow test --flow  --node  --interactive\n```"}
{"text_chunk": "Required Parameter\n\n`--flow`\n\nThe flow directory to test."}
{"text_chunk": "Optional Parameters\n\n`--inputs`\n\nInput data for the flow. Example: --inputs data1=data1_val data2=data2_val\n\n`--node`\n\nThe node name in the flow need to be tested.\n\n`--variant`\n\nNode & variant name in format of ${node_name.variant_name}.\n\n`--debug`\n\nDebug the single node in the flow.\n\n`--interactive`\n\nStart a interactive chat session for chat flow.\n\n`--verbose`\n\nDisplays the output for each step in the chat flow."}
{"text_chunk": "pf flow validate\n\nValidate the prompt flow and generate a `flow.tools.json` under `.promptflow`. This file is required when using flow as a component in a Azure ML pipeline.\n\n```bash\npf flow validate --source\n                 [--debug]\n                 [--verbose]\n```"}
{"text_chunk": "Examples\n\nValidate the flow.\n\n```bash\npf flow validate --source \n```"}
{"text_chunk": "Required Parameter\n\n`--source`\n\nThe flow source to validate."}
{"text_chunk": "pf flow build\n\nBuild a flow for further sharing or deployment.\n\n```bash\npf flow build --source\n              --output\n              --format\n              [--variant]\n              [--verbose]\n              [--debug]\n```"}
{"text_chunk": "Examples\n\nBuild a flow as docker, which can be built into Docker image via `docker build`.\n\n```bash\npf flow build --source  --output  --format docker\n```\n\nBuild a flow as docker with specific variant.\n\n```bash\npf flow build --source  --output  --format docker --variant '${node_name.variant_name}'\n```"}
{"text_chunk": "Required Parameter\n\n`--source`\n\nThe flow or run source to be used.\n\n`--output`\n\nThe folder to output built flow. Need to be empty or not existed.\n\n`--format`\n\nThe format to build flow into"}
{"text_chunk": "Optional Parameters\n\n`--variant`\n\nNode & variant name in format of ${node_name.variant_name}.\n\n`--verbose`\n\nShow more details for each step during build.\n\n`--debug`\n\nShow debug information during build."}
{"text_chunk": "pf flow serve\n\nServing a flow as an endpoint.\n\n```bash\npf flow serve --source\n              [--port]\n              [--host]\n              [--environment-variables]\n              [--verbose]\n              [--debug]\n```"}
{"text_chunk": "Examples\n\nServe flow as an endpoint.\n\n```bash\npf flow serve --source \n```\n\nServe flow as an endpoint with specific port and host.\n\n```bash\npf flow serve --source  --port  --host  --environment-variables key1=\"`${my_connection.api_key}`\" key2=\"value2\"\n```"}
{"text_chunk": "Required Parameter\n\n`--source`\n\nThe flow or run source to be used."}
{"text_chunk": "Optional Parameters\n\n`--port`\n\nThe port on which endpoint to run.\n\n`--host`\n\nThe host of endpoint.\n\n`--environment-variables`\n\nEnvironment variables to set by specifying a property path and value. Example: --environment-variable key1=\"\\`${my_connection.api_key}\\`\" key2=\"value2\". The value reference to connection keys will be resolved to the actual value, and all environment variables specified will be set into `os.environ`.\n\n`--verbose`\n\nShow more details for each step during serve.\n\n`--debug`\n\nShow debug information during serve."}
{"text_chunk": "pf connection\n\nManage prompt flow connections.\n\n| Command | Description |\n| --- | --- |\n| pf connection create | Create a connection. |\n| pf connection update | Update a connection. |\n| pf connection show | Show details of a connection. |\n| pf connection list | List all the connection. |\n| pf connection delete | Delete a connection. |"}
{"text_chunk": "pf connection create\n\nCreate a connection.\n\n```bash\npf connection create --file\n                     [--name]\n                     [--set]\n```"}
{"text_chunk": "Examples\n\nCreate a connection with YAML file.\n\n```bash\npf connection create -f \n```\n\nCreate a connection with YAML file with override.\n\n```bash\npf connection create -f  --set api_key=\"\"\n```\n\nCreate a custom connection with .env file; note that overrides specified by `--set` will be ignored.\n\n```bash\npf connection create -f .env --name \n```"}
{"text_chunk": "Required Parameter\n\n`--file -f`\n\nLocal path to the YAML file containing the prompt flow connection specification."}
{"text_chunk": "Optional Parameters\n\n`--name -n`\n\nName of the connection.\n\n`--set`\n\nUpdate an object by specifying a property path and value to set. Example: --set property1.property2=."}
{"text_chunk": "pf connection update\n\nUpdate a connection.\n\n```bash\npf connection update --name\n                     [--set]\n```"}
{"text_chunk": "Example\n\nUpdate a connection.\n\n```bash\npf connection update -n  --set api_key=\"\"\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the connection."}
{"text_chunk": "Optional Parameter\n\n`--set`\n\nUpdate an object by specifying a property path and value to set. Example: --set property1.property2=."}
{"text_chunk": "pf connection show\n\nShow details of a connection.\n\n```bash\npf connection show --name\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the connection."}
{"text_chunk": "pf connection list\n\nList all the connection.\n\n```bash\npf connection list\n```"}
{"text_chunk": "pf connection delete\n\nDelete a connection.\n\n```bash\npf connection delete --name\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the connection."}
{"text_chunk": "pf run\n\nManage prompt flow runs.\n\n| Command | Description |\n| --- | --- |\n| pf run create | Create a run. |\n| pf run update | Update a run metadata, including display name, description and tags. |\n| pf run stream | Stream run logs to the console. |\n| pf run list | List runs. |\n| pf run show | Show details for a run. |\n| pf run show-details | Preview a run's intput(s) and output(s). |\n| pf run show-metrics | Print run metrics to the console. |\n| pf run visualize | Visualize a run. |\n| pf run archive | Archive a run. |\n| pf run restore | Restore an archived run. |"}
{"text_chunk": "pf run create\n\nCreate a run.\n\n```bash\npf run create [--file]\n              [--flow]\n              [--data]\n              [--column-mapping]\n              [--run]\n              [--variant]\n              [--stream]\n              [--environment-variables]\n              [--connections]\n              [--set]\n              [--source]\n```"}
{"text_chunk": "Examples\n\nCreate a run with YAML file.\n\n```bash\npf run create -f \n```\n\nCreate a run from flow directory and reference a run.\n\n```bash\npf run create --flow  --data  --column-mapping groundtruth='${data.answer}' prediction='${run.outputs.category}' --run  --variant '${summarize_text_content.variant_0}' --stream\n```\n\nCreate a run from an existing run record folder.\n\n```bash\npf run create --source \n```"}
{"text_chunk": "Optional Parameters\n\n`--file -f`\n\nLocal path to the YAML file containing the prompt flow run specification; can be overwritten by other parameters. Reference here for YAML schema.\n\n`--flow`\n\nLocal path to the flow directory.\n\n`--data`\n\nLocal path to the data file.\n\n`--column-mapping`\n\nInputs column mapping, use `${data.xx}` to refer to data columns, use `${run.inputs.xx}` to refer to referenced run's data columns, and `${run.outputs.xx}` to refer to run outputs columns.\n\n`--run`\n\nReferenced flow run name. For example, you can run an evaluation flow against an existing run. For example, \"pf run create --flow evaluation_flow_dir --run existing_bulk_run\".\n\n`--variant`\n\nNode & variant name in format of `${node_name.variant_name}`.\n\n`--stream -s`\n\nIndicates whether to stream the run's logs to the console.  \ndefault value: False\n\n`--environment-variables`\n\nEnvironment variables to set by specifying a property path and value. Example:\n`--environment-variable key1='${my_connection.api_key}' key2='value2'`. The value reference\nto connection keys will be resolved to the actual value, and all environment variables\nspecified will be set into os.environ.\n\n`--connections`\n\nOverwrite node level connections with provided value.\nExample: `--connections node1.connection=test_llm_connection node1.deployment_name=gpt-35-turbo`\n\n`--set`\n\nUpdate an object by specifying a property path and value to set.\nExample: `--set property1.property2=`.\n\n`--source`\n\nLocal path to the existing run record folder."}
{"text_chunk": "pf run update\n\nUpdate a run metadata, including display name, description and tags.\n\n```bash\npf run update --name\n              [--set]\n```"}
{"text_chunk": "Example\n\nUpdate a run\n\n```bash\npf run update -n  --set display_name=\"\" description=\"\" tags.key=\"value\"\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the run."}
{"text_chunk": "Optional Parameter\n\n`--set`\n\nUpdate an object by specifying a property path and value to set. Example: --set property1.property2=."}
{"text_chunk": "pf run stream\n\nStream run logs to the console.\n\n```bash\npf run stream --name\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the run."}
{"text_chunk": "pf run list\n\nList runs.\n\n```bash\npf run list [--all-results]\n            [--archived-only]\n            [--include-archived]\n            [--max-results]\n```"}
{"text_chunk": "Optional Parameters\n\n`--all-results`\n\nReturns all results.  \ndefault value: False\n\n`--archived-only`\n\nList archived runs only.  \ndefault value: False\n\n`--include-archived`\n\nList archived runs and active runs.  \ndefault value: False\n\n`--max-results -r`\n\nMax number of results to return. Default is 50.  \ndefault value: 50"}
{"text_chunk": "pf run show\n\nShow details for a run.\n\n```bash\npf run show --name\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the run."}
{"text_chunk": "pf run show-details\n\nPreview a run's input(s) and output(s).\n\n```bash\npf run show-details --name\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the run."}
{"text_chunk": "pf run show-metrics\n\nPrint run metrics to the console.\n\n```bash\npf run show-metrics --name\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the run."}
{"text_chunk": "pf run visualize\n\nVisualize a run in the browser.\n\n```bash\npf run visualize --names\n```"}
{"text_chunk": "Required Parameter\n\n`--names -n`\n\nName of the runs, comma separated."}
{"text_chunk": "pf run archive\n\nArchive a run.\n\n```bash\npf run archive --name\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the run."}
{"text_chunk": "pf run restore\n\nRestore an archived run.\n\n```bash\npf run restore --name\n```"}
{"text_chunk": "Required Parameter\n\n`--name -n`\n\nName of the run."}
{"text_chunk": "pf tool\n\nManage promptflow tools.\n\n| Command | Description |\n| --- | --- |\n| pf tool init | Initialize a tool directory. |\n| pf tool list | List all tools in the environment. |\n| pf tool validate | Validate tools. |"}
{"text_chunk": "pf tool init\n\nInitialize a tool directory.\n\n```bash\npf tool init [--package]\n             [--tool]\n             [--set]\n```"}
{"text_chunk": "Examples\n\nCreating a package tool from scratch.\n\n```bash\npf tool init --package  --tool \n```\n\nCreating a package tool with extra info.\n\n```bash\npf tool init --package  --tool  --set icon= category= tags=\"{'': ''}\"\n```\n\nCreating a package tool from scratch.\n\n```bash\npf tool init --package  --tool \n```\n\nCreating a python tool from scratch.\n\n```bash\npf tool init --tool \n```"}
{"text_chunk": "Optional Parameters\n\n`--package`\n\nThe package name to create.\n\n`--tool`\n\nThe tool name to create.\n\n`--set`\n\nSet extra information about the tool, like category, icon and tags. Example: --set =."}
{"text_chunk": "pf tool list\n\nList all tools in the environment.\n\n```bash\npf tool list [--flow]\n```"}
{"text_chunk": "Examples\n\nList all package tool in the environment.\n\n```bash\npf tool list\n```\n\nList all package tool and code tool in the flow.\n\n```bash\npf tool list --flow \n```"}
{"text_chunk": "Optional Parameters\n\n`--flow`\n\nThe flow directory."}
{"text_chunk": "pf tool validate\n\nValidate tool.\n\n```bash\npf tool validate --source\n```"}
{"text_chunk": "Examples\n\nValidate single function tool.\n\n```bash\npf tool validate -\u2013source ..\n```\n\nValidate all tool in a package tool.\n\n```bash\npf tool validate -\u2013source \n```\n\nValidate tools in a python script.\n\n```bash\npf tool validate --source \n```"}
{"text_chunk": "Required Parameter\n\n`--source`\n\nThe tool source to be used."}
{"text_chunk": "pf config\n\nManage config for current user.\n\n| Command                           | Description                                |\n|-----------------------------------|--------------------------------------------|\n| pf config set   | Set prompt flow configs for current user.  |\n| pf config show | Show prompt flow configs for current user. |"}
{"text_chunk": "pf config set\n\nSet prompt flow configs for current user, configs will be stored at ~/.promptflow/pf.yaml.\n\n```bash\npf config set\n```"}
{"text_chunk": "Examples\n\nConfig connection provider to azure workspace for current user.\n\n```bash\npf config set connection.provider=\"azureml://subscriptions//resourceGroups//providers/Microsoft.MachineLearningServices/workspaces/\"\n```"}
{"text_chunk": "pf config show\n\nShow prompt flow configs for current user.\n\n```bash\npf config show\n```"}
{"text_chunk": "Examples\n\nShow prompt flow for current user.\n\n```bash\npf config show\n```"}
{"text_chunk": "pfazure\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nManage prompt flow resources on Azure with the prompt flow CLI.\n\n| Command | Description |\n| --- | --- |\n| pfazure flow | Manage flows. |\n| pfazure run | Manage runs. |"}
{"text_chunk": "pfazure flow\n\nManage flows.\n\n| Command | Description |\n| --- | --- |\n| pfazure flow create | Create a flow. |\n| pfazure flow list | List flows in a workspace. |"}
{"text_chunk": "pfazure flow create\n\nCreate a flow in Azure AI from a local flow folder.\n\n```bash\npfazure flow create [--flow]\n                    [--set]\n                    [--subscription]\n                    [--resource-group]\n                    [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--flow`\n\nLocal path to the flow directory.\n\n`--set`\n\nUpdate an object by specifying a property path and value to set.\n- `display_name`: Flow display name that will be created in remote. Default to be flow folder name + timestamp if not specified.\n- `type`: Flow type. Default to be \"standard\" if not specified. Available types are: \"standard\", \"evaluation\", \"chat\".\n- `description`: Flow description. e.g. \"--set description=\\.\"\n- `tags`: Flow tags. e.g. \"--set tags.key1=value1 tags.key2=value2.\"\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure flow list\n\nList remote flows on Azure AI.\n\n```bash\npfazure flow list [--max-results]\n                  [--include-others]\n                  [--type]\n                  [--output]\n                  [--archived-only]\n                  [--include-archived]\n                  [--subscription]\n                  [--resource-group]\n                  [--workspace-name]\n                  [--output]\n```"}
{"text_chunk": "Parameters\n\n`--max-results -r`\n\nMax number of results to return. Default is 50, upper bound is 100.\n\n`--include-others`\n\nInclude flows created by other owners. By default only flows created by the current user are returned.\n\n`--type`\n\nFilter flows by type. Available types are: \"standard\", \"evaluation\", \"chat\".\n\n`--archived-only`\n\nList archived flows only.\n\n`--include-archived`\n\nList archived flows and active flows.\n\n`--output -o`\n\nOutput format. Allowed values: `json`, `table`. Default: `json`.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run\n\nManage prompt flow runs.\n\n| Command | Description |\n| --- | --- |\n| pfazure run create | Create a run. |\n| pfazure run list | List runs in a workspace. |\n| pfazure run show | Show details for a run. |\n| pfazure run stream | Stream run logs to the console. |\n| pfazure run show-details | Show a run details. |\n| pfazure run show-metrics | Show run metrics. |\n| pfazure run visualize | Visualize a run. |\n| pfazure run archive | Archive a run. |\n| pfazure run restore | Restore a run. |\n| pfazure run update | Update a run. |\n| pfazure run download | Download a run. |"}
{"text_chunk": "pfazure run create\n\nCreate a run.\n\n```bash\npfazure run create [--file]\n                   [--flow]\n                   [--data]\n                   [--column-mapping]\n                   [--run]\n                   [--variant]\n                   [--stream]\n                   [--environment-variables]\n                   [--connections]\n                   [--set]\n                   [--subscription]\n                   [--resource-group]\n                   [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--file -f`\n\nLocal path to the YAML file containing the prompt flow run specification; can be overwritten by other parameters. Reference here for YAML schema.\n\n`--flow`\n\nLocal path to the flow directory.\n\n`--data`\n\nLocal path to the data file or remote data. e.g. azureml:name:version.\n\n`--column-mapping`\n\nInputs column mapping, use `${data.xx}` to refer to data columns, use `${run.inputs.xx}` to refer to referenced run's data columns, and `${run.outputs.xx}` to refer to run outputs columns.\n\n`--run`\n\nReferenced flow run name. For example, you can run an evaluation flow against an existing run. For example, \"pfazure run create --flow evaluation_flow_dir --run existing_bulk_run --column-mapping url='${data.url}'\".\n\n`--variant`\n\nNode & variant name in format of `${node_name.variant_name}`.\n\n`--stream -s`\n\nIndicates whether to stream the run's logs to the console.  \ndefault value: False\n\n`--environment-variables`\n\nEnvironment variables to set by specifying a property path and value. Example:\n`--environment-variable key1='${my_connection.api_key}' key2='value2'`. The value reference\nto connection keys will be resolved to the actual value, and all environment variables\nspecified will be set into os.environ.\n\n`--connections`\n\nOverwrite node level connections with provided value.\nExample: `--connections node1.connection=test_llm_connection node1.deployment_name=gpt-35-turbo`\n\n`--set`\n\nUpdate an object by specifying a property path and value to set.\nExample: `--set property1.property2=`.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run list\n\nList runs in a workspace.\n\n```bash\npfazure run list [--archived-only]\n                 [--include-archived]\n                 [--max-results]\n                 [--subscription]\n                 [--resource-group]\n                 [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--archived-only`\n\nList archived runs only.  \ndefault value: False\n\n`--include-archived`\n\nList archived runs and active runs.  \ndefault value: False\n\n`--max-results -r`\n\nMax number of results to return. Default is 50, upper bound is 100.  \ndefault value: 50\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run show\n\nShow details for a run.\n\n```bash\npfazure run show --name\n                 [--subscription]\n                 [--resource-group]\n                 [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run stream\n\nStream run logs to the console.\n\n```bash\npfazure run stream --name\n                   [--subscription]\n                   [--resource-group]\n                   [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run show-details\n\nShow a run details.\n\n```bash\npfazure run show-details --name\n                         [--subscription]\n                         [--resource-group]\n                         [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run show-metrics\n\nShow run metrics.\n\n```bash\npfazure run show-metrics --name\n                         [--subscription]\n                         [--resource-group]\n                         [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run visualize\n\nVisualize a run.\n\n```bash\npfazure run visualize --name\n                      [--subscription]\n                      [--resource-group]\n                      [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run archive\n\nArchive a run.\n\n```bash\npfazure run archive --name\n                    [--subscription]\n                    [--resource-group]\n                    [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run restore\n\nRestore a run.\n\n```bash\npfazure run restore --name\n                    [--subscription]\n                    [--resource-group]\n                    [--workspace-name]\n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run update\n\nUpdate a run's metadata, such as `display name`, `description` and `tags`.\n\n```bash\npfazure run update --name\n                    [--set display_name=\"\" description=\"\" tags.key=\"\"]\n                    [--subscription]\n                    [--resource-group]\n                    [--workspace-name]\n```"}
{"text_chunk": "Examples\n\nSet `display name`, `description` and `tags`:\n\n```bash\npfazure run update --name  --set display_name=\"\" description=\"\" tags.key=\"\"\n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--set`\n\nSet meta information of the run, like `display_name`, `description` or `tags`. Example: --set =.\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "pfazure run download\n\nDownload a run's metadata, such as `input`, `output`, `snapshot` and `artifact`. After the download is finished,  you can use `pf run create --source ` to register this run as a local run record, then you can use commands like `pf run show/visualize` to inspect the run just like a run that was created from local flow.\n\n```bash\npfazure run download --name\n                    [--output]\n                    [--overwrite]\n                    [--subscription]\n                    [--resource-group]\n                    [--workspace-name]\n```"}
{"text_chunk": "Examples\n\nDownload a run data to local:\n```bash\npfazure run download --name  --output \n```"}
{"text_chunk": "Parameters\n\n`--name -n`\n\nName of the run.\n\n`--output -o`\n\nOutput folder path to store the downloaded run data. Default to be `~/.promptflow/.runs` if not specified\n\n`--overwrite`\n\nOverwrite the existing run data if the output folder already exists. Default to be `False` if not specified\n\n`--subscription`\n\nSubscription id, required when there is no default value from `az configure`.\n\n`--resource-group -g`\n\nResource group name, required when there is no default value from `az configure`.\n\n`--workspace-name -w`\n\nWorkspace name, required when there is no default value from `az configure`."}
{"text_chunk": "PLACEHOLDER"}
{"text_chunk": "Run YAML Schema\n\n:::{admonition} Experimental feature\nThis is an experimental feature, and may change at any time. Learn more.\n:::\n\nThe source JSON schema can be found at Run.schema.json"}
{"text_chunk": "YAML syntax\n\n| Key                     | Type          | Description                                                                                                                                                                                                                                                             |\n|-------------------------|---------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `$schema`               | string        | The YAML schema. If you use the prompt flow VS Code extension to author the YAML file, including $schema at the top of your file enables you to invoke schema and resource completions.                                                                                 |\n| `name`                  | string        | The name of the run.                                                                                                                                                                                                                                                    |\n| `flow`                  | string        | Path of the flow directory.                                                                                                                                                                                                                                             |\n| `description`           | string        | Description of the run.                                                                                                                                                                                                                                                 |\n| `display_name`          | string        | Display name of the run.                                                                                                                                                                                                                                                |\n| `runtime`               | string        | The runtime for the run. Only supported for cloud run.                                                                                                                                                                                                                  |\n| `data`                  | string        | Input data for the run. Local path or remote uri(starts with azureml: or public URL) are supported. Note: remote uri is only supported for cloud run.                                                                                                                   |\n| `run`                   | string        | Referenced flow run name. For example, you can run an evaluation flow against an existing run.                                                                                                                                                                          |\n| `column_mapping`        | object        | Inputs column mapping, use `${data.xx}` to refer to data columns, use `${run.inputs.xx}` to refer to referenced run's data columns, and `${run.outputs.xx}` to refer to run outputs columns.                                                                                                                 |\n| `connections`           | object        | Overwrite node level connections with provided value. Example: --connections node1.connection=test_llm_connection node1.deployment_name=gpt-35-turbo                                                                                                                    |\n| `environment_variables` | object/string | Environment variables to set by specifying a property path and value. Example: `{\"key1\"=\"${my_connection.api_key}\"}`. The value reference to connection keys will be resolved to the actual value, and all environment variables specified will be set into os.environ. |\n| `properties`            | object        | Dictionary of properties of the run.                                                                                                                                                                                                                                    |\n| `tags`                  | object        | Dictionary of tags of the run.                                                                                                                                                                                                                                          |\n| `resources`             | object        | Dictionary of resources used for automatic runtime. Only supported for cloud run. See Resources Schema for the set of configurable properties.                                                                                                     |\n| `variant`               | string        | The variant for the run.                                                                                                                                                                                                                                                |\n| `status`                | string        | The status of the run. Only available for when getting an existing run. Won't take affect if set when creating a run.                                                                                                                                                   |"}
{"text_chunk": "Resources Schema\n\n| Key                                 | Type    | Description                                                 |\n|-------------------------------------|---------|-------------------------------------------------------------|\n| `instance_type`                     | string  | The instance type for automatic runtime of the run.         |\n| `idle_time_before_shutdown_minutes` | integer | The idle time before automatic runtime shutdown in minutes. |"}
{"text_chunk": "Examples\n\nRun examples are available in the GitHub repository.\n\n- basic\n- web-classification\n- flow-with-additional-includes"}
{"text_chunk": "Azure OpenAI GPT-4 Turbo with Vision"}
{"text_chunk": "Introduction\nAzure OpenAI GPT-4 Turbo with Vision tool enables you to leverage your AzureOpenAI GPT-4 Turbo with Vision model deployment to analyze images and provide textual responses to questions about them."}
{"text_chunk": "Prerequisites\n\n- Create AzureOpenAI resources\n\n    Create Azure OpenAI resources with instruction\n\n- Create a GPT-4 Turbo with Vision deployment\n\n    Browse to Azure OpenAI Studio and sign in with the credentials associated with your Azure OpenAI resource. During or after the sign-in workflow, select the appropriate directory, Azure subscription, and Azure OpenAI resource.\n\n    Under Management select Deployments and Create a GPT-4 Turbo with Vision deployment by selecting model name: `gpt-4` and model version `vision-preview`."}
{"text_chunk": "Connection\n\nSetup connections to provisioned resources in prompt flow.\n\n| Type        | Name     | API KEY  | API Type | API Version |\n|-------------|----------|----------|----------|-------------|\n| AzureOpenAI | Required | Required | Required | Required    |"}
{"text_chunk": "Inputs\n\n| Name                   | Type        | Description                                                                                    | Required |\n|------------------------|-------------|------------------------------------------------------------------------------------------------|----------|\n| connection             | AzureOpenAI | the AzureOpenAI connection to be used in the tool                                              | Yes      |\n| deployment\\_name       | string      | the language model to use                                                                      | Yes      |\n| prompt                 | string      | The text prompt that the language model will use to generate it's response.                    | Yes      |\n| max\\_tokens            | integer     | the maximum number of tokens to generate in the response. Default is 512.                      | No       |\n| temperature            | float       | the randomness of the generated text. Default is 1.                                            | No       |\n| stop                   | list        | the stopping sequence for the generated text. Default is null.                                 | No       |\n| top_p                  | float       | the probability of using the top choice from the generated tokens. Default is 1.               | No       |\n| presence\\_penalty      | float       | value that controls the model's behavior with regards to repeating phrases. Default is 0.      | No       |\n| frequency\\_penalty     | float       | value that controls the model's behavior with regards to generating rare phrases. Default is 0. | No       |"}
{"text_chunk": "Outputs\n\n| Return Type | Description                              |\n|-------------|------------------------------------------|\n| string      | The text of one response of conversation |"}
{"text_chunk": "Content Safety (Text)\n\nAzure Content Safety is a content moderation service developed by Microsoft that help users detect harmful content from different modalities and languages. This tool is a wrapper for the Azure Content Safety Text API, which allows you to detect text content and get moderation results. See the Azure Content Safety for more information."}
{"text_chunk": "Requirements\n\n- For AzureML users, the tool is installed in default image, you can use the tool without extra installation.\n- For local users,\n  `pip install promptflow-tools`\n> [!NOTE]\n> Content Safety (Text) tool is now incorporated into the latest `promptflow-tools` package. If you have previously installed the package `promptflow-contentsafety`, please uninstall it to avoid the duplication in your local tool list."}
{"text_chunk": "Prerequisites\n\n- Create an Azure Content Safety resource.\n- Add \"Azure Content Safety\" connection in prompt flow. Fill \"API key\" field with \"Primary key\" from \"Keys and Endpoint\" section of created resource."}
{"text_chunk": "Inputs\n\nYou can use the following parameters as inputs for this tool:\n\n| Name | Type | Description | Required |\n| ---- | ---- | ----------- | -------- |\n| text | string | The text that need to be moderated. | Yes |\n| hate_category | string | The moderation sensitivity for Hate category. You can choose from four options: *disable*, *low_sensitivity*, *medium_sensitivity*, or *high_sensitivity*. The *disable* option means no moderation for hate category. The other three options mean different degrees of strictness in filtering out hate content. The default option is *medium_sensitivity*. | Yes |\n| sexual_category | string | The moderation sensitivity for Sexual category. You can choose from four options: *disable*, *low_sensitivity*, *medium_sensitivity*, or *high_sensitivity*. The *disable* option means no moderation for sexual category. The other three options mean different degrees of strictness in filtering out sexual content. The default option is *medium_sensitivity*. | Yes |\n| self_harm_category | string | The moderation sensitivity for Self-harm category. You can choose from four options: *disable*, *low_sensitivity*, *medium_sensitivity*, or *high_sensitivity*. The *disable* option means no moderation for self-harm category. The other three options mean different degrees of strictness in filtering out self_harm content. The default option is *medium_sensitivity*. | Yes |\n| violence_category | string | The moderation sensitivity for Violence category. You can choose from four options: *disable*, *low_sensitivity*, *medium_sensitivity*, or *high_sensitivity*. The *disable* option means no moderation for violence category. The other three options mean different degrees of strictness in filtering out violence content. The default option is *medium_sensitivity*. | Yes |\n\nFor more information, please refer to Azure Content Safety"}
{"text_chunk": "Outputs\n\nThe following is an example JSON format response returned by the tool:\n\n\n  Output\n  \n```json\n{\n    \"action_by_category\": {\n      \"Hate\": \"Accept\",\n      \"SelfHarm\": \"Accept\",\n      \"Sexual\": \"Accept\",\n      \"Violence\": \"Accept\"\n    },\n    \"suggested_action\": \"Accept\"\n  }\n```\n\n\n\nThe `action_by_category` field gives you a binary value for each category: *Accept* or *Reject*. This value shows if the text meets the sensitivity level that you set in the request parameters for that category.\n\nThe `suggested_action` field gives you an overall recommendation based on the four categories. If any category has a *Reject* value, the `suggested_action` will be *Reject* as well."}
{"text_chunk": "Embedding"}
{"text_chunk": "Introduction\nOpenAI's embedding models convert text into dense vector representations for various NLP tasks. See the OpenAI Embeddings API for more information."}
{"text_chunk": "Prerequisite\nCreate OpenAI resources:\n\n- **OpenAI**\n\n    Sign up account OpenAI website\n    Login and Find personal API key\n\n- **Azure OpenAI (AOAI)**\n\n    Create Azure OpenAI resources with instruction"}
{"text_chunk": "**Connections**\n\nSetup connections to provide resources in embedding tool.\n\n| Type        | Name     | API KEY  | API Type | API Version |\n|-------------|----------|----------|----------|-------------|\n| OpenAI      | Required | Required | -        | -           |\n| AzureOpenAI | Required | Required | Required | Required    |"}
{"text_chunk": "Inputs\n\n|  Name                  | Type        | Description                                                           | Required |\n|------------------------|-------------|-----------------------------------------------------------------------|----------|\n| input                  | string      | the input text to embed                                               | Yes      |\n| connection             | string      | the connection for the embedding tool use to provide resources         | Yes      |\n| model/deployment_name  | string      | instance of the text-embedding engine to use. Fill in model name if you use OpenAI connection, or deployment name if use Azure OpenAI connection.    | Yes      |"}
{"text_chunk": "Outputs\n\n| Return Type | Description                              |\n|-------------|------------------------------------------|\n| list        | The vector representations for inputs    |\n\nThe following is an example response returned by the embedding tool:\n\n\n  Output\n  \n```\n[-0.005744616035372019,\n-0.007096089422702789,\n-0.00563855143263936,\n-0.005272455979138613,\n-0.02355326898396015,\n0.03955197334289551,\n-0.014260607771575451,\n-0.011810848489403725,\n-0.023170066997408867,\n-0.014739611186087132,\n...]\n```"}
{"text_chunk": "Faiss Index Lookup\n\nFaiss Index Lookup is a tool tailored for querying within a user-provided Faiss-based vector store. In combination with our Large Language Model (LLM) tool, it empowers users to extract contextually relevant information from a domain knowledge base."}
{"text_chunk": "Requirements\n- For AzureML users, the tool is installed in default image, you can use the tool without extra installation.\n- For local users, if your index is stored in local path,\n  \n  `pip install promptflow-vectordb`\n  \n  if your index is stored in Azure storage,\n\n  `pip install promptflow-vectordb[azure]`"}
{"text_chunk": "Prerequisites\n  - step 1. Prepare an accessible path on Azure Blob Storage. Here's the guide if a new storage account needs to be created:  Azure Storage Account.\n  - step 2. Create related Faiss-based index files on Azure Blob Storage. We support the LangChain format (index.faiss + index.pkl) for the index files, which can be prepared either by employing our promptflow-vectordb SDK or following the quick guide from LangChain documentation. Please refer to the instructions of An example code for creating Faiss index for building index using promptflow-vectordb SDK.\n  - step 3. Based on where you put your own index files, the identity used by the promptflow runtime should be granted with certain roles. Please refer to Steps to assign an Azure role:\n\n      | Location | Role |\n      | ---- | ---- |\n      | workspace datastores or workspace default blob | AzureML Data Scientist |\n      | other blobs | Storage Blob Data Reader |"}
{"text_chunk": "For local users,\n  - Create Faiss-based index files in local path by only doing step 2 above."}
{"text_chunk": "Inputs\n\nThe tool accepts the following inputs:\n\n| Name | Type | Description | Required |\n| ---- | ---- | ----------- | -------- |\n| path | string | URL or path for the vector store.local path (for local users):`` Azure blob URL format (with [azure] extra installed):https://``.blob.core.windows.net/``/``.AML datastore URL format (with [azure] extra installed):azureml://subscriptions/``/resourcegroups/``/workspaces/``/data/``public http/https URL (for public demonstration):http(s)://`` | Yes |\n| vector | list[float] | The target vector to be queried, which can be generated by the LLM tool. | Yes |\n| top_k | integer | The count of top-scored entities to return. Default value is 3. | No |"}
{"text_chunk": "Outputs\n\nThe following is an example for JSON format response returned by the tool, which includes the top-k scored entities. The entity follows a generic schema of vector search result provided by our promptflow-vectordb SDK. For the Faiss Index Search, the following fields are populated:\n\n| Field Name | Type | Description |\n| ---- | ---- | ----------- |\n| text | string | Text of the entity |\n| score | float |  Distance between the entity and the query vector |\n| metadata | dict | Customized key-value pairs provided by user when create the index |\n\n\n  Output\n  \n```json\n[\n  {\n    \"metadata\": {\n      \"link\": \"http://sample_link_0\",\n      \"title\": \"title0\"\n    },\n    \"original_entity\": null,\n    \"score\": 0,\n    \"text\": \"sample text #0\",\n    \"vector\": null\n  },\n  {\n    \"metadata\": {\n      \"link\": \"http://sample_link_1\",\n      \"title\": \"title1\"\n    },\n    \"original_entity\": null,\n    \"score\": 0.05000000447034836,\n    \"text\": \"sample text #1\",\n    \"vector\": null\n  },\n  {\n    \"metadata\": {\n      \"link\": \"http://sample_link_2\",\n      \"title\": \"title2\"\n    },\n    \"original_entity\": null,\n    \"score\": 0.20000001788139343,\n    \"text\": \"sample text #2\",\n    \"vector\": null\n  }\n]\n\n```"}
{"text_chunk": "LLM"}
{"text_chunk": "Introduction\nPrompt flow LLM tool enables you to leverage widely used large language models like OpenAI or Azure OpenAI (AOAI) for natural language processing. \n\nPrompt flow provides a few different LLM APIs:\n- **Completion**: OpenAI's completion models generate text based on provided prompts.\n- **Chat**: OpenAI's chat models facilitate interactive conversations with text-based inputs and responses.\n> [!NOTE]\n> We now remove the `embedding` option from LLM tool api setting. You can use embedding api with Embedding tool."}
{"text_chunk": "Prerequisite\nCreate OpenAI resources:\n\n- **OpenAI**\n\n    Sign up account OpenAI website\n    Login and Find personal API key\n\n- **Azure OpenAI (AOAI)**\n\n    Create Azure OpenAI resources with instruction"}
{"text_chunk": "**Connections**\n\nSetup connections to provisioned resources in prompt flow.\n\n| Type        | Name     | API KEY  | API Type | API Version |\n|-------------|----------|----------|----------|-------------|\n| OpenAI      | Required | Required | -        | -           |\n| AzureOpenAI | Required | Required | Required | Required    |"}
{"text_chunk": "Inputs\n\n| Name                   | Type        | Description                                                                             | Required |\n|------------------------|-------------|-----------------------------------------------------------------------------------------|----------|\n| prompt                 | string      | text prompt that the language model will complete                                       | Yes      |\n| model, deployment_name | string      | the language model to use                                                               | Yes      |\n| max\\_tokens            | integer     | the maximum number of tokens to generate in the completion. Default is 16.              | No       |\n| temperature            | float       | the randomness of the generated text. Default is 1.                                     | No       |\n| stop                   | list        | the stopping sequence for the generated text. Default is null.                          | No       |\n| suffix                 | string      | text appended to the end of the completion                                              | No       |\n| top_p                  | float       | the probability of using the top choice from the generated tokens. Default is 1.        | No       |\n| logprobs               | integer     | the number of log probabilities to generate. Default is null.                           | No       |\n| echo                   | boolean     | value that indicates whether to echo back the prompt in the response. Default is false. | No       |\n| presence\\_penalty      | float       | value that controls the model's behavior with regards to repeating phrases. Default is 0.                              | No       |\n| frequency\\_penalty     | float       | value that controls the model's behavior with regards to generating rare phrases. Default is 0.                             | No       |\n| best\\_of               | integer     | the number of best completions to generate. Default is 1.                               | No       |\n| logit\\_bias            | dictionary  | the logit bias for the language model. Default is empty dictionary.                     | No       |"}
{"text_chunk": "Chat\n\n\n| Name                   | Type        | Description                                                                                    | Required |\n|------------------------|-------------|------------------------------------------------------------------------------------------------|----------|\n| prompt                 | string      | text prompt that the language model will response                                              | Yes      |\n| model, deployment_name | string      | the language model to use                                                                      | Yes      |\n| max\\_tokens            | integer     | the maximum number of tokens to generate in the response. Default is inf.                      | No       |\n| temperature            | float       | the randomness of the generated text. Default is 1.                                            | No       |\n| stop                   | list        | the stopping sequence for the generated text. Default is null.                                 | No       |\n| top_p                  | float       | the probability of using the top choice from the generated tokens. Default is 1.               | No       |\n| presence\\_penalty      | float       | value that controls the model's behavior with regards to repeating phrases. Default is 0.      | No       |\n| frequency\\_penalty     | float       | value that controls the model's behavior with regards to generating rare phrases. Default is 0.| No       |\n| logit\\_bias            | dictionary  | the logit bias for the language model. Default is empty dictionary.                            | No       |\n| function\\_call         | object      | value that controls which function is called by the model. Default is null.                    | No       |\n| functions              | list        | a list of functions the model may generate JSON inputs for. Default is null.                   | No       |\n| response_format        | object      | an object specifying the format that the model must output. Default is null.                   | No       |"}
{"text_chunk": "Outputs\n\n| API        | Return Type | Description                              |\n|------------|-------------|------------------------------------------|\n| Completion | string      | The text of one predicted completion     |\n| Chat       | string      | The text of one response of conversation |"}
{"text_chunk": "How to use LLM Tool?\n\n1. Setup and select the connections to OpenAI resources\n2. Configure LLM model api and its parameters\n3. Prepare the Prompt with guidance."}
{"text_chunk": "Open Model LLM"}
{"text_chunk": "Introduction\n\nThe Open Model LLM tool enables the utilization of a variety of Open Model and Foundational Models, such as Falcon and Llama 2, for natural language processing in Azure ML Prompt Flow.\n\nHere's how it looks in action on the Visual Studio Code prompt flow extension. In this example, the tool is being used to call a LlaMa-2 chat endpoint and asking \"What is CI?\".\n\n!Screenshot of the Open Model LLM On VScode Prompt Flow extension\n\nThis prompt flow tool supports two different LLM API types:\n\n- **Chat**: Shown in the example above. The chat API type facilitates interactive conversations with text-based inputs and responses.\n- **Completion**: The Completion API type is used to generate single response text completions based on provided prompt input."}
{"text_chunk": "Quick Overview: How do I use Open Model LLM Tool?\n\n1. Choose a Model from the AzureML Model Catalog and get it deployed.\n2. Connect to the model deployment.\n3. Configure the open model llm tool settings.\n4. Prepare the Prompt with guidance.\n5. Run the flow."}
{"text_chunk": "Prerequisites: Model Deployment\n\n1. Pick the model which matched your scenario from the Azure Machine Learning model catalog.\n2. Use the \"Deploy\" button to deploy the model to a AzureML Online Inference endpoint.\n2.1. Use one of the Pay as you go deployment options.\n\nMore detailed instructions can be found here Deploying foundation models to endpoints for inferencing."}
{"text_chunk": "Prerequisites: Connect to the Model\n\nIn order for prompt flow to use your deployed model, you will need to connect to it. There are several ways to connect."}
{"text_chunk": "1. Endpoint Connections\n\nOnce associated to a AzureML or Azure AI Studio workspace, the Open Model LLM tool can use the endpoints on that workspace.\n\n1. **Using AzureML or Azure AI Studio workspaces**: If you are using prompt flow in one of the web page based browsers workspaces, the online endpoints available on that workspace will automatically who up.\n\n2. **Using VScode or Code First**: If you are using prompt flow in VScode or one of the Code First offerings, you will need to connect to the workspace. The Open Model LLM tool uses the azure.identity DefaultAzureCredential client for authorization. One way is through setting environment credential values."}
{"text_chunk": "2. Custom Connections\n\nThe Open Model LLM tool uses the CustomConnection. Prompt flow supports two types of connections:\n\n1. **Workspace Connections** - These are connections which are stored as secrets on an Azure Machine Learning workspace. While these can be used, in many places, the are commonly created and maintained in the Studio UI.\n\n2. **Local Connections** - These are connections which are stored locally on your machine. These connections are not available in the Studio UX's, but can be used with the VScode extension.\n\nInstructions on how to create a workspace or local Custom Connection can be found here.\n\nThe required keys to set are:\n\n1. **endpoint_url**\n    - This value can be found at the previously created Inferencing endpoint.\n2. **endpoint_api_key**\n    - Ensure to set this as a secret value.\n    - This value can be found at the previously created Inferencing endpoint.\n3. **model_family**\n    - Supported values: LLAMA, DOLLY, GPT2, or FALCON\n    - This value is dependent on the type of deployment you are targeting."}
{"text_chunk": "Running the Tool: Inputs\n\nThe Open Model LLM tool has a number of parameters, some of which are required. Please see the below table for details, you can match these to the screen shot above for visual clarity.\n\n| Name | Type | Description | Required |\n|------|------|-------------|----------|\n| api | string | This is the API mode and will depend on the model used and the scenario selected. *Supported values: (Completion \\| Chat)* | Yes |\n| endpoint_name | string | Name of an Online Inferencing Endpoint with a supported model deployed on it. Takes priority over connection. | No |\n| temperature | float | The randomness of the generated text. Default is 1. | No |\n| max_new_tokens | integer | The maximum number of tokens to generate in the completion. Default is 500. | No |\n| top_p | float | The probability of using the top choice from the generated tokens. Default is 1. | No |\n| model_kwargs | dictionary | This input is used to provide configuration specific to the model used. For example, the Llama-02 model may use {\\\"temperature\\\":0.4}. *Default: {}* | No |\n| deployment_name | string | The name of the deployment to target on the Online Inferencing endpoint. If no value is passed, the Inferencing load balancer traffic settings will be used. | No |\n| prompt | string | The text prompt that the language model will use to generate it's response. | Yes |"}
{"text_chunk": "Outputs\n\n| API        | Return Type | Description                              |\n|------------|-------------|------------------------------------------|\n| Completion | string      | The text of one predicted completion     |\n| Chat       | string      | The text of one response int the conversation |"}
{"text_chunk": "Deploying to an Online Endpoint\n\nWhen deploying a flow containing the Open Model LLM tool to an online endpoint, there is an additional step to setup permissions. During deployment through the web pages, there is a choice between System-assigned and User-assigned Identity types. Either way, using the Azure Portal (or a similar functionality), add the \"Reader\" Job function role to the identity on the Azure Machine Learning workspace or Ai Studio project which is hosting the endpoint. The prompt flow deployment may need to be refreshed."}
{"text_chunk": "OpenAI GPT-4V"}
{"text_chunk": "Introduction\nOpenAI GPT-4V tool enables you to leverage OpenAI's GPT-4 with vision, also referred to as GPT-4V or gpt-4-vision-preview in the API, to take images as input and answer questions about them."}
{"text_chunk": "Prerequisites\n\n- Create OpenAI resources\n\n    Sign up account OpenAI website\n    Login and Find personal API key\n\n- Get Access to GPT-4 API\n\n    To use GPT-4 with vision, you need access to GPT-4 API. Learn more about How to get access to GPT-4 API"}
{"text_chunk": "Connection\n\nSetup connections to provisioned resources in prompt flow.\n\n| Type        | Name     | API KEY  |\n|-------------|----------|----------|\n| OpenAI      | Required | Required |"}
{"text_chunk": "Inputs\n\n| Name                   | Type        | Description                                                                                    | Required |\n|------------------------|-------------|------------------------------------------------------------------------------------------------|----------|\n| connection             | OpenAI      | the OpenAI connection to be used in the tool                                                   | Yes      |\n| model                  | string      | the language model to use, currently only support gpt-4-vision-preview                         | Yes      |\n| prompt                 | string      | The text prompt that the language model will use to generate it's response.                    | Yes      |\n| max\\_tokens            | integer     | the maximum number of tokens to generate in the response. Default is 512.                      | No       |\n| temperature            | float       | the randomness of the generated text. Default is 1.                                            | No       |\n| stop                   | list        | the stopping sequence for the generated text. Default is null.                                 | No       |\n| top_p                  | float       | the probability of using the top choice from the generated tokens. Default is 1.               | No       |\n| presence\\_penalty      | float       | value that controls the model's behavior with regards to repeating phrases. Default is 0.      | No       |\n| frequency\\_penalty     | float       | value that controls the model's behavior with regards to generating rare phrases. Default is 0. | No       |"}
{"text_chunk": "Outputs\n\n| Return Type | Description                              |\n|-------------|------------------------------------------|\n| string      | The text of one response of conversation |"}
{"text_chunk": "Prompt"}
{"text_chunk": "Introduction\nThe Prompt Tool in PromptFlow offers a collection of textual templates that serve as a starting point for creating prompts. \nThese templates, based on the Jinja2 template engine, facilitate the definition of prompts. The tool proves useful \nwhen prompt tuning is required prior to feeding the prompts into the Language Model (LLM) model in PromptFlow."}
{"text_chunk": "Inputs\n\n| Name               | Type   | Description                                              | Required |\n|--------------------|--------|----------------------------------------------------------|----------|\n| prompt             | string | The prompt template in Jinja                            | Yes      |\n| Inputs             | -      | List of variables of prompt template and its assignments | -        |"}
{"text_chunk": "Outputs\n\nThe prompt text parsed from the prompt + Inputs"}
{"text_chunk": "How to write Prompt?\n\n1. Prepare jinja template. Learn more about Jinja\n\n_In below example, the prompt incorporates Jinja templating syntax to dynamically generate the welcome message and personalize it based on the user's name. It also presents a menu of options for the user to choose from. Depending on whether the user_name variable is provided, it either addresses the user by name or uses a generic greeting._\n\n```jinja\nWelcome to {{ website_name }}!\n{% if user_name %}\n    Hello, {{ user_name }}!\n{% else %}\n    Hello there!\n{% endif %}\nPlease select an option from the menu below:\n1. View your account\n2. Update personal information\n3. Browse available products\n4. Contact customer support\n```\n\n2. Assign value for the variables.\n\n_In above example, two variables would be automatically detected and listed in '**Inputs**' section. Please assign values._"}
{"text_chunk": "Sample 1\nInputs\n\n| Variable      | Type   | Sample Value | \n|---------------|--------|--------------|\n| website_name  | string | \"Microsoft\"  |\n| user_name     | string | \"Jane\"       |\n\nOutputs\n\n```\nWelcome to Microsoft! Hello, Jane! Please select an option from the menu below: 1. View your account 2. Update personal information 3. Browse available products 4. Contact customer support\n```"}
{"text_chunk": "Sample 2\n\nInputs\n\n| Variable     | Type   | Sample Value   | \n|--------------|--------|----------------|\n| website_name | string | \"Bing\"         |\n| user_name    | string | \"              |\n\nOutputs\n\n```\nWelcome to Bing! Hello there! Please select an option from the menu below: 1. View your account 2. Update personal information 3. Browse available products 4. Contact customer support\n```"}
{"text_chunk": "Python"}
{"text_chunk": "Introduction\nUsers are empowered by the Python Tool to offer customized code snippets as self-contained executable nodes in PromptFlow.\nUsers can effortlessly create Python tools, edit code, and verify results with ease."}
{"text_chunk": "Inputs\n\n| Name   | Type   | Description                                          | Required |\n|--------|--------|------------------------------------------------------|---------|\n| Code   | string | Python code snippet                                  | Yes     |\n| Inputs | -      | List of tool function parameters and its assignments | -       |"}
{"text_chunk": "Types\n\n| Type                                                | Python example                  | Description                                |\n|-----------------------------------------------------|---------------------------------|--------------------------------------------|\n| int                                                 | param: int                      | Integer type                               |\n| bool                                                | param: bool                     | Boolean type                               |\n| string                                              | param: str                      | String type                                |\n| double                                              | param: float                    | Double type                                |\n| list                                                | param: list or param: List[T]   | List type                                  |\n| object                                              | param: dict or param: Dict[K, V] | Object type                                |\n| Connection | param: CustomConnection         | Connection type, will be handled specially |\n\n\nParameters with `Connection` type annotation will be treated as connection inputs, which means:\n- Promptflow extension will show a selector to select the connection.\n- During execution time, promptflow will try to find the connection with the name same from parameter value passed in.\n\nNote that `Union[...]` type annotation is supported **ONLY** for connection type,\nfor example, `param: Union[CustomConnection, OpenAIConnection]`."}
{"text_chunk": "Outputs\n\nThe return of the python tool function."}
{"text_chunk": "How to write Python Tool?"}
{"text_chunk": "Guidelines\n\n1. Python Tool Code should consist of a complete Python code, including any necessary module imports.\n\n2. Python Tool Code must contain a function decorated with @tool (tool function), serving as the entry point for execution. The @tool decorator should be applied only once within the snippet.\n\n   _Below sample defines python tool \"my_python_tool\", decorated with @tool_\n\n3. Python tool function parameters must be assigned in 'Inputs' section\n\n    _Below sample defines inputs \"message\" and assign with \"world\"_\n\n4. Python tool function shall have return\n\n    _Below sample returns a concatenated string_"}
{"text_chunk": "Code\n\nThe snippet below shows the basic structure of a tool function. Promptflow will read the function and extract inputs\nfrom function parameters and type annotations.\n\n```python\nfrom promptflow import tool\nfrom promptflow.connections import CustomConnection"}
{"text_chunk": "The inputs section will change based on the arguments of the tool function, after you save the code\n@tool\ndef my_python_tool(message: str, my_conn: CustomConnection) -> str:\n    my_conn_dict = dict(my_conn)\n    # Do some function call with my_conn_dict...\n    return 'hello ' + message\n```"}
{"text_chunk": "Inputs\n\n| Name    | Type   | Sample Value in Flow Yaml | Value passed to function|\n|---------|--------|-------------------------| ------------------------|\n| message | string | \"world\"                 | \"world\"                 |\n| my_conn | CustomConnection | \"my_conn\"               | CustomConnection object |\n\nPromptflow will try to find the connection named 'my_conn' during execution time."}
{"text_chunk": "outputs\n\n```python\n\"hello world\"\n```"}
{"text_chunk": "Keyword Arguments Support\nStarting from version 1.0.0 of PromptFlow and version 1.4.0 of Prompt flow for VS Code,\nwe have introduced support for keyword arguments (kwargs) in the Python tool.\n\n\n```python\nfrom promptflow import tool\n\n\n@tool\ndef print_test(normal_input: str, **kwargs):\n    for key, value in kwargs.items():\n        print(f\"Key {key}'s value is {value}\")\n    return len(kwargs)\n\n```\nWhen you add `kwargs` in your python tool like above code, you can insert variable number of inputs by the `+Add input` button.\n\n!Screenshot of the kwargs On VScode Prompt Flow extension"}
{"text_chunk": "SerpAPI"}
{"text_chunk": "Introduction\n\nThe SerpAPI API is a Python tool that provides a wrapper to the SerpAPI Google Search Engine Results API and [SerpApi Bing Search Engine Results API\n](https://serpapi.com/bing-search-api). \nWe could use the tool to retrieve search results from a number of different search engines, including Google and Bing, and you can specify a range of search parameters, such as the search query, location, device type, and more."}
{"text_chunk": "Prerequisite\n\nSign up at SERP API homepage"}
{"text_chunk": "Connection\nConnection is the model used to establish connections with Serp API.\n\n| Type        | Name     | API KEY  |\n|-------------|----------|----------|\n| Serp        | Required | Required |\n\n_**API Key** is on SerpAPI account dashboard_"}
{"text_chunk": "Inputs\n\nThe **serp api** tool supports following parameters:\n\n\n| Name     | Type    | Description                                                   | Required |\n|----------|---------|---------------------------------------------------------------|----------|\n| query    | string  | The search query to be executed.                              | Yes      |\n| engine   | string  | The search engine to use for the search. Default is 'google'. | Yes      |\n| num      | integer | The number of search results to return.Default is 10.         | No       |\n| location | string  | The geographic location to execute the search from.           | No       |\n| safe     | string  | The safe search mode to use for the search. Default is 'off'. | No       |"}
{"text_chunk": "Outputs\nThe json representation from serpapi query.\n\n| Engine   | Return Type | Output                                                |\n|----------|-------------|-------------------------------------------------------|\n| google   | json        | Sample |\n| bing     | json        | Sample         |"}
{"text_chunk": "Vector DB Lookup\n\nVector DB Lookup is a vector search tool that allows users to search top k similar vectors from vector database. This tool is a wrapper for multiple third-party vector databases. The list of current supported databases is as follows.\n\n| Name | Description |\n| --- | --- |\n| Azure Cognitive Search | Microsoft's cloud search service with built-in AI capabilities that enrich all types of information to help identify and explore relevant content at scale. |\n| Qdrant | Qdrant is a vector similarity search engine that provides a production-ready service with a convenient API to store, search and manage points (i.e. vectors) with an additional payload. |\n| Weaviate | Weaviate is an open source vector database that stores both objects and vectors. This allows for combining vector search with structured filtering. |\n\nThis tool will support more vector databases."}
{"text_chunk": "Requirements\n- For AzureML users, the tool is installed in default image, you can use the tool without extra installation.\n- For local users,\n\n  `pip install promptflow-vectordb`"}
{"text_chunk": "Prerequisites\n\nThe tool searches data from a third-party vector database. To use it, you should create resources in advance and establish connection between the tool and the resource.\n\n  - **Azure Cognitive Search:**\n    - Create resource Azure Cognitive Search.\n    - Add \"Cognitive search\" connection. Fill \"API key\" field with \"Primary admin key\" from \"Keys\" section of created resource, and fill \"API base\" field with the URL, the URL format is `https://{your_serive_name}.search.windows.net`.\n\n  - **Qdrant:**\n    - Follow the installation to deploy Qdrant to a self-maintained cloud server.\n    - Add \"Qdrant\" connection. Fill \"API base\" with your self-maintained cloud server address and fill \"API key\" field.\n\n  - **Weaviate:**\n    - Follow the installation to deploy Weaviate to a self-maintained instance.\n    - Add \"Weaviate\" connection. Fill \"API base\" with your self-maintained instance address and fill \"API key\" field."}
{"text_chunk": "Inputs\n\nThe tool accepts the following inputs:\n- **Azure Cognitive Search:**\n\n  | Name | Type | Description | Required |\n  | ---- | ---- | ----------- | -------- |\n  | connection | CognitiveSearchConnection | The created connection for accessing to Cognitive Search endpoint. | Yes |\n  | index_name | string | The index name created in Cognitive Search resource. | Yes |\n  | text_field | string | The text field name. The returned text field will populate the text of output. | No |\n  | vector_field | string | The vector field name. The target vector is searched in this vector field. | Yes |\n  | search_params | dict | The search parameters. It's key-value pairs. Except for parameters in the tool input list mentioned above, additional search parameters can be formed into a JSON object as search_params. For example, use `{\"select\": \"\"}` as search_params to select the returned fields, use `{\"search\": \"\"}` to perform a hybrid search. | No |\n  | search_filters | dict | The search filters. It's key-value pairs, the input format is like `{\"filter\": \"\"}` | No |\n  | vector | list | The target vector to be queried, which can be generated by Embedding tool. | Yes |\n  | top_k | int | The count of top-scored entities to return. Default value is 3 | No |\n\n- **Qdrant:**\n\n  | Name | Type | Description | Required |\n  | ---- | ---- | ----------- | -------- |\n  | connection | QdrantConnection | The created connection for accessing to Qdrant server. | Yes |\n  | collection_name | string | The collection name created in self-maintained cloud server. | Yes |\n  | text_field | string | The text field name. The returned text field will populate the text of output. | No |\n  | search_params | dict | The search parameters can be formed into a JSON object as search_params. For example, use `{\"params\": {\"hnsw_ef\": 0, \"exact\": false, \"quantization\": null}}` to set search_params. | No |\n  | search_filters | dict | The search filters. It's key-value pairs, the input format is like `{\"filter\": {\"should\": [{\"key\": \"\", \"match\": {\"value\": \"\"}}]}}` | No |\n  | vector | list | The target vector to be queried, which can be generated by Embedding tool. | Yes |\n  | top_k | int | The count of top-scored entities to return. Default value is 3 | No |\n\n- **Weaviate:**\n\n  | Name | Type | Description | Required |\n  | ---- | ---- | ----------- | -------- |\n  | connection | WeaviateConnection | The created connection for accessing to Weaviate. | Yes |\n  | class_name | string | The class name. | Yes |\n  | text_field | string | The text field name. The returned text field will populate the text of output. | No |\n  | vector | list | The target vector to be queried, which can be generated by Embedding tool. | Yes |\n  | top_k | int | The count of top-scored entities to return. Default value is 3 | No |"}
{"text_chunk": "Outputs\n\nThe following is an example JSON format response returned by the tool, which includes the top-k scored entities. The entity follows a generic schema of vector search result provided by promptflow-vectordb SDK. \n- **Azure Cognitive Search:**\n\n  For Azure Cognitive Search, the following fields are populated:\n  | Field Name | Type | Description |\n  | ---- | ---- | ----------- |\n  | original_entity | dict | the original response json from search REST API|\n  | score | float |  @search.score from the original entity, which evaluates the similarity between the entity and the query vector |\n  | text | string | text of the entity|\n  | vector | list | vector of the entity|\n  \n    Output\n    \n  ```json\n  [\n    {\n      \"metadata\": null,\n      \"original_entity\": {\n        \"@search.score\": 0.5099789,\n        \"id\": \"\",\n        \"your_text_filed_name\": \"sample text1\",\n        \"your_vector_filed_name\": [-0.40517663431890405, 0.5856996257406859, -0.1593078462266455, -0.9776269170785785, -0.6145604369828972],\n        \"your_additional_field_name\": \"\"\n      },\n      \"score\": 0.5099789,\n      \"text\": \"sample text1\",\n      \"vector\": [-0.40517663431890405, 0.5856996257406859, -0.1593078462266455, -0.9776269170785785, -0.6145604369828972]\n    }\n  ]\n  ```\n  \n\n- **Qdrant:**\n\n  For Qdrant, the following fields are populated:\n  | Field Name | Type | Description |\n  | ---- | ---- | ----------- |\n  | original_entity | dict | the original response json from search REST API|\n  | metadata | dict | payload from the original entity|\n  | score | float | score from the original entity, which evaluates the similarity between the entity and the query vector|\n  | text | string | text of the payload|\n  | vector | list | vector of the entity|\n\n  \n    Output\n    \n  ```json\n  [\n    {\n      \"metadata\": {\n        \"text\": \"sample text1\"\n      },\n      \"original_entity\": {\n        \"id\": 1,\n        \"payload\": {\n          \"text\": \"sample text1\"\n        },\n        \"score\": 1,\n        \"vector\": [0.18257418, 0.36514837, 0.5477226, 0.73029673],\n        \"version\": 0\n      },\n      \"score\": 1,\n      \"text\": \"sample text1\",\n      \"vector\": [0.18257418, 0.36514837, 0.5477226, 0.73029673]\n    }\n  ]\n  ```\n  \n\n- **Weaviate:**\n\n  For Weaviate, the following fields are populated:\n  | Field Name | Type | Description |\n  | ---- | ---- | ----------- |\n  | original_entity | dict | the original response json from search REST API|\n  | score | float | certainty from the original entity, which evaluates the similarity between the entity and the query vector|\n  | text | string | text in the original entity|\n  | vector | list | vector of the entity|\n\n  \n    Output\n    \n  ```json\n  [\n    {\n      \"metadata\": null,\n      \"original_entity\": {\n        \"_additional\": {\n          \"certainty\": 1,\n          \"distance\": 0,\n          \"vector\": [\n            0.58,\n            0.59,\n            0.6,\n            0.61,\n            0.62\n          ]\n        },\n        \"text\": \"sample text1.\"\n      },\n      \"score\": 1,\n      \"text\": \"sample text1.\",\n      \"vector\": [\n        0.58,\n        0.59,\n        0.6,\n        0.61,\n        0.62\n      ]\n    }\n  ]\n  ```"}
{"text_chunk": "Outputs\n\nThe following is an example JSON format response returned by the tool, which includes the top-k scored entities. The entity follows a generic schema of vector search result provided by promptflow-vectordb SDK. \n- **Azure Cognitive Search:**\n\n  For Azure Cognitive Search, the following fields are populated:\n  | Field Name | Type | Description |\n  | ---- | ---- | ----------- |\n  | original_entity | dict | the original response json from search REST API|\n  | score | float |  @search.score from the original entity, which evaluates the similarity between the entity and the query vector |\n  | text | string | text of the entity|\n  | vector | list | vector of the entity|\n  \n    Output\n    \n  ```json\n  [\n    {\n      \"metadata\": null,\n      \"original_entity\": {\n        \"@search.score\": 0.5099789,\n        \"id\": \"\",\n        \"your_text_filed_name\": \"sample text1\",\n        \"your_vector_filed_name\": [-0.40517663431890405, 0.5856996257406859, -0.1593078462266455, -0.9776269170785785, -0.6145604369828972],\n        \"your_additional_field_name\": \"\"\n      },\n      \"score\": 0.5099789,\n      \"text\": \"sample text1\",\n      \"vector\": [-0.40517663431890405, 0.5856996257406859, -0.1593078462266455, -0.9776269170785785, -0.6145604369828972]\n    }\n  ]\n  ```\n  \n\n- **Qdrant:**\n\n  For Qdrant, the following fields are populated:\n  | Field Name | Type | Description |\n  | ---- | ---- | ----------- |\n  | original_entity | dict | the original response json from search REST API|\n  | metadata | dict | payload from the original entity|\n  | score | float | score from the original entity, which evaluates the similarity between the entity and the query vector|\n  | text | string | text of the payload|\n  | vector | list | vector of the entity|\n\n  \n    Output\n    \n  ```json\n  [\n    {\n      \"metadata\": {\n        \"text\": \"sample text1\"\n      },\n      \"original_entity\": {\n        \"id\": 1,\n        \"payload\": {\n          \"text\": \"sample text1\"\n        },\n        \"score\": 1,\n        \"vector\": [0.18257418, 0.36514837, 0.5477226, 0.73029673],\n        \"version\": 0\n      },\n      \"score\": 1,\n      \"text\": \"sample text1\",\n      \"vector\": [0.18257418, 0.36514837, 0.5477226, 0.73029673]\n    }\n  ]\n  ```\n  \n\n- **Weaviate:**\n\n  For Weaviate, the following fields are populated:\n  | Field Name | Type | Description |\n  | ---- | ---- | ----------- |\n  | original_entity | dict | the original response json from search REST API|\n  | score | float | certainty from the original entity, which evaluates the similarity between the entity and the query vector|\n  | text | string | text in the original entity|\n  | vector | list | vector of the entity|\n\n  \n    Output\n    \n  ```json\n  [\n    {\n      \"metadata\": null,\n      \"original_entity\": {\n        \"_additional\": {\n          \"certainty\": 1,\n          \"distance\": 0,\n          \"vector\": [\n            0.58,\n            0.59,\n            0.6,\n            0.61,\n            0.62\n          ]\n        },\n        \"text\": \"sample text1.\"\n      },\n      \"score\": 1,\n      \"text\": \"sample text1.\",\n      \"vector\": [\n        0.58,\n        0.59,\n        0.6,\n        0.61,\n        0.62\n      ]\n    }\n  ]\n  ```"}
