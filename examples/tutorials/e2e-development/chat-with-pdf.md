# Tutorial: Chat with PDF

In this tutorial, we will create a simple flow that mimic the functionality of retrieval of relevant information from the PDF to generate an answer with GPT. 

## Overview
Retrieval Augmented Generation (or RAG) has become a prevalent pattern to build intelligent application with Large Language Models (or LLMs) since it can infuse external knowledge into the model, which is not trained with those up-to-date or proprietary information. The screenshot below shows how new Bing in Edge sidebar can answer questions based on the page content on the left - in this case is a PDF file.
![edge-chat-pdf](../../flows/chat/chat-with-pdf/assets/edge-chat-pdf.png)
Note that new Bing will also search web for more information to generate the answer, let's ignore that part for now.

In this tutorial we will try to mimic the functionality of retrieval of relevant information from the PDF to generate an answer with GPT. 

**We will show you how to**: 
1. Create a console chatbot "chat_with_pdf" which accepts an url to PDF file as argument, then answer questions based on the content of the PDF.
2. Create a prompt flow for the chatbot, mostly reuse the code from step #1 above.
3. Create a dataset with multiple questions to quick test the flow.
4. Evaluate the quality of the answers generated by the chat_with_pdf flow.
5. Integrating the tests and evaluations into your development cycle - unit tests and CI/CD.
6. Deploy the flow to Azure App Service and Streamlit to serve real user traffic.

## Prerequisite
To go through this tutorial:
1. Install dependencies
```bash
   cd ../../flows/chat/chat-with-pdf/
   pip install -r requirements.txt
```

2. Install VS code extension (optional but highly recommended)
   // TODO

## Console chatbot chat_with_pdf
A typical RAG application has two steps:
- **Retrieval**: Retrieve context information from external systems (database, search engine, files, etc.)
- **Generation**: Construct the prompt with the retrieved context and get response from LLMs.

Since the retrieval step is more of a search problem, it can get really complicated. A popular, simple but also effective approach is vector search, which requires a index building process. Say if you have one or more documents containing the context information, the index building process looks like:
1. **Chunk** Turn the documents into multiple chunks of text.
2. **Embedding** For each of the text chunk call an embedding model to turn it into an array of floats (we call it embedding or vector).
3. **Indexing** Put the vectors into an index or a database which supports vector search - returning the top K relevant/similar vectors from the index or DB.

Once the index is built, the **Retrieval** step is just about turning the quesion into embedding/vector and perform a vector search on the index to get the most relevant context for the question.

OK now back to the chatbot we want to build, a simplified design can be:

<img src="../../flows/chat/chat-with-pdf/assets/chat_with_pdf_simple.png" width="300" alt="chat with pdf simple design"/>

A more robust or real application should consider using an external vector DB to store the vectors, for this simple example we choose to use [FAISS](https://github.com/facebookresearch/faiss) index which can be stored as a file. To avoid downloading and index building for same PDF file again and again, we will add a check that if the PDF file already exists then we won't download, same for index building.

This design can serve question and answering pretty well, except when you come to multi-turn conversation with the chatbot. Imagine a scenario like this:

> $User: what is BERT?
>
> $Bot: BERT stands for Bidirectional Encoder Representations from Transformers. 
>
> $User: is it better than GPT?
>
> $Bot: ...

You would expect the chatbot smart enough to figure out the **it** in your second question stands for BERT and you question is really "is BERT better than GPT". However, if you throw the question "is it better than GPT" to embedding model and then vector index/db, they won't know **it** stands for BERT. Thus you won't get the most relevant context back from the index. To fix this issue, we will get help from LLM (GPT) to "rewrite" the question based on previous question. The new design looks like this:

<img src="../../flows/chat/chat-with-pdf/assets/chat_with_pdf_with_rewrite.png" width="400" alt="chat with pdf better design"/>

A "rewrite_question" step is performed before feeding the question to "find_context" step.

### Take a look at the chatbot in action!
You should be able to run the console app by:
```shell
python chat_with_pdf/main.py https://arxiv.org/pdf/1810.04805.pdf
```
> Note: https://arxiv.org/pdf/1810.04805.pdf is the paper about one of the most famous earlier LLMs: BERT.

It looks like below if everything goes fine:
![chatbot console](../../flows/chat/chat-with-pdf/assets/chatbot_console.gif)

Now let's look at actual code implmenting the chatbot.

### Configurations
While this is a minimal LLM app, there are a few things we later want to configure or even experiment with, let's put them into environment variables. In later section we will show you how to experiment with these configurations to get a better quality for the chat app.

Create .env file in this folder with below content and later load_dotenv() can be used to populate these into environment variables. We will explain what these are later when talking about how each step is implemented.

Check out [example env file](.env.example).
```ini
OPENAI_API_BASE=<AOAI_endpoint>
OPENAI_API_KEY=<AOAI_key>
OPENAI_API_VERSION=2023-05-15
EMBEDDING_MODEL_DEPLOYMENT_NAME=text-embedding-ada-002
CHAT_MODEL_DEPLOYMENT_NAME=gpt-35-turbo
PROMPT_TOKEN_LIMIT=3000
MAX_COMPLETION_TOKENS=256
CHUNK_SIZE=1024
CHUNK_OVERLAP=64
VERBOSE=False
```
Note: CHAT_MODEL_DEPLOYMENT_NAME should point to a chat model like gpt-3.5-turbo or gpt-4

```bash
# create connection needed by flow
if pf connection list | grep azure_open_ai_connection; then
    echo "azure_open_ai_connection already exists"
else
    pf connection create --file azure_openai.yml --name azure_open_ai_connection --set api_key=<your_api_key> api_base=<your_api_base>
fi
```

### Implementation of each steps
#### Download pdf: [download.py](../../flows/chat/chat-with-pdf/chat_with_pdf/download.py)
The downloaded PDF file will be stored into a temp folder.

#### Build index: [build_index.py](../../flows/chat/chat-with-pdf/chat_with_pdf/build_index.py)
A few libraries are used in this step to build index:
1. PyPDF2 for extraction of text from the PDF file.
2. OpenAI python library for generating embedding.
3. FAISS library to construct vector index and persist to a file. Note there is a another dict used to store the mapping from the vector index to the actual text snippet because when later we try to query for most relevant context we need to find the text snippets instead of just embedding/vector.
   
#### Rewrite question: [rewrite_question.py](../../flows/chat/chat-with-pdf/chat_with_pdf/rewrite_question.py)
This step is to use ChatGPT/GPT4 to rewrite the question to be better fit for finding relevant context from the vector index. The prompt file [rewrite_question.md](../../flows/chat/chat-with-pdf/chat_with_pdf/rewrite_question_prompt.md) should give you a better idea how it works. 

#### Find context: [find_context.py](../../flows/chat/chat-with-pdf/chat_with_pdf/find_context.py)
Loading FAISS index and the dict that are built in the "build index" step, turn the question into a vector (using the same embedding function in the build index step). There is a small trick in this step to make sure the context will not exceed the token limit of model input prompt ([aoai model max request tokens](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models), OpenAI has similar limit). The output of this step is the final prompt that QnA step will send to chat model.

#### QnA: [qna.py](../../flows/chat/chat-with-pdf/chat_with_pdf/qna.py)
Use OpenAI's ChatGPT or GPT4 model and ChatCompletion API to get a good answer from the history and context from PDF.

#### The main loop: [main.py](../../flows/chat/chat-with-pdf/chat_with_pdf/main.py)
This is the main entry of the chatbot with a loop to read question from user input and call the steps above to answer the question.

To make this example simpler, we store downloaded file and also the built index as local files, though there is a mechanism to honor cached file/index, the index loading still takes certain time and add up to the latency that user can feel. Also if the chatbot is hosted on a server then it requires the requests for same PDF file to hit same server node to make the cache effective. In a real-world scenario, you will likely want to store the index in a centralized service/DB instead. There're many such DB available like [Azure Cognitive Search](https://learn.microsoft.com/en-us/azure/search/vector-search-overview), [Pinecone](https://www.pinecone.io/), [Qdrant](https://qdrant.tech/), ...

## Prompt flow: when you start thinking about quality of your LLM app
Now the simple chatbot is working, which straightforward. And this is just the first step or the simpler part of the journey. Just like any machine learning based application, building a good LLM app usually requires a lot of tuning work, e.g. trying different prompts (rewrite question, QnA), different parameters (chunk size, overlap size, context limit etc.) or even different workflow design (in our example with or without rewrite_question step is a configuration you may want to experiment with).

We need proper tooling to allow experimenting and tuning with these LLM apps. Prompt flow is designed for this and it enables you to test your LLM apps with:
- Running a few examples and manually verify how it works out.
- Running larger scale test dataset and use more formal approach (with metrics) to evaluate the quality of your app.

You might learnt how to create a prompt flow from scratch. Building a prompt flow from existing code is also straightforward. You can construct a [chat flow]() either by composing the YAML file or using the visual editor of [Visual Studio Code extension]() and create a few wrappers for existing code. 

Check out below:
- [flow.dag.yaml](../../flows/chat/chat-with-pdf/flow.dag.yaml)
- [setup_env.py](../../flows/chat/chat-with-pdf/setup_env.py)
- [download_tool.py](../../flows/chat/chat-with-pdf/download_tool.py)
- [build_index_tool.py](../../flows/chat/chat-with-pdf/build_index_tool.py)
- [rewrite_question_tool.py](../../flows/chat/chat-with-pdf/rewrite_question_tool.py)
- [find_context_tool.py](../../flows/chat/chat-with-pdf/find_context_tool.py)
- [qna_tool.py](../../flows/chat/chat-with-pdf/qna_tool.py)

E.g. build_index_tool wrapper:
```python
from promptflow import tool
from chat_with_pdf.build_index import create_faiss_index


@tool
def build_index_tool(pdf_path: str) -> str:
    return create_faiss_index(pdf_path)
```

The setup_env node requires some explanation: you might recall that we use environment to manage different configurations including OpenAI API key in the console chatbot, in prompt flow we use [Connection]() to manage access to external services like OpenAI and support passing configuration object into flow so that you can do experimentation easier. The setup_env node is to write the properties from connection and configuration object into environment variables, so that the core code of chatbot can work as-is.

The flow looks like:
<img src="../../flows/chat/chat-with-pdf/assets/multi-node-flow-chat-with-pdf.png" width="500" alt="chat with pdf flow, multi-node"/>

## Prompt flow evaluations
Now the prompt flow for chat_with_pdf is created, you might have already run/debug flow through the [Visual Studio Code extension](). It's time to do some testing and evaluation, which starts with:
1. Create a test dataset which contains a few question and pdf_url pairs.
2. Use existing [evaluation flows]() or develop new evaluation flows to generate metrics.

A small dataset can be found here: [bert-paper-qna.jsonl](../../flows/chat/chat-with-pdf/data/bert-paper-qna.jsonl) which contains around 10 questions for the BERT paper.

You can do a batch run with the test dataset and manual review the output. This can be done through the Visual Studio Code extension, or CLI or Python SDK.

**batch_run.yaml**
```yaml
name: chat_with_pdf_default_20230820_162219_559000
flow: .
data: ./data/bert-paper-qna.jsonl
#run: <Uncomment to select a run input>
column_mapping:
  chat_history: ${data.chat_history}
  pdf_url: ${data.pdf_url}
  question: ${data.question}
  config: 
    EMBEDDING_MODEL_DEPLOYMENT_NAME: text-embedding-ada-002
    CHAT_MODEL_DEPLOYMENT_NAME: gpt-35-turbo
    PROMPT_TOKEN_LIMIT: 3000
    MAX_COMPLETION_TOKENS: 256
    VERBOSE: true
    CHUNK_SIZE: 1024
    CHUNK_OVERLAP: 64
```
**CLI**
```bash
run_name="web_classification_"$(openssl rand -hex 12)
pf run create --file batch_run.yaml --stream --name $run_name
```

The output will include something like below:
```json
{
    "name": "chat_with_pdf_default_20230820_162219_559000",
    "created_on": "2023-08-20T16:23:39.608101",
    "status": "Completed",
    "display_name": "chat_with_pdf_default_20230820_162219_559000",
    "description": null,
    "tags": null,
    "properties": {
        "flow_path": "/Users/jietong/Work/azure-promptflow/scratchpad/chat_with_pdf",
        "output_path": "/Users/jietong/.promptflow/.runs/chat_with_pdf_default_20230820_162219_559000"
    },
    "flow_name": "chat_with_pdf",
    "data": "/Users/jietong/Work/azure-promptflow/scratchpad/chat_with_pdf/data/bert-paper-qna.jsonl",
    "output": "/Users/jietong/.promptflow/.runs/chat_with_pdf_default_20230820_162219_559000/    flow_outputs/output.jsonl"
}
```

And we developed two evaluation flows one for "[groundedness](../../flows/evaluation/eval-groundedness/)" and one for "[perceived intelligence](../../flows/evaluation/eval-perceived-intelligence/)". Reading the prompts will give you better idea what are these two metrics:
- [groundedness prompt](../../flows/evaluation/eval-groundedness/gpt_groundedness.md)
- [perceived intelligence prompt](../../flows/evaluation/eval-perceived-intelligence/gpt_perceived_intelligence.md)

Evaluation is also a batch run - batch run of evaluation flow with the previous run as input.

**eval_run.yaml:**
```yaml
flow: ../../evaluation/eval-groundedness
run: chat_with_pdf_default_20230820_162219_559000
column_mapping:
  question: ${run.inputs.question}
  answer: ${run.outputs.answer}
  context: ${run.outputs.context}
```
NOTE: the run property in eval_run.yaml is the run name of batch_run.yaml

**CLI:**
```bash
eval_run_name="eval_groundedness_"$(openssl rand -hex 12)
pf run create --file eval_run.yaml --run $run_name --name $eval_run_name
```

After the run completes you can use below commands to get detail of the runs:
```bash
pf run show-details --name $eval_run_name
pf run show-metrics --name $eval_run_name
pf run visualize --name $eval_run_name
```

## Experimentation!!
Now we've seen how to run tests/evaluations for prompt flow. And we have two metrics defined to measure the quality of our chat_with_pdf flow. We can try different settings/configurations and run the evaluations, and then compare the metrics to decide a best configuration for production deployment.

A few things we can try (but not limited to):
1. Different prompts for the rewrite_question and/or qna steps
2. Different chunk size or chunk overlap in index building
3. Different context limit

These can be controlled through the "config" object in the flow inputs. If you want to experiment with #1 (different prompts) you can add properties to the config object to control that behavior - just pointing to different prompt file(s).

Take a look at how we experiment with #3 in below test: [test_eval in tests/chat_with_pdf_test.py](../../flows/chat/chat-with-pdf/tests/azure_chat_with_pdf_test.py). This test will create 6 runs in total:

1. chat_with_pdf_2k_context
2. chat_with_pdf_3k_context
3. eval_groundedness_chat_with_pdf_2k_context
4. eval_perceived_intelligence_chat_with_pdf_2k_context
5. eval_groundedness_chat_with_pdf_3k_context
6. eval_perceived_intelligence_chat_with_pdf_3k_context

As you can probably tell through the names: run #3 and #4 generate metrics for run #1, run #5 and #6 generate metrics for run #2. You can compare these metrics to decide which performs better - 2K context or 3K context.

NOTE: [azure_chat_with_pdf_test](../../flows/chat/chat-with-pdf/tests/azure_chat_with_pdf_test.py) does the same tests but using Azure ML as backend, so you can see all the runs in a nice web portal where you can see logs and metrics comparison etc. 


Further reading:
- Learn [how to experiment with the chat-with-pdf flow](../../flows/chat/chat-with-pdf/chat-with-pdf.ipynb)
- Learn [how to experiment with the chat-with-pdf flow on Azure](../../flows/chat/chat-with-pdf/chat-with-pdf-azure.ipynb) so that you can collaborate with your team.

## Integrate prompt flow into your CI/CD workflow
It's also straightforward to integrate these into your CI/CD workflow using either CLI or SDK. In this example we have various unit tests to run tests/evaluations for chat_with_pdf flow.

Check the [test](../../flows/chat/chat-with-pdf/tests/) folder.

```bash
# run all the tests
python -m unittest discover -s tests -p '*_test.py'
```

## Deployment
//TODO: command line and screenshot of pf flow export and deploy to different cloud platforms