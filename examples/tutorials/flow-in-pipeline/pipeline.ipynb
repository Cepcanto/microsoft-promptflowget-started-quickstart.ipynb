{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Flow in Pipeline\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with compute cluster\n",
    "- A python environment (3.9 recommended)\n",
    "- Installed Azure Machine Learning Python SDK v2\n",
    "- Installed PromptFlow SDK\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Connect to your AML workspace from the Python SDK\n",
    "- Load a flow as a `ParallelComponent`\n",
    "- Using the component along with other components loaded from yaml in one `PipelineJob`.\n",
    "\n",
    "**Motivations** - This notebook covers the scenario that user use a flow along with other data processing steps in a pipeline.\n",
    "\n",
    "Below is the command to install dependent packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-sdk[builtins,azure]==0.0.99531891 --extra-index-url https://azuremlsdktestpypi.azureedge.net/promptflow/\n",
    "# azure-ai-ml is of a private version for now, so need to install separately after installing promptflow-sdk. \n",
    "# Targeting to release on 1.9.0 (July release)\n",
    "%pip install azure-ai-ml==1.9.0a20230703002 --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to other SDK in azure-ai-ml, you need to import related packages and prepare a ML client connecting to a specific workspace first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": \"<SUBSCRIPTION_ID>\",\n",
    "        \"resource_group\": \"<RESOURCE_GROUP>\",\n",
    "        \"workspace_name\": \"<AML_WORKSPACE_NAME>\",\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
    "        print(\n",
    "            \"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <AML_WORKSPACE_NAME> in notebook cell\"\n",
    "        )\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "\n",
    "        config_path = \"../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load flow as a component\n",
    "\n",
    "Suppose you have already authored a flow, you can load it as component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.azure import load_as_component, configure\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# the remote component will be created on calling load_as_component, \n",
    "# so ml_client must be configured for promptflow-sdk first\n",
    "configure(client=ml_client)\n",
    "\n",
    "flow_component = load_as_component(\n",
    "    r\"C:\\\\PyCharmProjects\\\\PromptFlow\\\\src\\\\promptflow-sdk\\\\tests\\\\test_configs\\\\flows\\web_classification\",\n",
    "    inputs_mapping={\n",
    "        \"groundtruth\": \"${data.answer}\",\n",
    "        \"prediction\": \"${variant.outputs.category}\",\n",
    "    },\n",
    "    component_type=\"parallel\"\n",
    ")\n",
    "\n",
    "print(flow_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use this component along with other components in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv2jsonl_component = load_component(\"./tsv2jsonl-component/component_spec.yaml\")\n",
    "\n",
    "@pipeline\n",
    "def pipeline_with_flow(input_data):\n",
    "    data_transfer = tsv2jsonl_component(input_data=input_data)\n",
    "\n",
    "    flow_node = flow_component(\n",
    "        # this can be either a URI jsonl file or a URI folder containing multiple jsonl files\n",
    "        data=data_transfer.outputs.output_data,\n",
    "        # this is to overwrite connection settings\n",
    "        connections={\n",
    "            # this is to overwrite connection related settings for a LLM node\n",
    "            # \"summarize_text_content\" is the node name\n",
    "            \"summarize_text_content\": {\n",
    "                \"deployment_name\": \"another_deployment_name\",\n",
    "                \"connection\": \"another_connection\"\n",
    "            },\n",
    "            # you can overwrite inputs mapping here\n",
    "            \"groundtruth\": \"Channel\",\n",
    "            # you can overwrite connection input of a python node here\n",
    "            # \"convert_to_dict\": {\n",
    "            #     \"conn1\": \"another_connection\"\n",
    "            # }\n",
    "        },\n",
    "    )\n",
    "    # node level run settings for flow node is similar to `ParallelComponent`\n",
    "    flow_node.logging_level = \"DEBUG\"\n",
    "    flow_node.max_concurrency_per_instance = 2\n",
    "    return flow_node.outputs\n",
    "\n",
    "pipeline = pipeline_with_flow(\n",
    "    input_data=Input(path=\"./data.tsv\", type=AssetTypes.URI_FILE),\n",
    ")\n",
    "\n",
    "pipeline.settings.default_compute = \"cpu-cluster\"\n",
    "\n",
    "created_job = ml_client.jobs.create_or_update(pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other pipeline jobs in azure-ai-ml, you can monitor the status of the job via `ml_client.jobs.stream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(created_job.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
