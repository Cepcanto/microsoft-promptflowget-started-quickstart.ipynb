{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use Azure machine learning(ML) pipelines to run your flows on the cloud?\n",
    "In real-world scenarios, flows serve various purposes. For example, consider a flow designed to evaluate the relevance score for a communication session between humans and agents. Suppose you want to trigger this flow every night to assess today’s performance and avoid peak hours for LLM (Language Model) endpoints. In this common scenario, people often encounter the following needs:\n",
    "- Handling Large Data Inputs: Running flows with thousands or millions of data inputs at once.\n",
    "- Scalability and Efficiency: Requiring a scalable, efficient, and resilient platform to ensure success.\n",
    "- Automations: Automatically triggering batch flows when upstream data is ready or at fixed intervals.\n",
    "\n",
    "__Azure ML pipelines__ address all these offline requirements effectively. With the integration of prompt flows and Azure ML pipeline, flow users could very easily achieve above goals and in this tutorial, you can learn:\n",
    "- How to use python SDK to automatically convert your flow into a 'step' in Azure ML pipeline.\n",
    "- How to feed your data into pipeline to trigger the batch flow runs.\n",
    "- How to build other pipeline steps ahead or behind your prompt flow step. e.g. data preprocessing or result aggregation.\n",
    "- How to setup a simple scheduler on my pipeline.\n",
    "- How to deploy pipeline to an Azure ML batch endpoint. Then I can invoke it with new data when needed.\n",
    "\n",
    "Before you begin, consider the following prerequisites:\n",
    "- Introduction to Azure ML Platform:\n",
    "    - [Core site of Azure ML platform](https://learn.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning?view=azureml-api-2).\n",
    "    - Understand what [Azure ML pipelines](https://learn.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines?view=azureml-api-2) and [component](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2) are.\n",
    "- Azure cloud setup:\n",
    "    - An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "    - Create an Azure ML resource from Azure portal - [Create a Azure ML workspace](https://ms.portal.azure.com/#view/Microsoft_Azure_Marketplace/MarketplaceOffersBlade/searchQuery/machine%20learning)\n",
    "    - Connect to your workspace then setup a basic computer cluster - [Configure workspace](../../configuration.ipynb)\n",
    "- Local environment setup:\n",
    "    - A python environment\n",
    "    - Installed Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md) - check the getting started section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, load_component, Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get a handle to the workspace\n",
    "\n",
    "We use 'config file' to connect to your workspace. Check [this notebook](../../configuration.ipynb) to get your config file from Azure ML workspace portal and paste it into this folder. Then if you pass the next code block, you've all set for the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cluster_name = \"cpu-cluster\"\n",
    "print(ml_client.compute.get(cluster_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load flow as component\n",
    "If you’ve already authored a flow using the Promptflow SDK or portal, you can locate the flow.dag.yaml file within the flow folder. This YAML specification is essential for loading your flow into an Azure ML component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_component = load_component(\"../../flows/standard/web-classification/flow.dag.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `load_component` function and the flow YAML specification, your flow is automatically transformed into a __[parallel component](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-parallel-job-in-pipeline?view=azureml-api-2&tabs=cliv2#why-are-parallel-jobs-needed)__. This parallel component is designed for large-scale, offline, parallelized processing with efficiency and resilience. Here are some key features of this auto-converted component:\n",
    "\n",
    " - Pre-defined input and output ports:\n",
    "\n",
    "    ![prompt flow base component image](../../../docs/media/cloud/flow-in-pipeline/pf-base-component.png)\n",
    "    | port name  |  type  | description |\n",
    "    | ---------- | ------ | ----------- |\n",
    "    | data | uri_folder or uri_file | Accepts batch data input to your flow. You can use either the `uri_file` data type if your data is a single file or the `uri_folder` data type if your folder contains multiple files with the same schema. The default data type is jsonl, but you can customize this setting after declaring an instance of this flow component in your pipeline. Note that your data will be converted into a dataframe, so ensure that your CSV or TSV data includes a header line for proper mapping. |\n",
    "    | flow_outputs | uri_file | Generates a single output file named parallel_run_step.jsonl. Each line in this data file corresponds to a JSON object representing the flow returns, along with an additional column called line_number indicating its position from the original file. |\n",
    "    | debug_info | uri_folder | If you run your flow component in __debug mode__, this port provides debugging information for each run of your lines. E.g. intermediate outputs between steps, or LLM response and token usage. |\n",
    "\n",
    " - Auto-generated parameters \n",
    " \n",
    "   These parameters represent all your flow inputs and connections associated with your flow steps. You can set default values in the flow/run definition, and they can be further customized during job submission. Use '[web-classification](../../flows/standard/web-classification/flow.dag.yaml)' sample flow for example, this flow has only one input named 'url' and 2 LLM steps 'summarize_text_content' and 'classify_with_llm'. The input parameters of this flow component are:\n",
    " \n",
    "   ![prompt flow base component image](../../../docs/media/cloud/flow-in-pipeline/pf-component-parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build your pipeline\n",
    "## 3.1 Declare input and output\n",
    "To supply your pipeline with data, you need to declare an input using the `path`, `type`, and `mode` properties. Please note: `mount` is the default and suggested mode for your file or folder data input.\n",
    "\n",
    "Declaring the pipeline output is optional. However, if you require a customized output path in the cloud, you can follow the example below to set the path on the datastore. For more detailed information on valid path values, refer to this documentation - [manage pipeline inputs outputs](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-inputs-outputs-pipeline?view=azureml-api-2&tabs=cli#path-and-mode-for-data-inputsoutputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = Input(\n",
    "    path=\"../../flows/standard/web-classification/data.jsonl\", \n",
    "    type=AssetTypes.URI_FILE,\n",
    "    mode=\"mount\"\n",
    ")\n",
    "\n",
    "pipeline_output = Output(\n",
    "    # Provide custom flow output file path if needed\n",
    "    # path=\"azureml://datastores/<data_store_name>/paths/<path>\",\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    # rw_mount is suggested for flow output\n",
    "    mode=\"rw_mount\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 Run pipeline with single flow component\n",
    "Since all Promptflow components are based on Azure ML parallel components, users can leverage specific __run settings__  to control the parallelization of flow runs. Below are some useful settings:\n",
    "\n",
    "| run settings | description | allowed values | default value |\n",
    "| ------------ | ----------- | -------------- | ------------- |\n",
    "| PF_INPUT_FORMAT | When utilizing `uri_folder` as the input data, this setting allows you to specify which file extensions should be treated as data files for initializing flow runs. | json, jsonl, csv, tsv | jsonl |\n",
    "| compute | Defines which compute cluster from your Azure ML workspace will be used for this job. | | |\n",
    "| instance_count | Define how many nodes from your compute cluster will be assigned to this job. | from 1 to node count of compute cluster. | 1 |\n",
    "| max_concurrency_per_instance | Defines how many dedicated processors will run the flow in parallel on 1 node. When combined with the 'instance_count' setting, the total parallelization of your flow will be instance_count*max_concurrency_per_instance.| >1 | 1 |\n",
    "| mini_batch_size | Define the number of lines for each mini-batches. A __mini-batch__ is the basic granularity for processing full data with parallelization. Each worker processor handles one mini-batch at a time, and all workers work in parallel across different nodes. | > 0 | 1 |\n",
    "| max_retries | Defines the retry count if any mini-batch encounters an inner exception. </br></br> Remark: The retry granularity is based on mini-batches. For instance, with the previous setting, you can set 100 lines per mini-batch. When one line execution encounters a transient issue or an unhandled exception, these 100 lines will be retried together, even if the remaining 99 lines are successful. Additionally, LLM responses with status code 429 will be handled internally for flow runs in most cases and will not trigger mini-batch failure. | >= 0 | 3 |\n",
    "| error_threshold | Defines how many failed lines are acceptable. If the count of failed lines exceeds this threshold, the job will be stopped and marked as failed. Set '-1' to disable this failure check. | -1 or >=0 | -1 |\n",
    "| mini_batch_error_threshold | Defines the maximum number of failed mini-batches that can be tolerated after all retries. Set '-1' to disable this failure check. | -1 or >=0 | -1 |\n",
    "| logging_level | Determines how parallel jobs save logs to disk. Setting to ‘DEBUG’ for the flow component allows the component to output intermediate flow logs into the ‘debug_info’ port. | INFO, WARNING, DEBUG | INFO |\n",
    "| timeout | Sets the timeout checker for each mini-batch execution in milliseconds. If a mini-batch runs longer than this threshold, it will be marked as failed and trigger the next retry. Consider setting a higher value based on your mini-batch size and total traffic throughput for your LLM endpoints. | > 0 | 600 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline as a function\n",
    "@pipeline()\n",
    "def pipeline_func_with_flow(\n",
    "    # Function inputs will be treated as pipeline input data or parameters. \n",
    "    # Pipeline input could be linked to step inputs to pass data between steps.\n",
    "    # Users are not required to define pipeline inputs. \n",
    "    # With pipeline inputs, user can provide the different data or values when they trigge different pipeline runs.\n",
    "    pipeline_input_data : Input,\n",
    "    parallel_node_count : int = 2,\n",
    "):\n",
    "    # Declare pipeline step 'flow_node' by using flow component\n",
    "    flow_node = flow_component(\n",
    "        # Bind the pipeline intput data to the port 'data' of the flow component\n",
    "        # If you don't have pipeline input, you can directly pass the 'data_input' object to the 'data'\n",
    "        # But with this approach, you can't provide different data when you trigger different pipeline runs.\n",
    "        # data=data_input,\n",
    "        data=pipeline_input_data,\n",
    "        # Declare which column of input data should be mapped to flow input\n",
    "        # the value pattern follows ${data.<column_name_from_data_input>}\n",
    "        url=\"${data.url}\",\n",
    "        # Provide the connection values of the flow component\n",
    "        # The value of connection and deployment_name should align with your workspace connection settings.\n",
    "        connections={\n",
    "            \"summarize_text_content\": {\n",
    "                \"connection\": \"azure_open_ai_connection\",\n",
    "                \"deployment_name\": \"gpt-35-turbo\",\n",
    "            },\n",
    "            \"classify_with_llm\": {\n",
    "                \"connection\": \"azure_open_ai_connection\",\n",
    "                \"deployment_name\": \"gpt-35-turbo\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Provide run settings of your flow component\n",
    "    # Only 'compute' is required and other setting will keep default value if not provided.\n",
    "    flow_node.environment_variables = {\n",
    "        \"PF_INPUT_FORMAT\": \"jsonl\",\n",
    "    }\n",
    "    flow_node.compute = \"cpu-cluster\"\n",
    "    flow_node.resources = { 'instance_count': parallel_node_count }\n",
    "    flow_node.max_concurrency_per_instance = 2\n",
    "    flow_node.mini_batch_size = 10\n",
    "    flow_node.retry_settings = {\n",
    "       \"max_retries\": -1,\n",
    "       \"timeout\":1200,\n",
    "    }\n",
    "    flow_node.error_threshold = -1\n",
    "    flow_node.mini_batch_error_threshold = -1\n",
    "    flow_node.logging_level = \"DEBUG\"\n",
    "\n",
    "    # Function return will be treated as pipeline output. This is not required.\n",
    "    return {\n",
    "        \"flow_result_folder\" : flow_node.outputs.flow_outputs\n",
    "    }\n",
    "\n",
    "\n",
    "# create pipeline instance\n",
    "pipeline_job = pipeline_func_with_flow(pipeline_input_data=data_input)\n",
    "pipeline_job.outputs.flow_result_folder = pipeline_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the pipeline job to your workspace then check the status of your job on UI through the link in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the pipeline job to your workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"Single_flow_component_pipeline_job\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.2 Run complex pipeline with multiple component\n",
    "In a typical pipeline, you’ll find multiple steps that encompass all your offline business requirements. If you’re aiming to construct a more intricate pipeline for production, explore the following resources:\n",
    " - [how to create component with SDK v2](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2)\n",
    " - Various component types:\n",
    "    - [Command](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-component-command?view=azureml-api-2)\n",
    "    - [Spark](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-component-spark?view=azureml-api-2)\n",
    "    - [Pipeline](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-component-pipeline?view=azureml-api-2)\n",
    "\n",
    "\n",
    "Additionally, consider the following sample code that loads two extra command components from a repository to construct a single offline pipeline:\n",
    " - __data_prep_component__ : This dummy data preprocessing step performs simple data sampling.\n",
    " - __result_parser_component__: Combining source data, flow results, and debugging output, it generates a single file containing origin queries, LLM predictions, and LLM token usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = Input(\n",
    "    path=\"../../flows/standard/web-classification/data.jsonl\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    mode=\"mount\"\n",
    ")\n",
    "\n",
    "# load components\n",
    "data_prep_component = load_component(\"./components/data-prep/data-prep.yaml\")\n",
    "result_parser_component = load_component(\"./components/result-parser/result-parser.yaml\")\n",
    "\n",
    "# load flow as component\n",
    "flow_component = load_component(\"../../flows/standard/web-classification/flow.dag.yaml\")\n",
    "\n",
    "@pipeline()\n",
    "def pipeline_func_with_flow(data):\n",
    "    data_prep_node = data_prep_component(\n",
    "        input_data_file=data,\n",
    "    )\n",
    "    data_prep_node.compute = \"cpu-cluster\"\n",
    "\n",
    "    flow_node = flow_component(\n",
    "        # Feed the output of data_prep_node to the flow component\n",
    "        data=data_prep_node.outputs.output_data_folder,\n",
    "        url=\"${data.url}\",\n",
    "        connections={\n",
    "            \"summarize_text_content\": {\n",
    "                \"connection\": \"azure_open_ai_connection\",\n",
    "                \"deployment_name\": \"gpt-35-turbo\",\n",
    "            },\n",
    "            \"classify_with_llm\": {\n",
    "                \"connection\": \"azure_open_ai_connection\",\n",
    "                \"deployment_name\": \"gpt-35-turbo\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    flow_node.environment_variables = {\"PF_INPUT_FORMAT\": \"csv\"}\n",
    "    flow_node.compute = \"cpu-cluster\"\n",
    "    flow_node.resources = { 'instance_count': 2 }\n",
    "    flow_node.outputs.flow_outputs.mode = \"rw_mount\"\n",
    "    flow_node.logging_level = \"DEBUG\"\n",
    "\n",
    "    result_parser_node = result_parser_component(\n",
    "        source_data=data_prep_node.outputs.output_data_folder,\n",
    "        pf_output_data=flow_node.outputs.flow_outputs,\n",
    "        pf_debug_data=flow_node.outputs.debug_info,\n",
    "    )\n",
    "    \n",
    "    result_parser_node.compute = \"cpu-cluster\"\n",
    "\n",
    "# create pipeline instance\n",
    "pipeline_job = pipeline_func_with_flow(data=data_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the pipeline job to your workspace then check the status of your job on UI through the link in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"Complex_flow_component_pipeline_job\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Setup scheduler for your pipeline\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. Deploy pipeline to an endpoint\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "You can see further examples of running a pipeline job [here](../)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           url    answer evidence  \\\n",
      "0             https://arxiv.org/abs/2307.04767  Academic     Both   \n",
      "1  https://www.youtube.com/watch?v=kYqRtjDBci8   Channel     Both   \n",
      "2  https://www.youtube.com/watch?v=kYqRtjDBci8   Channel     Both   \n",
      "3             https://arxiv.org/abs/2307.04767  Academic     Both   \n",
      "4             https://arxiv.org/abs/2307.04767  Academic     Both   \n",
      "\n",
      "   line_number pred_category pred_evidence prompt_tokens  duration  \\\n",
      "0            0      Academic          Both          1162  2.081417   \n",
      "1            1       Channel          Both           687  1.141858   \n",
      "2            2          None          None           684  2.037589   \n",
      "3            3      Academic          Both          1153  2.024096   \n",
      "4            4      Academic          Both          1162  2.641773   \n",
      "\n",
      "   completion_tokens  total_tokens  \n",
      "0              143.0        1305.0  \n",
      "1               40.0         727.0  \n",
      "2               37.0         721.0  \n",
      "3              134.0        1287.0  \n",
      "4              142.0        1304.0  \n"
     ]
    }
   ],
   "source": [
    "#### test section \n",
    "#### will removed later\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "input_data = pd.read_csv(\"./processed_data.csv\")\n",
    "output_data = pd.read_json(\"./parallel_run_step.jsonl\", lines=True)\n",
    "pf_debug_files = glob.glob(\"./debug_info/flow_artifacts/*.jsonl\")\n",
    "\n",
    "debug_df = pd.concat([pd.read_json(file, lines=True) for file in pf_debug_files])\n",
    "\n",
    "output_data.sort_values(by=\"line_number\", inplace=True, ignore_index=True)\n",
    "debug_df.sort_values(by=\"line_number\", inplace=True, ignore_index=True)\n",
    "\n",
    "# print(debug_df.head())\n",
    "# print( [i for i in debug_df.loc[:, \"run_info\"]] )\n",
    "\n",
    "\n",
    "input_data.loc[:, \"line_number\"] = output_data.loc [:, \"line_number\"]\n",
    "input_data.loc[:, \"pred_category\"] = output_data.loc[:, \"category\"]\n",
    "input_data.loc[:, \"pred_evidence\"] = output_data.loc[:, \"evidence\"]\n",
    "\n",
    "\n",
    "for i in range(len(debug_df)):\n",
    "    input_data.loc[i, \"prompt_tokens\"] = debug_df.loc[i, \"run_info\"][\"system_metrics\"][\"prompt_tokens\"]\n",
    "    input_data.loc[i, \"duration\"] = debug_df.loc[i, \"run_info\"][\"system_metrics\"][\"duration\"]\n",
    "    input_data.loc[i, \"completion_tokens\"] = debug_df.loc[i, \"run_info\"][\"system_metrics\"][\"completion_tokens\"]\n",
    "    input_data.loc[i, \"total_tokens\"] = debug_df.loc[i, \"run_info\"][\"system_metrics\"][\"total_tokens\"]\n",
    "\n",
    "print(input_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "pf_debug_files = glob.glob(\"./debug_info/flow_artifacts/*.jsonl\")\n",
    "\n",
    "debug_df = pd.concat([pd.read_json(file, lines=True) for file in pf_debug_files])\n",
    "\n",
    "# print(debug_df)\n",
    "\n",
    "input_data.loc[:, [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"duration\"]] = np.nan\n",
    "\n",
    "print(input_data)\n",
    "\n",
    "\n",
    "# with open(\"./processed_data_with_predictions.jsonl\", \"w\") as file:\n",
    "#     file.write(final_df.to_json(orient=\"records\", lines=True))"
   ]
  }
 ],
 "metadata": {
  "description": "Create pipeline using components to run a distributed job with tensorflow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "resources": "examples/flows/standard/web-classification"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
