{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use Azure machine learning(ML) pipelines to run your flows on the cloud?\n",
    "In real-world scenarios, flows serve various purposes. For example, consider a flow designed to evaluate the relevance score for a communication session between humans and agents. Suppose you want to trigger this flow every night to assess today’s performance and avoid peak hours for LLM (Language Model) endpoints. In this common scenario, people often encounter the following needs:\n",
    "- Handling Large Data Inputs: Running flows with thousands or millions of data inputs at once.\n",
    "- Scalability and Efficiency: Requiring a scalable, efficient, and resilient platform to ensure success.\n",
    "- Automations: Automatically triggering batch flows when upstream data is ready or at fixed intervals.\n",
    "\n",
    "__Azure ML pipelines__ address all these offline requirements effectively. With the integration of prompt flows and Azure ML pipeline, flow users could very easily achieve above goals.\n",
    "\n",
    "In this tutorial, you’ll learn:\n",
    "- How to use python SDK to automatically convert your flow into a 'step' in Azure ML pipeline.\n",
    "- How to feed your data into pipeline to trigger the batch flow runs.\n",
    "- How to build other pipeline steps ahead or behind your prompt flow step. e.g. data preprocessing or result aggregation.\n",
    "- How to setup a simple scheduler on my pipeline.\n",
    "- How to deploy my pipeline to an Azure ML batch endpoint. Then I can invoke it with new data when needed.\n",
    "\n",
    "Before you begin, consider the following prerequisites:\n",
    "- Introduction to Azure ML Platform:\n",
    "    - [Core site of Azure ML platform](https://learn.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning?view=azureml-api-2).\n",
    "    - Understand what [Azure ML pipelines](https://learn.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines?view=azureml-api-2) and [component](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2) are.\n",
    "- Azure cloud setup:\n",
    "    - An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "    - Create an Azure ML resource from Azure portal - [Create a Azure ML workspace](https://ms.portal.azure.com/#view/Microsoft_Azure_Marketplace/MarketplaceOffersBlade/searchQuery/machine%20learning)\n",
    "    - Connect to your workspace then setup a basic computer cluster - [Configure workspace](../../configuration.ipynb)\n",
    "- Local environment setup:\n",
    "    - A python environment\n",
    "    - Installed Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md) - check the getting started section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, load_component, Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
      "Attempted credentials:\n",
      "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
      "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n",
      "\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n",
      "\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n",
      "\tAzureCliCredential: Failed to invoke the Azure CLI\n",
      "\tAzurePowerShellCredential: Az.Account module >= 2.2.0 is not installed\n",
      "\tAzureDeveloperCliCredential: Azure Developer CLI could not be found. Please visit https://aka.ms/azure-dev for installation instructions and then,once installed, authenticate to your Azure account using 'azd auth login'.\n",
      "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get a handle to the workspace\n",
    "\n",
    "We use 'config file' to connect to your workspace. Check [this notebook](../../configuration.ipynb) to get your config file from Azure ML workspace portal and paste it into this folder. Then if you pass the next code block, you've all set for the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: D:\\repo\\promptflow-1\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable_node_public_ip: true\n",
      "id: /subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/alainli-rg/providers/Microsoft.MachineLearningServices/workspaces/alainli-prs-eastus2/computes/cpu-cluster\n",
      "identity:\n",
      "  principal_id: a94960a4-23cc-460e-9b1b-66b30a36ea87\n",
      "  tenant_id: 72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "  type: system_assigned\n",
      "idle_time_before_scale_down: 1800\n",
      "location: eastus2\n",
      "max_instances: 4\n",
      "min_instances: 0\n",
      "name: cpu-cluster\n",
      "provisioning_state: Succeeded\n",
      "size: STANDARD_D2_V2\n",
      "ssh_public_access_enabled: true\n",
      "tier: dedicated\n",
      "type: amlcompute\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cluster_name = \"cpu-cluster\"\n",
    "print(ml_client.compute.get(cluster_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load flow as component\n",
    "\n",
    "Suppose you already have a flow authored by Promptflow SDK or portal, you can find 'flow.dag.yaml' under the flow folder and we need this flow yaml spec to load your flow into an Azure ML component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_component = load_component(\"../../flows/standard/web-classification/flow.dag.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 'load_component' function and flow yaml spec, your flow is automatically converted into a __parallel component__. [Parallel component](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-parallel-job-in-pipeline?view=azureml-api-2&tabs=cliv2#why-are-parallel-jobs-needed) focuses on the offline large scale parallelized processing with efficiency and resilient. This auto converted component has:\n",
    " - Fixed input and output ports:\n",
    "\n",
    "    ![prompt flow base component image](../../../docs/media/cloud/flow-in-pipeline/pf-base-component.png)\n",
    "    | port name  |  type  | description |\n",
    "    | ---------- | ------ | ----------- |\n",
    "    | data | uri_folder or uri_file | Batch data input to your flow. This port accepts 4 file types: json, jsonl, csv, tsv. You could use 'uri_file' data type if your data is a single file or use 'uri_folder' with your folder path where contains all your file with same schema. The default data type would be 'jsonl' and you could overwrite this setting after declare a instance of this flow component in your pipeline. </br></br> Remark: Your data will be converted into dataframe which requires the column names to do the further mapping. Please make sure your csv or tsv data have the header line to initiate your data. |\n",
    "    | flow_outputs | uri_file | The output file of all returns from every flow run. This single file has fixed name and extension: 'parallel_run_step.jsonl'. Every line in this data file is an json object of your flow returns plus an extra column 'line_number' to indicate its place from origin file. |\n",
    "    | debug_info | uri_folder | Optional output port and only have data if you set your flow component run with debug mode. The data is mainly used for debugging purpose which contains all intermediate outputs between your flow steps for each of your lines. |\n",
    "\n",
    " - Input parameters which represent all your flow inputs and connections associate to your flow steps. Use 'web-classification' sample flow for example:\n",
    " \n",
    "   ![prompt flow base component image](../../../docs/media/cloud/flow-in-pipeline/pf-component-parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build your pipeline\n",
    "## 3.1 Declare input and output\n",
    "To feed your pipeline with your data, you need declare an Input with `path`, `type`, and `mode` properties.\n",
    "Pipeline output is not required to be declared. But if you need customized output path on cloud, you can follow the below example to set it path on datastore. Refer to [this doc](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-inputs-outputs-pipeline?view=azureml-api-2&tabs=cli#path-and-mode-for-data-inputsoutputs) for more detail value of path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = Input(\n",
    "    path=\"../../flows/standard/web-classification/data.jsonl\", \n",
    "    type=AssetTypes.URI_FILE,\n",
    "    mode=\"mount\"\n",
    ")\n",
    "\n",
    "pipeline_output = Output(\n",
    "    # Provide custom flow output file path if needed\n",
    "    # path=\"azureml://datastores/<data_store_name>/paths/<path>\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    mode=\"rw_mount\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 Build pipeline with single flow component\n",
    "As all prompt flow component are based on Azure ML parallel component, user could leverage parallel component __run settings__ to control the parallelization of flow runs. Here are some useful settings as below:\n",
    "\n",
    "| run settings | description | allowed values | default value |\n",
    "| ------------ | ----------- | -------------- | ------------- |\n",
    "| PF_INPUT_FORMAT | When you use `uri_folder` as the input data, this setting helps to define which file extension will be treated as data file to be initiated for flow runs. | json, jsonl, csv, tsv | jsonl |\n",
    "| compute | Define which compute cluster from your Azure ML workspace will be used for this job. | | |\n",
    "| instance_count | Define how many nodes from your compute cluster will be assigned to this job. | from 1 to node count of compute cluster. | 1 |\n",
    "| max_concurrency_per_instance | Define how many dedicated processor to run flow in parallel on 1 node. Combine with 'instance_count' setting, the total parallelization of your flow would be instance_count*max_concurrency_per_instance.| >1 | 1 |\n",
    "| mini_batch_size | Define the number of lines for each mini-batches. __Mini-batch__ is the basic granularity when process full date with parallelization. Every worker processor take one by one mini-batches and all workers work in parallel in different nodes. | > 0 | 1 |\n",
    "| max_retries | Define the retry count if any mini-batch encounter inner exception. </br></br> Remark: the retry granulartiy is based on mini-batch. For instance, with previous setting, user can set 100 lines a mini-batch. When one line execution meets transient issue or un-handled exception, this 100 lines will be tried together even the rest 99 lines are successful. Additionaly, LLM response 429 will be handled internal flow runs for every LLM steps and it will not trigger mini-batch failure in most of the cases. | >= 0 | 3 |\n",
    "| error_threshold | Define how many failures of lines could be acceptable. If the count of failed lines become higher than this threashold, the job will be stopped and marked as failed. Set '-1' to disable this failure check. | -1 or >=0 | -1 |\n",
    "| mini_batch_error_threshold | Define how many failed mini-batches could be acceptable after all retries. Set '-1' to disable this failure check. | -1 or >=0 | -1 |\n",
    "| logging_level | Define how parallel jobs save logs to disk. Set 'DEBUG' for flow component will allow the component to output flow intermediate logs into 'debug_info' port. | INFO, WARNING, DEBUG | INFO |\n",
    "| timeout | Define the timeout checker for each mini-batch execution in millisecond. If a mini-batch runs longer than this threshold, that mini-batch will be stopped as failed then trigger the next retry. Consider to set a higher number according to your mini-batch size and total traffic throuputs for your LLM endpoints. | > 0 | 600 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline()\n",
    "def pipeline_func_with_flow(data):\n",
    "    flow_node = flow_component(\n",
    "        data=data,\n",
    "        url=\"${data.url}\",\n",
    "        connections={\n",
    "            \"summarize_text_content\": {\n",
    "                \"connection\": \"azure_open_ai_connection\",\n",
    "                \"deployment_name\": \"gpt-35-turbo\",\n",
    "            },\n",
    "            \"classify_with_llm\": {\n",
    "                \"connection\": \"azure_open_ai_connection\",\n",
    "                \"deployment_name\": \"gpt-35-turbo\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    flow_node.environment_variables = {\n",
    "        \"PF_INPUT_FORMAT\": \"jsonl\",\n",
    "        # \"AZUREML_COLLIE_ENABLED\": \"true\",\n",
    "    }\n",
    "    \n",
    "    flow_node.compute = \"cpu-cluster\"\n",
    "    flow_node.resources = { 'instance_count': 3 }\n",
    "    flow_node.max_concurrency_per_instance = 2\n",
    "    flow_node.mini_batch_size = 10\n",
    "    flow_node.retry_settings = {\n",
    "       \"max_retries\": 1,\n",
    "       \"timeout\":1200,\n",
    "    }\n",
    "\n",
    "    flow_node.error_threshold = -1\n",
    "    flow_node.mini_batch_error_threshold = -1\n",
    "    flow_node.logging_level = \"DEBUG\"\n",
    "\n",
    "    return {\n",
    "        \"flow_result_folder\" : flow_node.outputs.flow_outputs\n",
    "    }\n",
    "\n",
    "\n",
    "# create pipeline instance\n",
    "pipeline_job = pipeline_func_with_flow(data=data_input)\n",
    "pipeline_job.outputs.flow_result_folder = pipeline_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = Input(\n",
    "    path=\"../../flows/standard/web-classification/data.jsonl\", type=AssetTypes.URI_FILE\n",
    ")\n",
    "\n",
    "data_prep_component = load_component(\"./components/data-prep/data-prep.yaml\")\n",
    "result_parser_component = load_component(\"./components/result-parser/result-parser.yaml\")\n",
    "flow_component = load_component(\"../../flows/standard/web-classification/flow.dag.yaml\")\n",
    "@pipeline()\n",
    "def pipeline_func_with_flow(data):\n",
    "    data_prep_node = data_prep_component(\n",
    "        input_data_file=data,\n",
    "    )\n",
    "    data_prep_node.compute = \"cpu-cluster\"\n",
    "\n",
    "    flow_node = flow_component(\n",
    "        data=data_prep_node.outputs.output_data_folder,\n",
    "        url=\"${data.url}\",\n",
    "        connections={\n",
    "            \"summarize_text_content\": {\n",
    "                \"connection\": \"azure_open_ai_connection\",\n",
    "                \"deployment_name\": \"gpt-35-turbo\",\n",
    "            },\n",
    "            \"classify_with_llm\": {\n",
    "                \"connection\": \"azure_open_ai_connection\",\n",
    "                \"deployment_name\": \"gpt-35-turbo\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    flow_node.environment_variables = {\"PF_INPUT_FORMAT\": \"csv\"}\n",
    "    flow_node.compute = \"cpu-cluster\"\n",
    "    flow_node.resources = { 'instance_count': 3 }\n",
    "    flow_node.outputs.flow_outputs.mode = \"rw_mount\"\n",
    "\n",
    "    result_parser_node = result_parser_component(\n",
    "        source_data=data_prep_node.outputs.output_data_folder,\n",
    "        pf_output_data=flow_node.outputs.flow_outputs,\n",
    "    )\n",
    "    \n",
    "    result_parser_node.compute = \"cpu-cluster\"\n",
    "\n",
    "# create pipeline instance\n",
    "pipeline_job = pipeline_func_with_flow(data=data_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_samples</td><td>polite_pizza_qvfhq41gfj</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/polite_pizza_qvfhq41gfj?wsid=/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourcegroups/alainli-rg/workspaces/alainli-prs-eastUS2&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x0000021C66FBC3A0>}, 'outputs': {'flow_result_folder': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x0000021C66FBC2E0>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'd:\\\\repo\\\\promptflow-1\\\\examples\\\\tutorials\\\\run-scalable-flow-with-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000021C66FBCBE0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'pipeline_func_with_flow', 'is_deterministic': None, 'inputs': {'data': {}}, 'outputs': {'flow_result_folder': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'flow_node': Parallel({'init': False, 'name': 'flow_node', 'type': 'parallel', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'd:\\\\repo\\\\promptflow-1\\\\examples\\\\tutorials\\\\run-scalable-flow-with-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000021C66FBE290>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': 'cpu-cluster', 'services': None, 'comment': None, 'job_inputs': {'url': '${data.url}', 'connections.summarize_text_content.connection': 'azure_open_ai_connection', 'connections.summarize_text_content.deployment_name': 'gpt-35-turbo', 'connections.classify_with_llm.connection': 'azure_open_ai_connection', 'connections.classify_with_llm.deployment_name': 'gpt-35-turbo', 'data': '${{parent.inputs.data}}'}, 'job_outputs': {'flow_outputs': '${{parent.outputs.flow_result_folder}}'}, 'inputs': {'url': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000021C66FBC430>, 'connections.summarize_text_content.connection': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000021C66FBCCA0>, 'connections.summarize_text_content.deployment_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000021C66FBCAF0>, 'connections.classify_with_llm.connection': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000021C66FBFC40>, 'connections.classify_with_llm.deployment_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000021C66FBC9A0>, 'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000021C66FBC070>}, 'outputs': {'flow_outputs': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x0000021C66FBC100>}, 'component': 'azureml_anonymous:1cf91b15-e0ad-438a-adaa-6ac8025a017f', 'referenced_control_flow_node_instance_id': None, 'kwargs': {}, 'instance_id': '8864c6af-436a-4618-9665-ad97e0188b8b', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'task': None, 'mini_batch_size': 10, 'partition_keys': None, 'input_data': None, 'retry_settings': {'timeout': 1200, 'max_retries': 1}, 'logging_level': 'DEBUG', 'max_concurrency_per_instance': 2, 'error_threshold': -1, 'mini_batch_error_threshold': -1, 'resources': {'instance_count': 3}, 'environment_variables': {'PF_INPUT_FORMAT': 'jsonl'}})}, 'job_types': {'parallel': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'polite_pizza_qvfhq41gfj', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/alainli0928/promptflow.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': '7469254f8b4f981bb69fb0ad40152d0f6ff41655', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/alainli-rg/providers/Microsoft.MachineLearningServices/workspaces/alainli-prs-eastUS2/jobs/polite_pizza_qvfhq41gfj', 'Resource__source_path': None, 'base_path': 'd:\\\\repo\\\\promptflow-1\\\\examples\\\\tutorials\\\\run-scalable-flow-with-pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000021C66FBC310>, 'serialize': <msrest.serialization.Serializer object at 0x0000021C66FBC6A0>, 'display_name': 'pipeline_func_with_flow', 'experiment_name': 'pipeline_samples', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://eastus2.api.azureml.ms/mlflow/v1.0/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/alainli-rg/providers/Microsoft.MachineLearningServices/workspaces/alainli-prs-eastUS2?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/polite_pizza_qvfhq41gfj?wsid=/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourcegroups/alainli-rg/workspaces/alainli-prs-eastUS2&tid=72f988bf-86f1-41af-91ab-2d7cd011db47', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"pipeline_samples\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "You can see further examples of running a pipeline job [here](../)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test section \n",
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv(\"./processed_data.csv\")\n",
    "output_data = pd.read_json(\"./parallel_run_step.jsonl\", lines=True)\n",
    "print(output_data.head())\n",
    "\n",
    "if len(input_data) != len(output_data):\n",
    "    raise Exception(\"Index mismatch between data source and result\")\n",
    "\n",
    "input_data = input_data.merge(output_data, how='left', left_index=True, right_on=\"line_number\", suffixes=('', '_predict'))\n",
    "\n",
    "print(input_data.head())\n",
    "\n",
    "with open(\"./processed_data_with_predictions.jsonl\", \"w\") as file:\n",
    "    file.write(input_data.to_json(orient=\"records\", lines=True))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "description": "Create pipeline using components to run a distributed job with tensorflow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "resources": "examples/flows/standard/web-classification"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
