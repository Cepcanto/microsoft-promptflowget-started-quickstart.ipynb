{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing with LLM application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing is a powerful tool for understanding the behavior of your LLM application, prompt flow tracing capability supports instrumentation for such scenario.\n",
    "\n",
    "This notebook will demonstrate how to use prompt flow to instrument and understand your LLM application.\n",
    "\n",
    "**Learning Objective** - Upon completion of this notebook, you will be able to:\n",
    "\n",
    "- Trace LLM application and visualize with prompt flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "To run this notebook example, please install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please configure your API key using an `.env` file, we have provided an example `.env.example` for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load api key and endpoint from .env to environ\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your LLM application\n",
    "\n",
    "This notebook example will build a LLM application with Azure OpenAI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# in this notebook example, we will use model \"gpt-35-turbo-16k\"\n",
    "deployment_name = \"gpt-35-turbo-16k\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_deployment=deployment_name,\n",
    "    api_version=\"2024-02-01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare one classic question for LLM\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the meaning of life?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=conversation,\n",
    "    model=deployment_name,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start trace using `promptflow.tracing.start_trace` to leverage prompt flow tracing capability; this will print a link to trace UI, where you can visualize the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-24 15:30:33,311][promptflow][DEBUG] - preparing home directory with default value.\n",
      "[2024-04-24 15:30:34,953][promptflow][DEBUG] - collection: trace-llm\n",
      "[2024-04-24 15:30:34,954][promptflow][DEBUG] - kwargs: {}\n",
      "[2024-04-24 15:30:34,955][promptflow][DEBUG] - read tracing context from environment: None\n",
      "[2024-04-24 15:30:34,956][promptflow][DEBUG] - operation context OTel attributes: {}\n",
      "[2024-04-24 15:30:34,956][promptflow][DEBUG] - start_trace_with_devkit.path(from kwargs): None\n",
      "[2024-04-24 15:30:34,957][promptflow][INFO] - pf.config.trace.destination: local\n",
      "[2024-04-24 15:30:34,957][promptflow][DEBUG] - trace destination does not need to be resolved, directly return...\n",
      "[2024-04-24 15:30:34,958][promptflow][INFO] - resolved tracing.trace.destination: local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt flow service...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-24 15:30:43,709][promptflow][DEBUG] - Prompt flow service is already running on port 23342, {\"promptflow\":\"1.9.0.dev0\"}\n",
      "\n",
      "[2024-04-24 15:30:43,710][promptflow][DEBUG] - Prompt flow service is serving on port 23342\n",
      "[2024-04-24 15:30:43,714][promptflow][DEBUG] - Prompt flow service is already running on port 23342, {\"promptflow\":\"1.9.0.dev0\"}\n",
      "\n",
      "[2024-04-24 15:30:43,715][promptflow][DEBUG] - set collection to environ: trace-llm\n",
      "[2024-04-24 15:30:43,716][promptflow][DEBUG] - set OTLP endpoint to environ: http://localhost:23342/v1/traces\n",
      "[2024-04-24 15:30:43,717][promptflow][DEBUG] - start setup exporter to prompt flow service...\n",
      "[2024-04-24 15:30:43,718][promptflow][DEBUG] - collection from environ: trace-llm\n",
      "[2024-04-24 15:30:43,719][promptflow][DEBUG] - collection_id from environ: None\n",
      "[2024-04-24 15:30:43,720][promptflow][DEBUG] - experiment from environ: None\n",
      "[2024-04-24 15:30:43,720][promptflow][DEBUG] - resource attributes: {'collection': 'trace-llm', 'service.name': 'promptflow'}\n",
      "[2024-04-24 15:30:43,722][promptflow][INFO] - tracer provider is already set, will merge the resource attributes...\n",
      "[2024-04-24 15:30:43,722][promptflow][DEBUG] - current resource: {'service.name': 'promptflow', 'collection': 'trace-llm'}\n",
      "[2024-04-24 15:30:43,723][promptflow][INFO] - tracer provider is updated with resource attributes: {'service.name': 'promptflow', 'collection': 'trace-llm'}\n",
      "[2024-04-24 15:30:43,723][promptflow][DEBUG] - environ OTEL_EXPORTER_OTLP_ENDPOINT: http://localhost:23342/v1/traces\n",
      "[2024-04-24 15:30:43,724][promptflow][INFO] - have not set exporter to prompt flow service, will set it...\n",
      "[2024-04-24 15:30:43,726][promptflow][DEBUG] - finish setup exporter to prompt flow service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can stop the prompt flow service with the following command:'\u001b[1mpf service stop\u001b[0m'.\n",
      "Alternatively, if no requests are made within 1 hours, it will automatically stop.\n",
      "You can view the traces from local: http://localhost:23342/v1.0/ui/traces/?#collection=trace-llm\n"
     ]
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace(collection=\"trace-llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the LLM application again, and you should be able to see new trace logged in the trace UI, and it is clickable to see more details.\n",
    "\n",
    "![llm-trace-detail](../../../../docs/media/trace/llm-app-trace-detail.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a deeply philosophical question and can vary depending on individual beliefs and perspectives. Some people find meaning in personal relationships, pursuing goals and passions, making a positive impact on the world, or seeking spiritual fulfillment. Ultimately, the meaning of life is subjective and can be discovered through self-reflection and introspection.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=conversation,\n",
    "    model=deployment_name,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "By now you have successfully tracing your LLM application with prompt flow.\n",
    "\n",
    "You can check out more examples:\n",
    "\n",
    "- [Trace LangChain](https://github.com/microsoft/promptflow/blob/main/examples/tutorials/tracing/langchain/trace-langchain.ipynb): tracing `LangChain` and visualize leveraging prompt flow.\n",
    "- [Trace AutoGen](https://github.com/microsoft/promptflow/blob/main/examples/tutorials/tracing/autogen-groupchat/trace-autogen-groupchat.ipynb): tracing `AutoGen` and visualize leveraging prompt flow.\n",
    "- [Trace your flow](https://github.com/microsoft/promptflow/blob/main/examples/flex-flows/basic/flex-flow-quickstart.ipynb): using promptflow @trace to structurally tracing your app and do evaluation on it with batch run.\n"
   ]
  }
 ],
 "metadata": {
  "build_doc": {
   "author": [
    "zhengfeiwang@github.com"
   ],
   "category": "local",
   "section": "Tracing",
   "weight": 10
  },
  "description": "Tracing LLM application",
  "kernelspec": {
   "display_name": "pf-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "resources": ""
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
