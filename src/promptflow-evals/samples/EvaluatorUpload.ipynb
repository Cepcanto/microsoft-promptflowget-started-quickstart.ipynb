{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Upload of evaluators\n",
    "In this notebook we are demonstrating the upload of the standard evaluators.\n",
    "\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3e21ceb-1e58-4ba7-884a-5e103aea7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import uuid\n",
    "import yaml\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import (\n",
    "    Model\n",
    ")\n",
    "\n",
    "from promptflow.client import PFClient\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "from promptflow.evals.evaluators import F1ScoreEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "## End to end demonstration of evaluator saving and uploading to Azure.\n",
    "### Saving the standard evaluators to the flex format.\n",
    "First we will create the promptflow client, which will be used to save the existing flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PFClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fcaf38-6caa-4c1b-ada5-479686232cd1",
   "metadata": {},
   "source": [
    "We will use F1 score evaluator from the standard evaluator set and save it to local directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66fae04d-f6d0-4cc3-b149-a6058158c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.flows.save(F1ScoreEvaluator, path='./f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03552c5a-3ecc-4154-a0bd-e3fe2831e323",
   "metadata": {},
   "source": [
    "Let us inspect, what has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a27602c-e2c8-49c2-8d00-1eb5a11e55a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score.py\n",
      "flow\n",
      "flow.flex.yaml\n",
      "__init__.py\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(os.listdir('f1_score')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4dbfe-faa0-44f9-a53e-310c946d91fe",
   "metadata": {},
   "source": [
    "The file, defining entrypoint of our model is called flow.flex.yaml, let us display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c30c3e3-8113-4e0f-9210-b062e7354099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluator entrypoint is f1_score:F1ScoreEvaluator\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('f1_score', 'flow.flex.yaml')) as fp:\n",
    "    flex_definition = yaml.safe_load(fp)\n",
    "print(f\"The evaluator entrypoint is {flex_definition['entry']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9bb9a12-8bdd-4679-9957-998f3c7ceb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-23 11:51:20,257][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run test_76ad6428-01a2-11ef-b9d7-00224877b1ea, log path: C:\\Users\\anksing\\.promptflow\\.runs\\test_76ad6428-01a2-11ef-b9d7-00224877b1ea\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the traces from local: http://localhost:64667/v1.0/ui/traces/?#run=test_76ad6428-01a2-11ef-b9d7-00224877b1ea\n",
      "2024-04-23 11:51:20 -0700   40292 execution          WARNING  Starting run without column mapping may lead to unexpected results. Please consult the following documentation for more information: https://aka.ms/pf/column-mapping\n",
      "2024-04-23 11:51:20 -0700   40292 execution.bulk     INFO     Current system's available memory is 31091.125MB, memory consumption of current process is 297.828125MB, estimated available worker count is 31091.125/297.828125 = 104\n",
      "2024-04-23 11:51:20 -0700   40292 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 104}.\n",
      "2024-04-23 11:51:22 -0700   40292 execution.bulk     INFO     Process name(SpawnProcess-2)-Process id(17416)-Line number(0) start execution.\n",
      "2024-04-23 11:51:22 -0700   40292 execution.bulk     INFO     Process name(SpawnProcess-4)-Process id(27636)-Line number(1) start execution.\n",
      "2024-04-23 11:51:22 -0700   40292 execution.bulk     INFO     Process name(SpawnProcess-3)-Process id(25448)-Line number(2) start execution.\n",
      "2024-04-23 11:51:23 -0700   17416 execution          WARNING  [Flex in line 0 (index starts from 0)] stderr> [2024-04-23 11:51:23,030][flowinvoker][INFO] - Getting connections from pf client with provider from args: local...\n",
      "2024-04-23 11:51:23 -0700   17416 execution          WARNING  [Flex in line 0 (index starts from 0)] stderr> [2024-04-23 11:51:23,040][flowinvoker][INFO] - Promptflow get connections successfully. keys: dict_keys([])\n",
      "2024-04-23 11:51:23 -0700   17416 execution          WARNING  [Flex in line 0 (index starts from 0)] stderr> [2024-04-23 11:51:23,040][flowinvoker][INFO] - Promptflow executor starts initializing...\n",
      "2024-04-23 11:51:23 -0700   17416 execution          WARNING  [Flex in line 0 (index starts from 0)] stderr> [2024-04-23 11:51:23,079][flowinvoker][INFO] - Promptflow executor initiated successfully.\n",
      "2024-04-23 11:51:23 -0700   17416 execution          WARNING  [Flex in line 0 (index starts from 0)] stderr> [2024-04-23 11:51:23,080][flowinvoker][INFO] - Validating flow input with data {'answer': \"To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\\n\\n```\\nfrom azureml.core import Experiment, Run\\nfrom azureml.core.workspace import Workspace\\n\\n# Define workspace and experiment\\nws = Workspace.from_config()\\nexp = Experiment(workspace=ws, name='my_experiment')\\n\\n# Create a new run\\nrun = exp.start_logging()\\n```\\n\\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.\", 'ground_truth': 'Paris is the capital of France.'}\n",
      "2024-04-23 11:51:23 -0700   17416 execution          WARNING  [Flex in line 0 (index starts from 0)] stderr> [2024-04-23 11:51:23,081][flowinvoker][INFO] - Execute flow with data {'answer': \"To create a run using the Azure Machine Learning API, you first need to create an Experiment. Once you have an experiment, you can create a Run object that is associated with that experiment. Here is some Python code that demonstrates this process:\\n\\n```\\nfrom azureml.core import Experiment, Run\\nfrom azureml.core.workspace import Workspace\\n\\n# Define workspace and experiment\\nws = Workspace.from_config()\\nexp = Experiment(workspace=ws, name='my_experiment')\\n\\n# Create a new run\\nrun = exp.start_logging()\\n```\\n\\nIn this code, the `from_config()` method reads the configuration file that you created when you set up your Azure Machine Learning workspace. The `Experiment` constructor creates an Experiment object that is associated with your workspace, and the `start_logging()` method creates a new Run object that is associated with the Experiment. Now you can use the `run` object to log metrics, upload files, and track other information related to your machine learning experiment.\", 'ground_truth': 'Paris is the capital of France.'}\n",
      "2024-04-23 11:51:23 -0700   40292 execution.bulk     INFO     Process name(SpawnProcess-2)-Process id(17416)-Line number(0) completed.\n",
      "2024-04-23 11:51:23 -0700   40292 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-04-23 11:51:23 -0700   40292 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 2.0 seconds.\n",
      "2024-04-23 11:51:25 -0700   25448 execution          WARNING  [Flex in line 2 (index starts from 0)] stderr> [2024-04-23 11:51:25,280][flowinvoker][INFO] - Getting connections from pf client with provider from args: local...\n",
      "2024-04-23 11:51:25 -0700   27636 execution          WARNING  [Flex in line 1 (index starts from 0)] stderr> [2024-04-23 11:51:25,280][flowinvoker][INFO] - Getting connections from pf client with provider from args: local...\n",
      "2024-04-23 11:51:25 -0700   25448 execution          WARNING  [Flex in line 2 (index starts from 0)] stderr> [2024-04-23 11:51:25,285][flowinvoker][INFO] - Promptflow get connections successfully. keys: dict_keys([])\n",
      "2024-04-23 11:51:25 -0700   25448 execution          WARNING  [Flex in line 2 (index starts from 0)] stderr> [2024-04-23 11:51:25,286][flowinvoker][INFO] - Promptflow executor starts initializing...\n",
      "s: dict_keys([])\n",
      "2024-04-23 11:51:25 -0700   27636 execution          WARNING  [Flex in line 1 (index starts from 0)] stderr> [2024-04-23 11:51:25,286][flowinvoker][INFO] - Promptflow executor starts initializing...\n",
      "2024-04-23 11:51:25 -0700   25448 execution          WARNING  [Flex in line 2 (index starts from 0)] stderr> [2024-04-23 11:51:25,326][flowinvoker][INFO] - Promptflow executor initiated successfully.\n",
      "2024-04-23 11:51:25 -0700   27636 execution          WARNING  [Flex in line 1 (index starts from 0)] stderr> [2024-04-23 11:51:25,326][flowinvoker][INFO] - Promptflow executor initiated successfully.\n",
      "2024-04-23 11:51:25 -0700   25448 execution          WARNING  [Flex in line 2 (index starts from 0)] stderr> [2024-04-23 11:51:25,327][flowinvoker][INFO] - Validating flow input with data {'answer': 'Paris is the capital of France.', 'ground_truth': 'Paris is the capital of France.'}\n",
      "2024-04-23 11:51:25 -0700   25448 execution          WARNING  [Flex in line 2 (index starts from 0)] stderr> [2024-04-23 11:51:25,327][flowinvoker][INFO] - Execute flow with data {'answer': 'Paris is the capital of France.', 'ground_truth': 'Paris is the capital of France.'}\n",
      "2024-04-23 11:51:25 -0700   27636 execution          WARNING  [Flex in line 1 (index starts from 0)] stderr> [2024-04-23 11:51:25,327][flowinvoker][INFO] - Validating flow input with data {'answer': \"There are a few ways to log models in Azure Machine Learning. \\n\\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\\n\\n```python\\nfrom azureml.core import Model\\n\\nmodel_path = './outputs/my_model.pkl'\\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\\n```\\n\\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \\n\\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\\n\\n```python\\nfrom sklearn.linear_model import LogisticRegression\\nfrom azureml.core.run import Run\\n\\n# Initialize a run object\\nrun = Run.get_context()\\n\\n# Train your model\\nX_train, y_train = ...\\nclf = LogisticRegression().fit(X_train, y_train)\\n\\n# Save the model to the Run object's outputs directory\\nmodel_path = 'outputs/model.pkl'\\njoblib.dump(value=clf, filename=model_path)\\n\\n# Log the model as a run artifact\\nrun.upload_file(name=model_path, path_or_stream=model_path)\\n```\\n\\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.\", 'ground_truth': 'Paris is the capital of France.'}\n",
      "2024-04-23 11:51:25 -0700   27636 execution          WARNING  [Flex in line 1 (index starts from 0)] stderr> [2024-04-23 11:51:25,328][flowinvoker][INFO] - Execute flow with data {'answer': \"There are a few ways to log models in Azure Machine Learning. \\n\\nOne way is to use the `register_model()` method of the `Run` object. The `register_model()` method logs a model file in the Azure Machine Learning service workspace and makes it available for deployment. Here's an example:\\n\\n```python\\nfrom azureml.core import Model\\n\\nmodel_path = './outputs/my_model.pkl'\\nmodel = Model.register(workspace=ws, model_path=model_path, model_name='my_model')\\n```\\n\\nThis code registers the model file located at `model_path` to the Azure Machine Learning service workspace with the name `my_model`. \\n\\nAnother way to log a model is to save it as an output of a `Run`. If your model generation code is part of a script or Jupyter notebook that runs as an Azure Machine Learning experiment, you can save the model file as an output of the `Run` object. Here's an example:\\n\\n```python\\nfrom sklearn.linear_model import LogisticRegression\\nfrom azureml.core.run import Run\\n\\n# Initialize a run object\\nrun = Run.get_context()\\n\\n# Train your model\\nX_train, y_train = ...\\nclf = LogisticRegression().fit(X_train, y_train)\\n\\n# Save the model to the Run object's outputs directory\\nmodel_path = 'outputs/model.pkl'\\njoblib.dump(value=clf, filename=model_path)\\n\\n# Log the model as a run artifact\\nrun.upload_file(name=model_path, path_or_stream=model_path)\\n```\\n\\nIn this code, `Run.get_context()` retrieves the current run context object, which you can use to track metadata and metrics for the run. After training your model, you can use `joblib.dump()` to save the model to a file, and then log the file as an artifact of the run using `run.upload_file()`.\", 'ground_truth': 'Paris is the capital of France.'}\n",
      "2024-04-23 11:51:25 -0700   40292 execution.bulk     INFO     Process name(SpawnProcess-3)-Process id(25448)-Line number(2) completed.\n",
      "2024-04-23 11:51:25 -0700   40292 execution.bulk     INFO     Process name(SpawnProcess-4)-Process id(27636)-Line number(1) completed.\n",
      "2024-04-23 11:51:25 -0700   40292 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-04-23 11:51:25 -0700   40292 execution.bulk     INFO     Average execution time for completed lines: 1.01 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-04-23 11:51:25 -0700   40292 execution.bulk     INFO     The thread monitoring the process [17416-SpawnProcess-2] will be terminated.\n",
      "2024-04-23 11:51:25 -0700   40292 execution.bulk     INFO     The thread monitoring the process [25448-SpawnProcess-3] will be terminated.\n",
      "2024-04-23 11:51:25 -0700   40292 execution.bulk     INFO     The thread monitoring the process [27636-SpawnProcess-4] will be terminated.\n",
      "2024-04-23 11:51:25 -0700   17416 execution.bulk     INFO     The process [17416] has received a terminate signal.\n",
      "2024-04-23 11:51:25 -0700   25448 execution.bulk     INFO     The process [25448] has received a terminate signal.\n",
      "2024-04-23 11:51:25 -0700   27636 execution.bulk     INFO     The process [27636] has received a terminate signal.\n",
      "2024-04-23 11:51:28 -0700   40292 execution.bulk     INFO     Process 25448 terminated.\n",
      "2024-04-23 11:51:28 -0700   40292 execution.bulk     INFO     Process 17416 terminated.\n",
      "2024-04-23 11:51:28 -0700   40292 execution.bulk     INFO     Process 27636 terminated.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"test_76ad6428-01a2-11ef-b9d7-00224877b1ea\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-04-23 11:51:15.925433\"\n",
      "Duration: \"0:00:12.775181\"\n",
      "Output path: \"C:\\Users\\anksing\\.promptflow\\.runs\\test_76ad6428-01a2-11ef-b9d7-00224877b1ea\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf = PFClient()\n",
    "run = pf.run(\n",
    "    flow='f1_score',\n",
    "    data='data.jsonl',\n",
    "    name=f'test_{uuid.uuid1()}',\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fbd6bf-6977-457b-aa55-f6dad9ec1f73",
   "metadata": {},
   "source": [
    "Now let us test the flow with the simple dataset, consisting of one ground true and one actual sentense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb73f698-7cab-4b30-8947-411c2060560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    \"ground_truth\": [\"January is the coldest winter month.\"],\n",
    "    \"answer\": [\"June is the coldest summer month.\"]\n",
    "})\n",
    "in_file = 'sample_data.jsonl'\n",
    "data.to_json('sample_data.jsonl', orient='records', lines=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3589c8-0df8-489f-b5a8-beb5ae2aec6a",
   "metadata": {},
   "source": [
    "Load the evaluator in a FLEX format and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "841d6109-23b9-45c5-b709-b588f932f29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces from local: http://localhost:64667/v1.0/ui/traces/?#collection=f1_score\n",
      "Flow outputs: {'f1_score': 0.6}\n"
     ]
    }
   ],
   "source": [
    "flow_result = pf.test(flow='f1_score', inputs='sample_data.jsonl')\n",
    "print(f\"Flow outputs: {flow_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94aab5-73f8-4c7d-a7e0-a92853db0198",
   "metadata": {},
   "source": [
    "Now we have all the tools to upload our model to Azure\n",
    "### Uploading data to Azure\n",
    "First we will need to authenticate to azure. For this purpose we will use the the configuration file of the net structure.\n",
    "```json\n",
    "{\n",
    "    \"resource_group_name\": \"resource-group-name\",\n",
    "    \"workspace_name\": \"ws-name\",\n",
    "    \"subscription_id\": \"subscription-uuid\",\n",
    "    \"registry_name\": \"registry-name\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7188cb7d-d7c1-460f-9f3e-91546d8b8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    configuration = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e6627-67d8-43c3-b491-e3a8802197b2",
   "metadata": {},
   "source": [
    "#### Uploading to the workspace\n",
    "In this scenario we will not need the `registry_name` in our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36338014-05f9-4f37-9fb0-726bb1c137b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ws = configuration.copy()\n",
    "del config_ws[\"registry_name\"]\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    **config_ws,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4a912-047f-4614-85a7-cfff86874303",
   "metadata": {},
   "source": [
    "We will use the evaluator operations API to upload our model to workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a4edb19-f0b8-498c-908d-c7e23ba7b30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model({'job_name': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'F1Score-Evaluator', 'description': 'Measures the ratio of the number of shared words between the model generation and the ground truth answers.', 'tags': {}, 'properties': {'is-promptflow': 'true', 'is-evaluator': 'true'}, 'print_as_yaml': False, 'id': '/subscriptions/b17253fa-f327-42d6-9686-f3e553e24763/resourceGroups/anksing-vanilla-eval/providers/Microsoft.MachineLearningServices/workspaces/anksing-vanilla-eval/models/F1Score-Evaluator/versions/1', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\anksing\\\\Repos\\\\AML\\\\promptflow\\\\src\\\\promptflow-evals\\\\samples', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x00000209025CD2B0>, 'serialize': <msrest.serialization.Serializer object at 0x00000209025CDA90>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/b17253fa-f327-42d6-9686-f3e553e24763/resourceGroups/anksing-vanilla-eval/workspaces/anksing-vanilla-eval/datastores/workspaceblobstore/paths/LocalUpload/2d2a340d57888049d2a0ab4f58c25eb5/f1_score', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model', 'stage': 'Development'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = Model(\n",
    "    path=\"f1_score\",\n",
    "    name='F1Score-Evaluator',\n",
    "    description=\"Measures the ratio of the number of shared words between the model generation and the ground truth answers.\",\n",
    ")\n",
    "ml_client.evaluators.create_or_update(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6423eb6-415c-463c-839d-da0cf70bf245",
   "metadata": {},
   "source": [
    "Now we will retrieve model and check that it is functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e18dd491-ee43-4dda-8a5b-d5317f8cb64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading the model LocalUpload/2d2a340d57888049d2a0ab4f58c25eb5/f1_score at f1_score_downloaded\\F1Score-Evaluator\\f1_score\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_client.evaluators.download('F1Score-Evaluator', version='1', download_path='f1_score_downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a15468f0-681a-49fe-a883-0da44f68293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces from local: http://localhost:64667/v1.0/ui/traces/?#collection=f1_score\n",
      "Flow outputs: {'f1_score': 0.015384615384615384}\n"
     ]
    }
   ],
   "source": [
    "flow_result = pf.test(flow=os.path.join('f1_score_downloaded', 'F1Score-Evaluator', 'f1_score'), inputs='data.jsonl')\n",
    "print(f\"Flow outputs: {flow_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0708cd5-66e7-46f2-a8d0-b41e82278a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('f1_score_downloaded')\n",
    "assert not os.path.isdir('f1_score_downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a4588-a0b7-4bd9-adc6-6595084da3b7",
   "metadata": {},
   "source": [
    "#### Uploading to the registry\n",
    "In this scenario we will not need the `workspace_name` in our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75b57845-77f0-4a2e-9b2f-ccb3fb825da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_reg = configuration.copy()\n",
    "del config_reg[\"workspace_name\"]\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    **config_reg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0f024-c6e5-4e51-aaaa-021eaa4c14c4",
   "metadata": {},
   "source": [
    "We are creating new eval here, because create_or_update changes the model inplace, adding non existing link to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b488f4ff-b97f-43e6-82ab-a78a2e9e2da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:msrest.serialization:Subtype value SAS has no mapping, use base class DataReferenceCredentialDto.\n",
      "\u001b[32mUploading f1_score (0.01 MBs):  78%|#######8  | 9289/11837 [00:03<00:00, 2561.80it/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: amldevos5mguse01.blob.core.windows.net. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: amldevos5mguse01.blob.core.windows.net. Connection pool size: 10\n",
      "\u001b[32mUploading f1_score (0.01 MBs): 100%|##########| 11837/11837 [00:03<00:00, 3089.58it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model({'job_name': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'F1Score-Evaluator', 'description': 'Measures the ratio of the number of shared words between the model generation and the ground truth answers.', 'tags': {}, 'properties': {'show-artifact': 'true', 'is-promptflow': 'true', 'is-evaluator': 'true'}, 'print_as_yaml': False, 'id': 'azureml://registries/azureml-dev/models/F1Score-Evaluator/versions/2', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\anksing\\\\Repos\\\\AML\\\\promptflow\\\\src\\\\promptflow-evals\\\\samples', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000020902407C10>, 'serialize': <msrest.serialization.Serializer object at 0x0000020954791310>, 'version': '2', 'latest_version': None, 'path': 'https://amldevos5mguse01.blob.core.windows.net/azureml-de-c8c82dfc-230d-5228-a79b-1e6ef394d7be/f1_score', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model', 'stage': None})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = Model(\n",
    "    path=\"f1_score\",\n",
    "    name='F1Score-Evaluator',\n",
    "    description=\"Measures the ratio of the number of shared words between the model generation and the ground truth answers.\",\n",
    "    properties={\"show-artifact\": \"true\"}\n",
    ")\n",
    "ml_client.evaluators.create_or_update(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6603c7d-7eaf-454e-a59a-2dd01fd3afc6",
   "metadata": {},
   "source": [
    "Now we will perform the same sanity check, we have done for the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccfc3d3b-db1a-4a5a-97c5-4ff701051695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:msrest.serialization:Subtype value SAS has no mapping, use base class DataReferenceCredentialDto.\n",
      "Downloading the model f1_score at f1_score_downloaded\\F1Score-Evaluator\\f1_score\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces from local: http://localhost:64667/v1.0/ui/traces/?#collection=f1_score\n",
      "Flow outputs: {'f1_score': 0.015384615384615384}\n"
     ]
    }
   ],
   "source": [
    "ml_client.evaluators.download('F1Score-Evaluator', version='1', download_path='f1_score_downloaded')\n",
    "flow_result = pf.test(flow=os.path.join('f1_score_downloaded', 'F1Score-Evaluator', 'f1_score'), inputs='data.jsonl')\n",
    "print(f\"Flow outputs: {flow_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167df44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import Flow\n",
    "\n",
    "# This is not working but it should. Will uncomment once PF team provides a fix.\n",
    "# f = Flow.load('f1_score_downloaded/F1Score-Evaluator/f1_score')\n",
    "# f(question='What is the capital of France?', answer='Paris', ground_truth='Paris is the capital of France.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5dc1c9-9c7a-40f7-9b23-9b30bedd5dd4",
   "metadata": {},
   "source": [
    "Finally, we will do the cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a90b52e8-1f46-454d-a6a5-2ad725e927fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('f1_score_downloaded')\n",
    "assert not os.path.isdir('f1_score_downloaded')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
