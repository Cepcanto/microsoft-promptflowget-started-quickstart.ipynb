{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Flex flow as evaluator\n",
    "In this notebook we will demonstrate saving evaluator to the container registry, and using saved flex flow evaluators.\n",
    "\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e21ceb-1e58-4ba7-884a-5e103aea7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity._credentials.default import AzureCliCredential\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "from promptflow.client import PFClient, load_flow\n",
    "from promptflow.core import Flow\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "from promptflow.evals.evaluators import F1ScoreEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "## Save and uploading of the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PFClient()\n",
    "pf.flows.save(F1ScoreEvaluator, path=\"./f1_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94aab5-73f8-4c7d-a7e0-a92853db0198",
   "metadata": {},
   "source": [
    "### Authenticate to azure\n",
    "First we will need to authenticate to azure. For this purpose we will use the the configuration file of the net structure.\n",
    "```json\r\n",
    "{\r\n",
    "    \"resource_group_name\": \"resource-group-namee\",\r\n",
    "    \"subscription_id\": \"subscription-uuid\",\r\n",
    "    \"registry_name\": \"registry-name\"\r\n",
    "}\r\n",
    "```\r\n",
    "**Note:** If the `registry_name` will be replaced by `workspace_name`, the evaluator will be saved to Azure ML Workspace instead of registry.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188cb7d-d7c1-460f-9f3e-91546d8b8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\") as f:\n",
    "    configuration = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e6627-67d8-43c3-b491-e3a8802197b2",
   "metadata": {},
   "source": [
    "Upload the saved evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36338014-05f9-4f37-9fb0-726bb1c137b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = AzureCliCredential()\n",
    "ml_client = MLClient(credential=credential, **configuration)\n",
    "\n",
    "eval = Model(\n",
    "    path=\"f1_score\",\n",
    "    name=\"f1_score_uploaded\",\n",
    "    description=\"F1 score evaluator.\",\n",
    ")\n",
    "ml_client.evaluators.create_or_update(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6423eb6-415c-463c-839d-da0cf70bf245",
   "metadata": {},
   "source": [
    "Download the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dd491-ee43-4dda-8a5b-d5317f8cb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.evaluators.download(\"f1_score_uploaded\", version=\"1\", download_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f123961-7568-4f87-8f41-f0c6f247aa3c",
   "metadata": {},
   "source": [
    "## Data\n",
    "Now let us generate some data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c6948-33e8-48cd-90ac-759b46ba33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\"What is the name of Alexander the Great horse?\", \"What is Sun?\"],\n",
    "        \"answer\": [\"Binkey\", \"The star.\"],\n",
    "        \"ground_truth\": [\"Bucephalus\", \"The star.\"],\n",
    "    }\n",
    ")\n",
    "data_raw.to_json(\"data.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5477231-b5f3-4f0e-b5ca-9f2f16c6eb48",
   "metadata": {},
   "source": [
    "## Test the downloaded evaluator\n",
    "We will load evaluator in two ways:\n",
    "- Using `Flow.load` method;\n",
    "- Using `promptflow.client.load_flow` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfb7ef-4eef-43cf-a59d-eafb3547b95c",
   "metadata": {},
   "source": [
    "### Run using `Flow.load`\n",
    "<font color='red'> This section is commented out, because it used to fail </font><br>\n",
    "First we will try using flow as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9abe27-3e44-42cd-a7a9-049824d10a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = os.path.join(\"f1_score_uploaded\", \"f1_score\")\n",
    "# downloaded_flow = Flow.load(flow_path)\n",
    "# downloaded_flow(**data_raw[:1].T[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed993e52-349a-4221-af49-31397b9caf32",
   "metadata": {},
   "source": [
    "Now we will run the evaluation using `evaluate` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15468f0-681a-49fe-a883-0da44f68293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = evaluate(\n",
    "#     data='data.jsonl',\n",
    "#     evaluators = {'prompty_eval': downloaded_flow}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3228c-425b-45d3-a76c-1c9abc9e53b7",
   "metadata": {},
   "source": [
    "### Run using `promptflow.client.load_flow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1b381-dd17-4edb-bc1f-fc3804a6e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_flow = load_flow(flow_path)\n",
    "# FlexFlow can not be called as a function.\n",
    "# downloaded_flow(**data_raw[:1].T[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8740be0-11af-49ff-b28d-2e4e56a82485",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(data=\"data.jsonl\", evaluators={\"prompty_eval\": downloaded_flow})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0569608-5db9-4865-8b5f-6c62fd2a3696",
   "metadata": {},
   "source": [
    "View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c984dec-ca03-4412-9705-0e3f4954bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{results['metrics']=}\")\n",
    "pd.DataFrame(results[\"rows\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
