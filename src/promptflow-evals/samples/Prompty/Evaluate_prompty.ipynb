{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0908ff40-686b-49e1-a98f-98ac85584975",
   "metadata": {},
   "source": [
    "# Evaluators with Prompty\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48f33a-3147-4fe4-b09b-01fc8194ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from promptflow.client import PFClient\n",
    "from promptflow.client import load_flow\n",
    "from promptflow.core import Flow\n",
    "from promptflow.evals.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff066dd-69b2-4865-8a7d-4c9700397065",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232f215-1e41-4996-9484-c30f0559e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(\"eval-basic\"), \"Please download the eval-basic folder into the directory with this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49623494-3646-46b4-a888-25bf1a96c0d5",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f97e54-5958-4c67-a7db-33f395e58b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = \"eval-basic\"\n",
    "data_raw = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\"What is Sun?\", \"What is the name of Alexander the Great horse?\"],\n",
    "        \"answer\": [\"The star.\", \"Binkey\"],\n",
    "        \"ground_truth\": [\"The star.\", \"Bucephalus\"],\n",
    "    }\n",
    ")\n",
    "data_raw.to_json(\"data.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f09907-6dad-4fa9-922b-6a1e80200bf1",
   "metadata": {},
   "source": [
    "## Run prompty through the evaluators.\n",
    "Fist we will need to set the authentication variables.\n",
    "Please create the deployment of gpt-3.5 or gpt-4 in Azure Open AI and create the json file with the next contents.\n",
    "\n",
    "```json\n",
    "{\r\n",
    "    \"AZURE_OPENAI_API_KEY\": super_secret_ke4\",\r\n",
    "    \"AZURE_OPENAI_ENDPOINT\": \"https://deployment_name.openai.azure.com/\"\r\n",
    "}\n",
    "```\n",
    "In the `eval-basic\\eval.prompty` file make sure that `azure_deployment` is set to the value, equal to the name of your deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1da9c6-d40c-4f8e-8e74-afc51d2ea388",
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_data = json.load(open(\"openai_auth.json\"))\n",
    "assert \"AZURE_OPENAI_API_KEY\" in secure_data\n",
    "assert \"AZURE_OPENAI_ENDPOINT\" in secure_data\n",
    "\n",
    "for k, v in secure_data.items():\n",
    "    os.environ[k] = v\n",
    "\n",
    "flow_path = os.path.join(\"eval-basic\", \"eval.prompty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32dae67-3f3c-44e4-9a3d-34345777e5f0",
   "metadata": {},
   "source": [
    "Prompty evaluator may be loaded in two ways:\n",
    "- Using `Flow.load` method;\n",
    "- Using `promptflow.client.load_flow` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01348-a87e-477c-b905-0d13f4f4a41c",
   "metadata": {},
   "source": [
    "### Run using `Flow.load`\n",
    "<font color='red'> This section is commented out, because it used to fail </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c70f5-54c1-4a4a-9c63-cc5adad48f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompty_flow = Flow.load(flow_path)\n",
    "# results = evaluate(\n",
    "#     data='data.jsonl',\n",
    "#     evaluators = {'prompty_eval': prompty_flow}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59eda90-1134-42de-9e98-f094b15ba269",
   "metadata": {},
   "source": [
    "View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10516496-4f85-48c0-b8b6-48415457b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{results['metrics']=}\")\n",
    "# pd.DataFrame(results[\"rows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9843ed5-c8a9-435d-9db2-9b0f5a36ed8f",
   "metadata": {},
   "source": [
    "### Run using `promptflow.client.load_flow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c286588-be4c-4276-8b9e-fa4790e55979",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompty_flow = load_flow(flow_path)\n",
    "results = evaluate(data=\"data.jsonl\", evaluators={\"prompty_eval\": prompty_flow})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbfa2a-fdfa-43df-b3b7-a8c313031410",
   "metadata": {},
   "source": [
    "View resuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c9323-d17d-4917-b65a-591b71faa381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{results['metrics']=}\")\n",
    "pd.DataFrame(results[\"rows\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
