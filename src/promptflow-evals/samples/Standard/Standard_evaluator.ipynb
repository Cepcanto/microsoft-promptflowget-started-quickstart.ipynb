{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e932e4c-5d55-461e-a313-3a087d8983b5",
   "metadata": {},
   "source": [
    "# Standard evaluators and target functions.\n",
    "In this notebook we will demonstrate how to use the target functions with the standard evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fd898-7ef2-4d89-872e-da9e426aaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "from promptflow.evals.evaluators import F1ScoreEvaluator, GroundednessEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352b517-70b0-4f4f-a3ad-bc99eae67b2e",
   "metadata": {},
   "source": [
    "## Target function\n",
    "Target function is a callable, which should be applied to data before calculating metrics by the evaluator. In this example, we will apply a simple function, answering questions and then apply F1 and Groundedness evaluator to outputs. Due to current limitations function needs to be saved as a separate python file and then imported. The function will generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9bb466-324f-42ce-924a-56e1bc52471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_response(question):\n",
    "    data = {\"sun\": \"The star.\", \"alexander the great\": \"Binkey\"}\n",
    "    for k, v in data.items():\n",
    "        if k in question.lower():\n",
    "            return {\"answer\": v}\n",
    "    return {\"answer\": \"I do not know.\"}\n",
    "\n",
    "\n",
    "lines = inspect.getsource(fun_response)\n",
    "with open(\"get_response.py\", \"w\") as fp:\n",
    "    fp.write(lines)\n",
    "\n",
    "from get_response import fun_response as target_fun\n",
    "\n",
    "target_fun(\"What is Sun?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641385d-12d8-4ec2-b477-3b1aeed6e86c",
   "metadata": {},
   "source": [
    "## Data\n",
    "Because we want to generate the groundedness evaluation, we will need the data context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e777f-3889-49c2-bc53-25488dade7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\"What is the name of Alexander the Great horse?\", \"What is Sun?\"],\n",
    "        \"context\": [\n",
    "            \"Bucephalus or Bucephalas was the horse of Alexander the Great, and one of the most famous horses of classical antiquity.\",\n",
    "            \"The Sun is the star at the center of the Solar System.\",\n",
    "        ],\n",
    "        \"ground_truth\": [\"Bucephalus\", \"The star.\"],\n",
    "    }\n",
    ")\n",
    "data_raw.to_json(\"data.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98713968-1307-4045-87cc-fd4ac1f70009",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "To use groundednessEvaluator, we will need to deploy the model.\n",
    "Please create the deployment of gpt-3.5 or gpt-4 in Azure Open AI and create the `openai_auth.json` file with the next contents.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"AZURE_OPENAI_API_KEY\": super_secret_key\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\": \"https:deployment_namei2.openai.azure.com/\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f76d0c-2d44-44e0-8296-110477c7e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_data = json.load(open(\"openai_auth.json\"))\n",
    "\n",
    "configuration = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=secure_data[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=secure_data[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_deployment=\"gpt-35-turbo-1106\",  # Please use the name of a model you have deployed.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b63dd-031d-469d-8232-84affd517f0f",
   "metadata": {},
   "source": [
    "## Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1dd39-f0a3-4392-bf99-14eecda3e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    target=target_fun,\n",
    "    evaluators={\n",
    "        \"f1_evaluator\": F1ScoreEvaluator(),\n",
    "        \"groundedess_evaluator\": GroundednessEvaluator(model_config=configuration),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d4569-4e1b-4b44-92ed-9063eccb68ae",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec6443-14a7-410e-9fc2-1411461dc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{results['metrics']=}\")\n",
    "pd.DataFrame(results[\"rows\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
