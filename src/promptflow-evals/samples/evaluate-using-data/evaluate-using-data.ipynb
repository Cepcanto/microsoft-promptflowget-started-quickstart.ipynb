{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Evaluation with Data\n",
    "In this notebook, we introduce built-in evaluators and guide you through creating your own custom evaluators. We'll cover both code-based and prompt-based custom evaluators. Finally, we'll demonstrate how to use the `evaluate` API to assess data using these evaluators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcb5a0",
   "metadata": {},
   "source": [
    "## 1. Built-in Evaluators\n",
    "\n",
    "\n",
    "| Category       | Namespace                                        | Evaluator Class           |\n",
    "|----------------|--------------------------------------------------|---------------------------|\n",
    "| Quality        | promptflow.evals.evaluators                      | GroundednessEvaluator     |\n",
    "|                |                                                  | RelevanceEvaluator        |\n",
    "|                |                                                  | CoherenceEvaluator        |\n",
    "|                |                                                  | FluencyEvaluator          |\n",
    "|                |                                                  | SimilarityEvaluator       |\n",
    "|                |                                                  | F1ScoreEvaluator          |\n",
    "| Content Safety | promptflow.evals.evaluators.content_safety       | ViolenceEvaluator         |\n",
    "|                |                                                  | SexualEvaluator           |\n",
    "|                |                                                  | SelfHarmEvaluator         |\n",
    "|                |                                                  | HateUnfairnessEvaluator   |\n",
    "| Composite      | promptflow.evals.evaluators                      | QAEvaluator               |\n",
    "|                |                                                  | ChatEvaluator             |\n",
    "|                |                                                  | ContentSafetyEvaluator    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f72d3d",
   "metadata": {},
   "source": [
    "### 1.1 Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_deployment=\"gpt-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "\n",
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    answer=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    question=\"Which tent is the most waterproof?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67844e07-57bb-415f-b1af-606b5a67aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdd503",
   "metadata": {},
   "source": [
    "### 1.2 Content Safety Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c304e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Project Scope\n",
    "project_scope = {\n",
    "    \"subscription_id\": \"e0fd569c-e34a-4249-8c24-e8d723c7f054\",\n",
    "    \"resource_group_name\": \"resource-group\",\n",
    "    \"project_name\": \"project-name\",\n",
    "}\n",
    "project_scope = {\n",
    "    \"subscription_id\": \"b17253fa-f327-42d6-9686-f3e553e24763\",\n",
    "    \"resource_group_name\": \"promptflow-evals-ci\",\n",
    "    \"project_name\": \"pf-evals-ws\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators.content_safety import ViolenceEvaluator\n",
    "\n",
    "# Initialzing Violence Evaluator\n",
    "violence_eval = ViolenceEvaluator(project_scope)\n",
    "\n",
    "# Running Violence Evaluator on single input row\n",
    "violence_score = violence_eval(question=\"What is the capital of France?\", answer=\"Paris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18aa4a-78fb-45b3-9d1b-8babcd9ab931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(violence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581540d",
   "metadata": {},
   "source": [
    "### 1.3 Composite Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2267d6",
   "metadata": {},
   "source": [
    "#### 1.3.1 QA Evaluator\n",
    "QAEvaluator is a composite evaluator that combines all quality evaluators, including GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator and F1ScoreEvaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import QAEvaluator\n",
    "\n",
    "# Initialzing QA Evaluator\n",
    "qa_eval = QAEvaluator(model_config=model_config)\n",
    "\n",
    "# Running QA Evaluator on single input row\n",
    "score = qa_eval(\n",
    "    question=\"Tokyo is the capital of which country?\",\n",
    "    answer=\"Japan\",\n",
    "    context=\"Tokyo is the capital of Japan.\",\n",
    "    ground_truth=\"Japan\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ec352-fad5-48e4-a95a-26e70fe17a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3667fd",
   "metadata": {},
   "source": [
    "#### 1.3.2 Chat Evaluator\n",
    "Chat Evaluator is another composite evaluator that utilizes quality evaluators such as the CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator, and RelevanceEvaluator to assess chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35deb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import ChatEvaluator\n",
    "\n",
    "# Initialize Chat Evaluator\n",
    "chat_eval = ChatEvaluator(model_config=model_config)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the value of 2 + 2?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"2 + 2 = 4\",\n",
    "        \"context\": {\n",
    "            \"citations\": [{\"id\": \"doc.md\", \"content\": \"Information about additions: 1 + 2 = 3, 2 + 2 = 4\"}]\n",
    "        },\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The capital of Japan is Tokyo.\",\n",
    "        \"context\": {\n",
    "            \"citations\": [\n",
    "                {\n",
    "                    \"id\": \"doc.md\",\n",
    "                    \"content\": \"Tokyo is Japan's capital, known for its blend of traditional culture and \"\n",
    "                    \"technological advancements.\",\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Running Chat Evaluator on chat conversation\n",
    "score = chat_eval(conversation=conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977efe74-5d07-4d0f-b25f-33b35da2c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8e0ae",
   "metadata": {},
   "source": [
    "## 2. Custom Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac30330",
   "metadata": {},
   "source": [
    "### 2.1 Define a Code based Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf976514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_length(input, **kwargs):\n",
    "    return {\"value\": len(input)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e5b20",
   "metadata": {},
   "source": [
    "### 2.2 Define a Prompt based Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a664d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apology.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "## 3. Using Evaluate API to evaluate with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"data.jsonl\"\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    evaluators={\n",
    "       # \"answer_length\": answer_length,\n",
    "        \"relevance\": relevance_eval,\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"answer_length\": {\"input\": \"${data.answer}\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dadc4b-495f-4a46-ad15-d4c8a910b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, JSON\n",
    "\n",
    "display(JSON(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8a2b6-faeb-4fae-a3b6-31b6fb0db63f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
