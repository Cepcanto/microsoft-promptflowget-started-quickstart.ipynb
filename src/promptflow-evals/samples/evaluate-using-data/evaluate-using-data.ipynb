{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Evaluation with Data\n",
    "In this notebook, we introduce built-in evaluators and guide you through creating your own custom evaluators. We'll cover both code-based and prompt-based custom evaluators. Finally, we'll demonstrate how to use the `evaluate` API to assess data using these evaluators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcb5a0",
   "metadata": {},
   "source": [
    "## 1. Built-in Evaluators\n",
    "\n",
    "The table below lists all the built-in evaluators we support. In the following sections, we will select a few of these evaluators to demonstrate how to use them.\n",
    "\n",
    "\n",
    "| Category       | Namespace                                        | Evaluator Class           |\n",
    "|----------------|--------------------------------------------------|---------------------------|\n",
    "| Quality        | promptflow.evals.evaluators                      | GroundednessEvaluator     |\n",
    "|                |                                                  | RelevanceEvaluator        |\n",
    "|                |                                                  | CoherenceEvaluator        |\n",
    "|                |                                                  | FluencyEvaluator          |\n",
    "|                |                                                  | SimilarityEvaluator       |\n",
    "|                |                                                  | F1ScoreEvaluator          |\n",
    "| Content Safety | promptflow.evals.evaluators.content_safety       | ViolenceEvaluator         |\n",
    "|                |                                                  | SexualEvaluator           |\n",
    "|                |                                                  | SelfHarmEvaluator         |\n",
    "|                |                                                  | HateUnfairnessEvaluator   |\n",
    "| Composite      | promptflow.evals.evaluators                      | QAEvaluator               |\n",
    "|                |                                                  | ChatEvaluator             |\n",
    "|                |                                                  | ContentSafetyEvaluator    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f72d3d",
   "metadata": {},
   "source": [
    "### 1.1 Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf075450-1835-4b15-8002-76ae249caab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    answer=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    question=\"Which tent is the most waterproof?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67844e07-57bb-415f-b1af-606b5a67aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdd503",
   "metadata": {},
   "source": [
    "### 1.2 Content Safety Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ab346",
   "metadata": {},
   "source": [
    "Unlike quality evaluators, which are prompt-based, content safety evaluators are supported by the RAI service. To use these evaluators, you need to provide the subscription ID, resource group, and project name of your Azure AI project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c304e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Project Scope\n",
    "project_scope = {\n",
    "    \"subscription_id\": \"e0fd569c-e34a-4249-8c24-e8d723c7f054\",\n",
    "    \"resource_group_name\": \"resource-group\",\n",
    "    \"project_name\": \"project-name\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators.content_safety import ViolenceEvaluator\n",
    "\n",
    "# Initialzing Violence Evaluator\n",
    "violence_eval = ViolenceEvaluator(project_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be423024-4725-4f19-a5e1-191a01066860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Violence Evaluator on single input row\n",
    "violence_score = violence_eval(question=\"What is the capital of France?\", answer=\"Paris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18aa4a-78fb-45b3-9d1b-8babcd9ab931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(violence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581540d",
   "metadata": {},
   "source": [
    "### 1.3 Composite Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2267d6",
   "metadata": {},
   "source": [
    "#### 1.3.1 QA Evaluator\n",
    "The QAEvaluator is a composite evaluator built on top of individual quality evaluators. It provides a combined result from these underlying evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import QAEvaluator\n",
    "\n",
    "# Initialzing QA Evaluator\n",
    "qa_eval = QAEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d7fc2-ada8-43b8-a9b8-c3b592688ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running QA Evaluator on single input row\n",
    "score = qa_eval(\n",
    "    question=\"Tokyo is the capital of which country?\",\n",
    "    answer=\"Japan\",\n",
    "    context=\"Tokyo is the capital of Japan.\",\n",
    "    ground_truth=\"Japan\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3e33b-f6cb-45f8-a4f7-bef34dc3e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3667fd",
   "metadata": {},
   "source": [
    "#### 1.3.2 Chat Evaluator\n",
    "The Chat Evaluator is another composite evaluator that utilizes quality evaluators, similar to the QAEvaluator. The key difference is that the Chat Evaluator is specifically designed for evaluating chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35deb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import ChatEvaluator\n",
    "\n",
    "# Initialize Chat Evaluator\n",
    "chat_eval = ChatEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb0e3d-9685-4b44-94f7-bab5f769b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the value of 2 + 2?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"2 + 2 = 4\",\n",
    "        \"context\": {\n",
    "            \"citations\": [{\"id\": \"doc.md\", \"content\": \"Information about additions: 1 + 2 = 3, 2 + 2 = 4\"}]\n",
    "        },\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The capital of Japan is Tokyo.\",\n",
    "        \"context\": {\n",
    "            \"citations\": [\n",
    "                {\n",
    "                    \"id\": \"doc.md\",\n",
    "                    \"content\": \"Tokyo is Japan's capital, known for its blend of traditional culture and \"\n",
    "                    \"technological advancements.\",\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Running Chat Evaluator on chat conversation\n",
    "score = chat_eval(conversation=conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977efe74-5d07-4d0f-b25f-33b35da2c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8e0ae",
   "metadata": {},
   "source": [
    "## 2. Custom Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f07695",
   "metadata": {},
   "source": [
    "After gaining a basic understanding of how the built-in evaluators work, let's explore how to create your own evaluator. In this section, we will discuss creating both a code-based custom evaluator and a prompt-based custom evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac30330",
   "metadata": {},
   "source": [
    "### 2.1 Define a Code based Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10faf0",
   "metadata": {},
   "source": [
    "A code-based evaluator can be as simple as a Python function. In the example below, we will show you a simple evaluator that calculates the length of an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf976514",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"custom/answer_length.py\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86568cf3-15ff-43b2-b80d-bb3671f557ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.answer_length import answer_length\n",
    "print(answer_length(\"some answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271d78c-ade7-4eb7-85e1-cba9e5c4ad53",
   "metadata": {},
   "source": [
    "### 2.2 Define a Prompty based Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fedd4",
   "metadata": {},
   "source": [
    "Prompty is a file with .prompty extension for developing prompt template. The prompty asset is a markdown file with a modified front matter. The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"custom/apology.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344d8ff-7ef8-4042-ac70-e3f3bc4db5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import load_flow\n",
    "\n",
    "# load apology evaluatorfrom prompty\n",
    "apology_eval = load_flow(source=\"custom/apology.prompty\", model={\"configuration\": model_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = apology_eval(\n",
    "    question=\"What is the capital of France?\", answer=\"Paris\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9019d37-a323-4c98-9c55-a7acb72d2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "## 3. Using Evaluate API to evaluate with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe3fa2",
   "metadata": {},
   "source": [
    "In previous sections, we walked you through how to use built-in evaluators to evaluate a single row and how to define your own custom evaluators. Now, we will show you how to use these evaluators with the powerful `evaluate` API to assess an entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2265e",
   "metadata": {},
   "source": [
    "First, let's take a peek at what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"data.jsonl\"\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcfc57",
   "metadata": {},
   "source": [
    "Now, we will invoke the `evaluate` API using three evaluators:\n",
    "1. `qa`, which is a composite evaluator.\n",
    "2. `answer_length`, which is a custom code-based evaluator.\n",
    "3. `apology`, which is a prompt-based evaluator.\n",
    "\n",
    "Additionally, we have a column mapping to map the `truth` column from the dataset to `ground_truth`, which is accepted by the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    evaluators={\n",
    "        \"qa\": qa_eval,\n",
    "        \"answer_length\": answer_length,\n",
    "        \"apology\": apology_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"ground_truth\": \"${data.truth}\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0fb00",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's check the results produced by the evaluate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dadc4b-495f-4a46-ad15-d4c8a910b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, JSON\n",
    "\n",
    "display(JSON(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
