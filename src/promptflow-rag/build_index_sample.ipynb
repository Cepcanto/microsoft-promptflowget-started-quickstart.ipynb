{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "This sample shows how to create an index from local files or an existing ai search index. The result MLIndex file will be generated locally.\n",
    "\n",
    "This sample is useful for developers and data scientists who wish to use their data to create an Index which can be used in the RAG pattern.\n",
    "\n",
    "This sample shows how to:\n",
    "- create an index locally\n",
    "- register a local index to cloud\n",
    "- retrieve index from the cloud\n",
    "- consume an index in langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of indexes we will create\n",
    "local_index_name = \"local-test\"\n",
    "\n",
    "# project details\n",
    "subscription_id: str = \"<your-subscription-id>\"\n",
    "resource_group_name: str = \"<your-resource-group>\"\n",
    "project_name: str = \"<your-project-name>\"\n",
    "\n",
    "# model used for embedding\n",
    "embedding_model_aoai: str = \"text-embedding-ada-002\"\n",
    "deployment_name_aoai: str = \"text-embedding-ada-002\"\n",
    "embedding_model_cohere: str = \"cohere-embed-v3-multilingual\" # or \"cohere-embed-v3-english\"\n",
    "\n",
    "# connection details\n",
    "ai_search_connection_name: str = \"<your-ai-search-connection>\"\n",
    "aoai_connection_name: str = \"<your-aoai-connection>\"\n",
    "# serverless_connection_name: str = \"<your-serverless-connection>\"\n",
    "\n",
    "import os\n",
    "# set credentials to your Azure OpenAI instance\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-07-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = \"api-key\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"api-endpoint\"\n",
    "\n",
    "# set credentials to your Azure AI Search instance\n",
    "os.environ[\"AZURE_AI_SEARCH_KEY\"] = \"<your-acs-key>\"\n",
    "os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"] = \"<your-acs-endpoint>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to your project\n",
    "\n",
    "To start with let us create a config file with your project details. This file can be used in this sample or other samples to connect to your workspace. To get the required details, you can go to the Project Overview page in the AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "p = Path(\"config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize MlClient to interact with resources in your Azure AI Studio.\n",
    "\n",
    "Please make sure you have connections for your embedding model and Azure AI Search in this workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "client=MLClient.from_config(DefaultAzureCredential(), path=\"./config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve connections to embedding model and AI Search\n",
    "We will use an Azure Open AI service to access the LLM and embedding model. We will also use an Azure Cognitive Search to store the index. Let us get the details of these from your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_connection = client.connections.get(aoai_connection_name)\n",
    "ai_search_connection = client.connections.get(ai_search_connection_name)\n",
    "# serverless_connection = client.connections.get(serverless_connection_name)\n",
    "\n",
    "print(f\"aoai connection id is {aoai_connection.id}\")\n",
    "print(f\"aoai connection id is {ai_search_connection.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input types\n",
    "\n",
    "You can build index from local files or an existing ai search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import AzureAISearchSource, LocalSource\n",
    "# local files\n",
    "input_source_local = LocalSource(input_data=\"product-info/\")\n",
    "\n",
    "# existing ai search index \n",
    "# keys might be different, please refer to your MLIndex\n",
    "input_source = AzureAISearchSource(ai_search_index_name=\"<index-name>\",\n",
    "                                   ai_search_content_key=\"content\",\n",
    "                                   ai_search_embedding_key=\"contentVector\",\n",
    "                                   ai_search_title_key=\"title\",\n",
    "                                   ai_search_metadata_key=\"meta_json_string\",\n",
    "                                   ai_search_connection_id=\"<connection-id>\"\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build index locally\n",
    "\n",
    "### 1. With AOAI embedding model\n",
    "To connect to your aoai embedding model, you can either set your api-key and endpoint in the environment variable, or pass in an optional ConnectionConfig if you have a connection to the model deployment in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import LocalSource, AzureAISearchConfig, EmbeddingsModelConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "ai_search_index_path=build_index(\n",
    "    name=local_index_name + \"aoai\",  # name of your index\n",
    "    vector_store=\"azure_ai_search\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=embedding_model_aoai,\n",
    "        deployment_name=deployment_name_aoai\n",
    "        # connection_config = ConnectionConfig(\n",
    "        #     subscription = \"<subscription>\",\n",
    "        #     resource_group = \"<resource-group>\",\n",
    "        #     workspace = \"<workspace>\",\n",
    "        #     connection_name = \"<connection-name>\"\n",
    "        # ),\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"product-info/\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=local_index_name + \"-aoai-store\" # the name of the index store inside the azure ai search service\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With Cohere embedding model\n",
    "To use your cohere embedding model, please specify the ConnectionConfig to the model deployment you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import LocalSource, AzureAISearchConfig, EmbeddingsModelConfig, ConnectionConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "ai_search_index_path=build_index(\n",
    "    name=local_index_name + \"cohere\",  # name of your index\n",
    "    vector_store=\"azure_ai_search\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=embedding_model_cohere,\n",
    "        connection_config=ConnectionConfig(\n",
    "            subscription = \"<subscription>\",\n",
    "            resource_group = \"<resource-group>\",\n",
    "            workspace = \"<workspace>\",\n",
    "            connection_name = \"<connection-name>\"\n",
    "        )\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"product-info/\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=local_index_name + \"cohere-store\" # the name of the index store inside the azure ai search service\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the index\n",
    "Register the index so that it shows up in the AI Studio Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Index\n",
    "client.indexes.create_or_update(Index(name=local_index_name, path=ai_search_index_path, version=\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve index from the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_index=client.indexes.get(name=index_name, label=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Consume the index as a langchain retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag import get_langchain_retriever_from_index\n",
    "\n",
    "# Get the Index\n",
    "retriever=get_langchain_retriever_from_index(ml_index.path)\n",
    "retriever.get_relevant_documents(\"which tent is the most waterproof?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
