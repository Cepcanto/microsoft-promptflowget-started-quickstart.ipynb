{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "This sample shows how to create an index from local files or an existing ai search index. The result MLIndex file will be generated locally.\n",
    "\n",
    "This sample is useful for developers and data scientists who wish to use their data to create an Index which can be used in the RAG pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of indexes we will create\n",
    "local_index_name = \"local-test\"\n",
    "\n",
    "# model used for embedding\n",
    "embedding_model_aoai: str = \"text-embedding-ada-002\"\n",
    "deployment_name_aoai: str = \"text-embedding-ada-002\"\n",
    "embedding_model_cohere: str = \"cohere-embed-v3-multilingual\" # or \"cohere-embed-v3-english\"\n",
    "\n",
    "import os\n",
    "\n",
    "# set credentials to your Azure OpenAI instance\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-07-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = \"api-key\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"api-endpoint\"\n",
    "\n",
    "# set credentials to your Azure AI Search instance\n",
    "os.environ[\"AZURE_AI_SEARCH_KEY\"] = \"<your-acs-key>\"\n",
    "os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"] = \"<your-acs-endpoint>\"\n",
    "\n",
    "from promptflow.rag.resources import AzureAISearchSource\n",
    "input_source = AzureAISearchSource(ai_search_index_name=\"<index-name>\",\n",
    "                                   ai_search_content_key=\"content\",\n",
    "                                   ai_search_embedding_key=\"contentVector\",\n",
    "                                   ai_search_title_key=\"title\",\n",
    "                                   ai_search_metadata_key=\"meta_json_string\",\n",
    "                                   ai_search_connection_id=\"<connection-id>\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input types\n",
    "\n",
    "You can build index from local files or an existing ai search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.resources import AzureAISearchSource, LocalSource\n",
    "# local files\n",
    "input_source_local = LocalSource(input_data=\"product-info/\")\n",
    "\n",
    "# existing ai search index \n",
    "# keys might be different, please refer to your MLIndex\n",
    "input_source = AzureAISearchSource(ai_search_index_name=\"<index-name>\",\n",
    "                                   ai_search_content_key=\"content\",\n",
    "                                   ai_search_embedding_key=\"contentVector\",\n",
    "                                   ai_search_title_key=\"title\",\n",
    "                                   ai_search_metadata_key=\"meta_json_string\",\n",
    "                                   ai_search_connection_id=\"<connection-id>\"\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build index locally\n",
    "\n",
    "### 1. With AOAI embedding model\n",
    "To connect to your aoai embedding model, you can either set your api-key and endpoint in the environment variable, or pass in an optional ConnectionConfig if you have a connection to the model deployment in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.resources import LocalSource, AzureAISearchConfig, EmbeddingsModelConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "ai_search_index_path=build_index(\n",
    "    name=local_index_name + \"aoai\",  # name of your index\n",
    "    vector_store=\"azure_ai_search\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=embedding_model_aoai,\n",
    "        deployment_name=deployment_name_aoai\n",
    "        # connection_config = ConnectionConfig(\n",
    "        #     subscription = \"<subscription>\",\n",
    "        #     resource_group = \"<resource-group>\",\n",
    "        #     workspace = \"<workspace>\",\n",
    "        #     connection_name = \"<connection-name>\"\n",
    "        # ),\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"product-info/\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=local_index_name + \"-aoai-store\" # the name of the index store inside the azure ai search service\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With Cohere embedding model\n",
    "To use your cohere embedding model, please specify the ConnectionConfig to the model deployment you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.resources import LocalSource, AzureAISearchConfig, EmbeddingsModelConfig, ConnectionConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "ai_search_index_path=build_index(\n",
    "    name=local_index_name + \"cohere\",  # name of your index\n",
    "    vector_store=\"azure_ai_search\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=embedding_model_cohere,\n",
    "        connection_config=ConnectionConfig(\n",
    "            subscription = \"<subscription>\",\n",
    "            resource_group = \"<resource-group>\",\n",
    "            workspace = \"<workspace>\",\n",
    "            connection_name = \"<connection-name>\"\n",
    "        )\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"product-info/\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=local_index_name + \"cohere-store\" # the name of the index store inside the azure ai search service\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
