{
  "flow": {
    "id": "QnA_relevance_scores",
    "name": "QnA_relevance_scores",
    "nodes": [
      {
        "name": "scores",
        "tool": "compute_relevance_scores",
        "inputs": {
          "question": "${flow.question}",
          "answers": "${flow.answers}",
          "max_tokens": "256",
          "deployment_name": "text-davinci-003",
          "temperature": "0.7"
        },
        "api": "completion",
        "provider": "AzureOpenAI",
        "connection": "azure_open_ai_connection"
      },
      {
        "name": "results",
        "tool": "compare_with_baseline",
        "inputs": {
          "scores": "${scores.output}",
          "variant_ids": "${flow.variant_ids}"
        }
      },
      {
        "name": "aggregate_variants_results",
        "tool": "aggregate_variants_results",
        "inputs": {
          "results": "${results.output}",
          "line_number": "${flow.line_number}",
          "variant_ids": "${flow.variant_ids}"
        },
        "reduce": true
      }
    ],
    "inputs": {
      "question": {
        "type": "string"
      },
      "answers": {
        "type": "list"
      },
      "line_number": {
        "type": "int"
      },
      "variant_ids": {
        "type": "list"
      }
    },
    "outputs": {
      "score": {
        "type": "list",
        "reference": "${aggregate_variants_results.output.score}"
      },
      "win_rate": {
        "type": "list",
        "reference": "${aggregate_variants_results.output.win_rate}"
      }
    },
    "tools": [
      {
        "name": "compute_relevance_scores",
        "type": "llm",
        "inputs": {
          "question": {
            "type": [
              "string"
            ]
          },
          "answers": {
            "type": [
              "string"
            ]
          }
        },
        "description": "This is a llm tool",
        "code": "I want you to act as a ranking system.\nYour task is to evaluate how well each answer in the given answer candidates matches a given question,\nand give relevance scores of all answers.\nScores range from 0 to 100, where higher scores indicate higher confidence in the answer.\n\nQuestion: Who is the actor of Iron Man?\nAnswers: [\"Matt Salinger\", \"Downey\", \"Iron\", \"Robert Downey\"]\nScores: [20, 50, 0, 100]\n\nQuestion: How does AutoGPT compare to ChatGPT?\nAnswers: [\"AutoGPT is designed to be fully autonomous\",\n\"AutoGPT and ChatGPT are both AI applications that use large language models to generate text.\",\n\"AutoGPT and ChatGPT are both AI applications that use large language models to generate text,\nbut they have some key differences.]\nScores: [80, 50, 90]\n\nQuestion: {{question}}\nAnswers: {{answers}}\nScores:\n"
      },
      {
        "name": "compare_with_baseline",
        "type": "python",
        "inputs": {
          "scores": {
            "type": [
              "string"
            ]
          },
          "variant_ids": {
            "type": [
              "object"
            ]
          }
        },
        "code": "from typing import List, Mapping, Dict\nfrom promptflow import tool\n\n@tool\ndef compare_with_baseline(scores: str, variant_ids: List[str]):\n    # need json load first because \"scores\" are generated by llm node.\n    import json\n    scores_list = json.loads(scores)\n    # we use variant0 as baseline.\n    baseline_score = scores_list[0]\n    results = {\n        variant_ids[0]: {\n            \"score\": baseline_score,\n            \"win_rate\": \"\"\n        }\n    }\n\n    for index in range(1, len(variant_ids)):\n        name = variant_ids[index]\n        if name not in results.keys():\n            results[name] = {}\n        score = scores_list[index]\n        results[name][\"score\"] = score\n        results[name][\"win_rate\"] = 1 if score >= baseline_score else 0\n\n    return results\n",
        "function": "compare_with_baseline"
      },
      {
        "name": "aggregate_variants_results",
        "type": "python",
        "inputs": {
          "line_number": {
            "type": [
              "object"
            ]
          },
          "variant_ids": {
            "type": [
              "object"
            ]
          },
          "results": {
            "type": [
              "object"
            ]
          }
        },
        "code": "from typing import List, Mapping, Dict\nfrom promptflow import tool, log_metric\n\n@tool\ndef aggregate_variants_results(line_number: List[int], variant_ids: List[List[str]], results: List[dict]):\n    aggregate_results = {}\n    for index in range(len(line_number)):\n        result = results[index]\n        for name, value in result.items():\n            if name not in aggregate_results.keys():\n                aggregate_results[name] = []\n\n            aggregate_results[name].append(value[\"score\"])\n\n    # average scores for each result and log score metrics\n    baseline_name = variant_ids[0][0]\n    for name, value in aggregate_results.items():\n        average_score = round(sum(value) / len(value), 1)\n        aggregate_results[name] = average_score\n        log_metric(\"score\", average_score, variant_id=name)\n\n    log_metric(\"average_score\", sum(aggregate_results.values()) / len(aggregate_results))\n\n    # log win rate metrics\n    baseline_average_score = aggregate_results[baseline_name]\n    for index in range(1, len(variant_ids[0])):\n        name = variant_ids[0][index]\n        if aggregate_results[name] >= baseline_average_score:\n            log_metric(\"win_rate\", 1, variant_id=name)\n        else:\n            log_metric(\"win_rate\", 0, variant_id=name)\n\n    # collect scores and win rates\n    scores, win_rates = [], []\n    for i in range(len(variant_ids)):\n        scores_i, win_rates_i = [], []\n        for j in range(len(variant_ids[i])):\n            variant_name = variant_ids[i][j]\n            scores_i.append(results[i][variant_name][\"score\"])\n            win_rates_i.append(results[i][variant_name][\"win_rate\"])\n        scores.append(scores_i)\n        win_rates.append(win_rates_i)\n\n    return {\"score\": scores, \"win_rate\": win_rates}\n",
        "function": "aggregate_variants_results"
      }
    ]
  },
  "connections": {},
  "batch_inputs": [
    {
      "question": "What is your favorite meat?",
      "answers": [
        "answer1",
        "answer2",
        "answer3"
      ],
      "line_number": 0,
      "variant_ids": [
        "variant_0",
        "variant_1",
        "variant_2"
      ]
    }
  ],
  "name": "",
  "description": "",
  "baseline_variant_id": "",
  "variants": {},
  "variants_tools": []
}
