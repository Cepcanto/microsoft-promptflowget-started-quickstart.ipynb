{
    "flow_graph": "ask_wikipedia_variant.json",
    "flow_run_settings": {
      "runMode": "SingleNode",
      "nodeName": "augmented_qna",
      "nodeInputs": {
        "inputs.question": "When did OpenAI announce GPT-4?",
        "process_search_result.output": "Content: Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its numbered \"GPT-n\" series of GPT foundation models.[1] It was released on March 14, 2023, and has been made publicly available in a limited form via the chatbot product ChatGPT Plus (a premium version of ChatGPT), and with access to the GPT-4 based version of OpenAI's API being provided via a waitlist.[1] As a transformer based model, GPT-4 was pretrained to predict the next token (using both public data and \"data licensed from third-party providers\"), and was then fine-tuned with reinforcement learning from human and AI feedback for human alignment and policy compliance.[2]:â€Š2. Observers reported the GPT-4 based version of ChatGPT to be an improvement on the previous (GPT-3.5 based) ChatGPT, with the caveat that GPT-4 retains some of the same problems.[3] Unlike the predecessors, GPT-4 can take images as well as text as input.[4] OpenAI has declined to reveal technical information such as the size of the GPT-4 model.[5]. OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\"[6] It was based on the transformer architecture and trained on a large corpus of books.[7] The next year, they introduced GPT-2, a larger model that could generate coherent text.[8] In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples.[9] GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.. OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\"[10] They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively.[11] Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks[12] in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input;[4] this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams.[13]. To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.[13]. When instructed to do so, GPT-4 can interact with external interfaces.[14] For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.[15].\nSource: https://en.wikipedia.org/w/index.php?search=GPT-4\n\nContent: OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing friendly AI.. OpenAI was founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members.[4][5][6] Microsoft provided OpenAI LP with a $1 billion investment in 2019 and a $10 billion investment in 2023.[7][8]. In December 2015, Sam Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Elon Musk, Amazon Web Services (AWS), Infosys, and YC Research announced[9] the formation of OpenAI and pledged over $1 billion to the venture. According an investigation led by TechCrunch, the non-profit's funding remains murky, with Musk its biggest funder while another donor, YC Research, did not contribute anything at all.[10] The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public.[11][12] OpenAI is headquartered at the Pioneer Building in Mission District, San Francisco.[13][14]. According to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of the deep learning movement, and drew up a list of the \"best researchers in the field\".[15] Brockman was able to hire nine of them as the first employees in December 2015.[15] In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.[15]. Microsoft's Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect.[15] OpenAI's potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\"[15] Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\"[15] OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.[15]. In April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research.[16] In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.[17][18][19][20]. In 2017 OpenAI spent $7.9 million, or a quarter of its functional expenses, on cloud computing alone.[21] In comparison, DeepMind's total expenses in 2017 were $442 million. In summer 2018, simply training OpenAI's Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks..\nSource: https://en.wikipedia.org/w/index.php?search=OpenAI"
      }
    }
}
