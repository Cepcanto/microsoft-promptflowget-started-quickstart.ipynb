{
  "flow": {
    "id": "Simple_mock_answer",
    "name": "Simple_mock_answer",
    "nodes": [
      {
        "name": "simple_prompt",
        "tool": "Prompt_dxe3",
        "inputs": {
          "question": "${flow.question}"
        }
      },
      {
        "name": "python_mock_answer",
        "tool": "Python_b4xz",
        "inputs": {
          "input1": "${simple_prompt.output}"
        }
      }
    ],
    "inputs": {
      "question": {
        "type": "string"
      }
    },
    "outputs": {
      "output": {
        "type": "string",
        "reference": "${python_mock_answer.output}"
      }
    },
    "tools": [
      {
        "name": "Prompt_dxe3",
        "type": "prompt",
        "inputs": {
          "question": {
            "type": [
              "string"
            ]
          }
        },
        "code": "{{question}}"
      },
      {
        "name": "Prompt_dxe3__variant_1",
        "type": "prompt",
        "inputs": {
          "question": {
            "type": [
              "string"
            ]
          }
        },
        "code": "{{question}}"
      },
      {
        "name": "Prompt_dxe3__variant_2",
        "type": "prompt",
        "inputs": {
          "question": {
            "type": [
              "string"
            ]
          }
        },
        "code": "{{question}}"
      },
      {
        "name": "Python_b4xz",
        "type": "python",
        "inputs": {
          "input1": {
            "type": [
              "string"
            ]
          }
        },
        "code": "from promptflow import tool\n\n# The inputs section will change based on the arguments of the tool function, after you save the code\n# Adding type to arguments and return value will help the system show the types properly\n# Please update the function name/signature per need\n@tool\ndef my_python_tool(input1: str) -> str:\n  return \"A fruit.\"",
        "function": "my_python_tool"
      }
    ]
  },
  "connections": {},
  "name": "Simple_mock_answer-bulktest",
  "baseline_variant_id": "variant_0",
  "variants": {
    "variant_1": [
      {
        "name": "simple_prompt",
        "tool": "Prompt_dxe3__variant_1",
        "inputs": {
          "question": "${flow.question}"
        }
      }
    ],
    "variant_2": [
      {
        "name": "simple_prompt",
        "tool": "Prompt_dxe3__variant_2",
        "inputs": {
          "question": "${flow.question}"
        }
      }
    ]
  },
  "variants_runs": {
    "variant_1": "BaseFlowRunId_BulkTestId_variant_1",
    "variant_2": "BaseFlowRunId_BulkTestId_variant_2"
  },
  "bulk_test_id": "BulkTestId",
  "eval_flow": {
    "id": "dummy_qna_eval",
    "name": "Dummy - QnA Relevance Scores Evaluation",
    "nodes": [
      {
        "name": "compute_relevance_scores",
        "tool": "compute_relevance_scores",
        "inputs": {
          "question": "${flow.question}",
          "answers": "${flow.answers}"
        }
      },
      {
        "name": "compare_with_baseline",
        "tool": "compare_with_baseline",
        "inputs": {
          "scores": "${compute_relevance_scores.output}",
          "variant_ids": "${flow.variant_ids}"
        }
      },
      {
        "name": "aggregate_variants_results",
        "tool": "aggregate_variants_results",
        "inputs": {
          "line_number": "${flow.line_number}",
          "variant_ids": "${flow.variant_ids}",
          "scores": "${compare_with_baseline.output.score}"
        },
        "reduce": true
      }
    ],
    "inputs": {
      "question": {
        "type": "string"
      },
      "answers": {
        "type": "list"
      },
      "line_number": {
        "type": "int"
      },
      "variant_ids": {
        "type": "list"
      }
    },
    "outputs": {
      "score": {
        "type": "list",
        "reference": "${compare_with_baseline.output.score}"
      },
      "win_rate": {
        "type": "list",
        "reference": "${compare_with_baseline.output.win_rate}"
      }
    },
    "tools": [
      {
        "name": "compute_relevance_scores",
        "type": "python",
        "inputs": {
          "question": {
            "type": [
              "string"
            ]
          },
          "answers": {
            "type": [
              "object"
            ]
          }
        },
        "code": "from typing import List\n\nfrom promptflow import tool\n\n\n@tool\ndef compute_relevance_scores(question: str, answers: List):\n    \n    compute_relevance_scores_template = \"\"\"Your task is to evaluate how well the given answer candidates matches a given question,\n    and give a score for this answer.\n    Scores range from 0 to 100, where higher score indicate higher confidence in the answer.\n\n    Question: Who is the actor of Iron Man?\n    Answer: \"Matt Salinger\"\n    Score: 20\n\n    Question: Who is the actor of Iron Man?\n    Answer: \"Robert Downey\"\n    Score: 100\n\n    Question: How does AutoGPT compare to ChatGPT?\n    Answer: AutoGPT is designed to be fully autonomous\"\n    Score: 80\n\n    Question: How does AutoGPT compare to ChatGPT?\n    Answer: \"AutoGPT and ChatGPT are both AI applications that use large language models to generate text.\"\n    Score: 50\n\n    Question: {{question}}\n    Answer: {{answer}}\n    Score:\n    \"\"\"\n\n    # for relevance score evaluation must have at least two answers\n    assert len(answers) > 1\n\n    scores = []\n    current_score = 90\n    for answer in answers:\n        scores.append(current_score)\n        current_score -= 10\n\n    return scores\n",
        "function": "compute_relevance_scores"
      },
      {
        "name": "compare_with_baseline",
        "type": "python",
        "inputs": {
          "scores": {
            "type": [
              "list"
            ]
          },
          "variant_ids": {
            "type": [
              "object"
            ]
          }
        },
        "source": "compare_with_baseline.py",
        "code": "from typing import List\n\nfrom promptflow import tool\n\n\n@tool\ndef compare_with_baseline(scores: List, variant_ids: List[str]):\n    # we use variant0 as baseline.\n    baseline_score = scores[0]\n    score_map = {\n        variant_ids[0]: baseline_score\n    }\n    win_rate_map = {}\n\n    for index in range(1, len(variant_ids)):\n        name = variant_ids[index]\n        if name not in score_map.keys():\n            score_map[name] = {}\n\n        if name not in win_rate_map.keys():\n            win_rate_map[name] = {}\n\n        score = scores[index]\n        score_map[name] = score\n        win_rate_map[name] = 1 if score >= baseline_score else 0\n\n    return {\"score\": score_map, \"win_rate\": win_rate_map}",
        "function": "compare_with_baseline"
      },
      {
        "name": "aggregate_variants_results",
        "type": "python",
        "inputs": {
          "line_number": {
            "type": [
              "list"
            ]
          },
          "variant_ids": {
            "type": [
              "list"
            ]
          },
          "scores": {
            "type": [
              "list"
            ]
          }
        },
        "source": "aggregate_variants_results.py",
        "code": "from typing import List\n\nfrom promptflow import tool, log_metric\n\n\n@tool\ndef aggregate_variants_results(line_number: List[int], variant_ids: List[List[str]], scores: List[dict]):\n    aggregate_results = {}\n    for index in range(len(line_number)):\n        score = scores[index]\n        for name, value in score.items():\n            if name not in aggregate_results.keys():\n                aggregate_results[name] = []\n\n            aggregate_results[name].append(value)\n\n    # average scores for each result and log score metrics\n    baseline_name = variant_ids[0][0]\n    for name, value in aggregate_results.items():\n        average_score = round(sum(value) / len(value), 1)\n        aggregate_results[name] = average_score\n        log_metric(\"score\", average_score, variant_id=name)\n\n    # log win rate metrics\n    baseline_average_score = aggregate_results[baseline_name]\n    for index in range(1, len(variant_ids[0])):\n        name = variant_ids[0][index]\n        if aggregate_results[name] >= baseline_average_score:\n            log_metric(\"win_rate\", 1, variant_id=name)\n        else:\n            log_metric(\"win_rate\", 0, variant_id=name)\n\n    return aggregate_results\n",
        "function": "aggregate_variants_results"
      }
    ]
  },
  "eval_flow_run_id": "EvalFlowRunId",
  "eval_flow_inputs_mapping": {
    "question": "data.question",
    "answers": "output.output"
  },
  "batch_inputs": [
    {
      "question": "what is apple?"
    },
    {
      "question": "what is banana?"
    }
  ]
}
