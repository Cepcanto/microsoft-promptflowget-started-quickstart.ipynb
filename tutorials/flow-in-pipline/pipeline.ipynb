{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Flow in Pipeline\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with compute cluster\n",
    "- A python environment (3.9 recommended)\n",
    "- Installed Azure Machine Learning Python SDK v2\n",
    "- Installed PromptFlow SDK\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Connect to your AML workspace from the Python SDK\n",
    "- Load a flow as a `ParallelComponent`\n",
    "- Using the component along with other components loaded from yaml in one `PipelineJob`.\n",
    "\n",
    "**Motivations** - This notebook covers the scenario that user use a flow along with other data processing steps in a pipeline.\n",
    "\n",
    "Below is the command to install dependent packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure-ai-ml is of a private version for now, targeting to release on 1.9.0 (July release)\n",
    "%pip install promptflow-sdk[azure, built-ins] azure-ai-ml==1.9.0a20230703002 --extra-index-url https://azuremlsdktestpypi.azureedge.net/promptflow/ --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similar to other SDK in azure-ai-ml, you need to prepare a ML client connecting to a specific workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "import promptflow as pf\n",
    "\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cluster_name = \"cpu-cluster\"\n",
    "print(ml_client.compute.get(cluster_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load flow as a component\n",
    "\n",
    "Suppose you have already authoring a flow, you can load it as component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the remote component will be created on calling load_as_component, \n",
    "# so ml_client must be configured for promptflow-sdk first\n",
    "pf.azure.configure(ml_client)\n",
    "\n",
    "flow_component = pf.load_as_component(\n",
    "    \"../../flows/standard/web-classification\",\n",
    "    inputs_mapping={\n",
    "        \"groundtruth\": \"1\",\n",
    "        \"prediction\": \"${{variant.outputs.category}}\"\n",
    "    },\n",
    "    node_variant=\"variant_1\"\n",
    ")\n",
    "\n",
    "print(flow_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process_component = load_component()\n",
    "\n",
    "@pipeline\n",
    "def pipeline_with_flow(input_data):\n",
    "    data_process = data_process_component(input_data=input_data)\n",
    "\n",
    "    flow_node = data_process_component(\n",
    "        data=data_process.outputs.output_data,\n",
    "        # this is to overwrite\n",
    "        connections={\n",
    "            # this is to overwrite connection related settings for a LLM node\n",
    "            \"summarize_text_content\": {\n",
    "                \"deployment_name\": \"another_deployment_name\",\n",
    "                \"connection\": \"another_connection\"\n",
    "            },\n",
    "            # TODO: not sure if we should mention this\n",
    "            # you can also overwrite connection input of a python node here\n",
    "            \"post_process\": {\n",
    "                \"conn1\": \"another_connection\"\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    # node level run settings for flow node is similar to `ParallelComponent`\n",
    "    flow_node.logging_level = \"DEBUG\"\n",
    "    flow_node.max_concurrency_per_instance = 2\n",
    "    return flow_node.outputs\n",
    "\n",
    "pipeline = pipeline_with_flow(\n",
    "    input_data=Input(path=f\"../../data\", type=AssetTypes.URI_FOLDER),\n",
    ")\n",
    "\n",
    "pipeline.settings.default_compute = \"cpu-cluster\"\n",
    "\n",
    "created_job = ml_client.jobs.create_or_update(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(created_job.name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
